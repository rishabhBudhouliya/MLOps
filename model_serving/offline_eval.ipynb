{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c27193b2-d1a3-4bb0-a0f4-43a1d86a7c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install necessary packages\n",
    "!pip install transformers datasets torch accelerate bitsandbytes sentencepiece peft trl bert-score mlflow\n",
    "!pip install evaluate bert-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f51947f0-2f03-41c0-8ad4-dccde63b331f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "import time\n",
    "import accelerate\n",
    "from accelerate import infer_auto_device_map, dispatch_model\n",
    "from types import MethodType\n",
    "import gc\n",
    "from transformers import BitsAndBytesConfig, AutoTokenizer, AutoModelForCausalLM, TrainingArguments, GenerationConfig\n",
    "import mlflow\n",
    "import mlflow.pytorch\n",
    "import subprocess\n",
    "from transformers.cache_utils import Cache, DynamicCache\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from trl import SFTTrainer\n",
    "from bert_score import score as bertscore\n",
    "import numpy as np\n",
    "import gzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "40412002-1b79-4d0e-a258-131788abad70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "33c1d2bf-ebae-45f4-ada3-44d689c36219",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and process dataset\n",
    "guidelines = \"\"\"Key guidelines to follow:\n",
    "- Use standard Java libraries instead of external ones like Commons I/O when possible.\n",
    "- Avoid deprecated APIs, especially in Jenkins core and plugins.\n",
    "- Write clear, descriptive method and variable names.\n",
    "- Add or update tests when modifying functionality or fixing bugs.\n",
    "- Do not include commented-out code or leftover TODOs.\n",
    "- Update documentation if user-facing behavior changes.\n",
    "- Keep commits focused and avoid mixing unrelated changes.\n",
    "- Code must compile cleanly and pass all tests.\n",
    "- Maintain consistent formatting and follow Jenkins coding style.\n",
    "Also consider other good practices not explicitly listed above.\"\"\"\n",
    "def format_prompt(example):\n",
    "    offset = example.get('offset')\n",
    "    offset_info = f\"The comment refers to line {offset} in the diff.\" if offset is not None else \"\"\n",
    "\n",
    "    formatted_comment = (\n",
    "        f\"<COMMENT offset=\\\"{offset}\\\">{example['comment']}\\n\"\n",
    "        if offset is not None and example.get('comment')\n",
    "        else example.get('comment', '')\n",
    "    )\n",
    "\n",
    "    prompt = f\"\"\"### Instruction:\n",
    "You are a code reviewer for a Jenkins plugin. Review the following diff for potential improvements or guideline violations.\n",
    "{offset_info}\n",
    "\n",
    "{guidelines}\n",
    "\n",
    "### Input:\n",
    "Diff snippet:\n",
    "{example['diff']}\n",
    "\n",
    "### Response:\n",
    "{formatted_comment}\"\"\"\n",
    "\n",
    "    tokens = tokenizer(prompt, truncation=True, padding='max_length', max_length=1024)\n",
    "    tokens[\"labels\"] = tokens[\"input_ids\"].copy()\n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8e056550-ce05-41da-b551-a8d89756aba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments, DataCollatorForLanguageModeling, DataCollatorForSeq2Seq\n",
    "from peft import get_peft_model, LoraConfig, TaskType, prepare_model_for_kbit_training\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c89e1da-9612-4108-9a36-805b667ee7f0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load tokenizer and model\n",
    "model_name = \"codellama/CodeLlama-7b-Instruct-hf\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Load the model with proper device mapping\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",\n",
    "    load_in_8bit=True,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "state_dict = torch.load(\"final_model.pth\", map_location=\"cuda\")  # or \"cuda\" if on GPU\n",
    "\n",
    "# If the .pth is just a `state_dict`, load it directly\n",
    "model.load_state_dict(state_dict, strict=False)  # strict=False allows partial loading\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "061a9798-2462-4ba5-b2a4-fae50a72342f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First record of the processed dataset:\n",
      "{\n",
      "    \"diff\": \"func (s *PluginsService) checkForUpdates(ctx context.Context) error {\\n@@ -195,6 +224,33 @@ func canUpdate(v1, v2 string) bool {\\n \\treturn ver1.LessThan(ver2)\\n }\\n \\n+func (s *PluginsService) isPluginUpdatable(ctx context.Context, plugin pluginstore.Plugin) bool {\\n+\\tif plugin.IsCorePlugin() || s.isManaged(ctx, plugin.ID) || s.isProvisioned(ctx, plugin.ID) {\",\n",
      "    \"comment\": \"You'd also need to check if the plugin is pinned to a version, like here:\\r\\nhttps://github.com/grafana/grafana/blob/main/apps/advisor/pkg/app/checks/plugincheck/check.go#L171\\r\\n\\r\\nSince we are duplicating code here, it sounds like it would be nice to create a service to expose an \\\"IsUpdatable\\\" function?\",\n",
      "    \"offset\": 4,\n",
      "    \"comment_commit_id\": \"0ee7c64baa6b455704545e432ce8956a665f047e\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import gzip\n",
    "\n",
    "def load_and_filter_dataset(file_path):\n",
    "    \"\"\"\n",
    "    Loads a gzipped JSONL file and extracts specific fields. Processes only the first comment\n",
    "    encountered for each unique 'comment_commit_id'.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): The path to the .jsonl.gz file.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of dictionaries, where each dictionary contains\n",
    "              'diff', 'comment', and 'offset' for each record.\n",
    "    \"\"\"\n",
    "    filtered_data = []\n",
    "    seen_commit_ids = set()  # Track seen commit IDs\n",
    "\n",
    "    with gzip.open(file_path, \"rt\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            try:\n",
    "                record = json.loads(line)\n",
    "\n",
    "                # Skip if we've already seen this comment_commit_id\n",
    "                comment_commit_id = record.get('comment_commit_id')\n",
    "                if comment_commit_id and comment_commit_id in seen_commit_ids:\n",
    "                    continue  # Skip this record\n",
    "\n",
    "                # Mark this comment_commit_id as seen\n",
    "                if comment_commit_id:\n",
    "                    seen_commit_ids.add(comment_commit_id)\n",
    "\n",
    "                # The 'diff' in your original data is the main part of the diff.\n",
    "                # The 'diff_hunk_header' is the line that usually starts with '@@'.\n",
    "                # We'll combine them if both exist, or use whichever is present.\n",
    "                \n",
    "                diff_parts = []\n",
    "                if 'diff_hunk_header' in record and record['diff_hunk_header']:\n",
    "                    diff_parts.append(record['diff_hunk_header'])\n",
    "                if 'diff' in record and record['diff']:\n",
    "                    diff_parts.append(record['diff'])\n",
    "                \n",
    "                full_diff = \"\\n\".join(diff_parts) if diff_parts else None\n",
    "\n",
    "                filtered_record = {\n",
    "                    'diff': full_diff,\n",
    "                    'comment': record.get('comment_body'),  # Use .get() for safety if key might be missing\n",
    "                    'offset': record.get('line_offset'),    # Use .get() for safety\n",
    "                    'comment_commit_id': comment_commit_id  # Keep track of the commit ID\n",
    "                }\n",
    "                filtered_data.append(filtered_record)\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Skipping line due to JSON decode error: {e} - Line: {line.strip()}\")\n",
    "            except KeyError as e:\n",
    "                print(f\"Skipping record due to missing key: {e} - Record: {record}\")\n",
    "\n",
    "    return filtered_data\n",
    "\n",
    "# --- --- --- --- --- --- --- --- --- --- --- --- --- --- ---\n",
    "#                  Example Usage:\n",
    "# --- --- --- --- --- --- --- --- --- --- --- --- --- --- ---\n",
    "\n",
    "# 1. Make sure 'train.jsonl.gz' is in the same directory as your script,\n",
    "#    or provide the full path to the file.\n",
    "file_path = \"val.jsonl.gz\"\n",
    "processed_dataset = load_and_filter_dataset(file_path)\n",
    "\n",
    "# 2. View the first record of your new dataset\n",
    "if processed_dataset:\n",
    "    print(\"First record of the processed dataset:\")\n",
    "    print(json.dumps(processed_dataset[0], indent=4)) # Pretty print the JSON\n",
    "else:\n",
    "    print(\"No data was processed. Check your file path and file content.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "33d00999-67a8-4497-843e-815f280fc00c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_prompt_for_inference(diff):\n",
    "    prompt = f\"\"\"### Instruction:\n",
    "You are a code reviewer for a Jenkins plugin. Review the following diff for potential improvements or guideline violations.\n",
    "\n",
    "Your response must follow this format exactly:\n",
    "<COMMENT offset=\"LINE_NUMBER\">Your review comment here.\n",
    "\n",
    "Where offset is the line number the review comment is talking about. If no issues are found, respond with: <COMMENT offset=\"None\">.\n",
    "\n",
    "{guidelines}\n",
    "\n",
    "### Input:\n",
    "Diff snippet:\n",
    "{diff}\n",
    "\n",
    "### Response:\"\"\"\n",
    "    tokens = tokenizer(prompt, truncation=True, padding='max_length', max_length=1024)\n",
    "    tokens[\"labels\"] = tokens[\"input_ids\"].copy()\n",
    "    return tokens\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Prepare prompts\n",
    "inference_prompts = [\n",
    "    format_prompt_for_inference(e['diff'])\n",
    "    for e in processed_dataset if e['diff'] and e['comment']\n",
    "]\n",
    "eval_dataset = Dataset.from_list(inference_prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2ff802e5-f524-4eb2-874b-2e7daf50e75a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32016, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear8bitLt(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear8bitLt(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear8bitLt(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear8bitLt(in_features=4096, out_features=4096, bias=False)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear8bitLt(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear8bitLt(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear8bitLt(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32016, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "model.eval()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1b087e7f-6eb8-4fa3-94e7-3ba8a6426085",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from bert_score import score\n",
    "\n",
    "from transformers import default_data_collator\n",
    "subset_dataset = eval_dataset.select(range(0, 100))\n",
    "eval_loader = DataLoader(subset_dataset, batch_size=16, collate_fn=default_data_collator)\n",
    "\n",
    "\n",
    "generated_outputs = []\n",
    "references = []\n",
    "model.gradient_checkpointing_disable()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f32ecee7-73dd-4c26-88f7-64a4f88f54ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "813\n"
     ]
    }
   ],
   "source": [
    "print(len(eval_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "192fa247-e0f4-4034-bd9e-43db57efba97",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/7 [00:00<?, ?it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 14%|█▍        | 1/7 [00:18<01:51, 18.56s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 29%|██▊       | 2/7 [00:37<01:32, 18.50s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 43%|████▎     | 3/7 [00:55<01:13, 18.43s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 57%|█████▋    | 4/7 [01:13<00:55, 18.44s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 71%|███████▏  | 5/7 [01:32<00:36, 18.47s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 86%|████████▌ | 6/7 [01:50<00:18, 18.47s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "100%|██████████| 7/7 [02:03<00:00, 17.69s/it]\n"
     ]
    }
   ],
   "source": [
    "for i, batch in enumerate(tqdm(eval_loader)):\n",
    "    input_ids = batch[\"input_ids\"].to(device)\n",
    "    attention_mask = batch[\"attention_mask\"].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output_ids = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            max_new_tokens=128,\n",
    "            do_sample=False,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "    decoded = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    # Remove prompt portion\n",
    "    input_text = tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
    "    response_part = decoded[len(input_text):].strip()\n",
    "\n",
    "    generated_outputs.append(response_part)\n",
    "    references.append(processed_dataset[i][\"comment\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2eb8914e-5c5b-4fc2-a29d-74f9e177e2d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17ef5c21a7b148c1b9265554bd3b4f53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "875645ba57b64e6bbe0463db14ad0ee3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 0.10 seconds, 70.59 sentences/sec\n",
      "\n",
      "--- BERTScore ---\n",
      "Average Precision: 0.7647\n",
      "Average Recall:    0.7794\n",
      "Average F1 Score:  0.7711\n"
     ]
    }
   ],
   "source": [
    "# Compute BERTScore\n",
    "assert len(generated_outputs) == len(references), \"Mismatch in output and reference lengths\"\n",
    "\n",
    "P, R, F1 = score(generated_outputs, references, lang=\"en\", verbose=True)\n",
    "\n",
    "print(f\"\\n--- BERTScore ---\")\n",
    "print(f\"Average Precision: {P.mean().item():.4f}\")\n",
    "print(f\"Average Recall:    {R.mean().item():.4f}\")\n",
    "print(f\"Average F1 Score:  {F1.mean().item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a6a927b-f38a-4ea2-a8ca-9e65831bbe53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9895dee0-ef87-4581-b48a-212fa21c31bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
