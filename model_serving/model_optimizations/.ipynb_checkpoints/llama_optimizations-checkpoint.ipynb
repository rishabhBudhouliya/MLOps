{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fcc6e203-7aa8-4dee-82e6-28be0a766a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import gzip, json, time\n",
    "\n",
    "# Hugging Face\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "# from optimum.onnxruntime import ORTModelForCausalLM\n",
    "\n",
    "# ONNX runtime\n",
    "import onnx\n",
    "import onnxruntime as ort\n",
    "\n",
    "# Optional: for model inspection, optimizations, quantization\n",
    "# import neural_compressor  # if you're applying quantization\n",
    "# from neural_compressor import quantization\n",
    "\n",
    "# Optional: benchmarking visualizations\n",
    "# import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81ba2b28-2e9a-4820-97f3-da7cc81706c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from optimum.onnxruntime import ORTModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef634522-7782-488e-b7db-9087277cbc2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "import gc\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "99102de2-dcbf-4073-acfd-7bfc824d4508",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f7c50d6e81e4ebc90de3a0a600d267a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"codellama/CodeLlama-7b-Instruct-hf\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model = AutoModelForCausalLM.from_pretrained(\"codellama/CodeLlama-7b-Instruct-hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7259a2ba-f3b9-4713-b0f0-fac43d8e294b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(32016, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32016, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1d01799a-d78c-44f4-8d25-484671ed776d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_size(model):\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    dtype_size = torch.finfo(model.dtype).bits // 8\n",
    "    \n",
    "    model_size_gb = total_params * dtype_size / (1024 ** 3)\n",
    "    # print(f\"Model size: {model_size_gb:2f} GB\")\n",
    "    return model_size_gb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a8390be8-f2f0-44d2-a514-31c9f5646a43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size: 25.103043 GB\n"
     ]
    }
   ],
   "source": [
    "get_model_size(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c6ba147-ff75-4ae3-a934-0fed0478d870",
   "metadata": {},
   "source": [
    "### Floating Point 15 params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "121c0545-e9ff-4873-a42e-3bcabf34231f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90fc795087b74fd0b2663ea7a6f0932f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"codellama/CodeLlama-7b-Instruct-hf\",\n",
    "    torch_dtype=torch.float16,\n",
    "    # device_map={\"\": 0},\n",
    "    # low_cpu_mem_usage=True\n",
    "    device_map=\"cuda:0\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c5b9097e-bc5f-482c-b5f6-8ec29b6977e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size: 12.551521 GB\n"
     ]
    }
   ],
   "source": [
    "get_model_size(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "77cc5210-f4f3-4842-a142-1a9ddb7f00ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun May 11 13:22:11 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 560.35.05              Driver Version: 560.35.05      CUDA Version: 12.6     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA A100 80GB PCIe          Off |   00000000:27:00.0 Off |                    0 |\n",
      "| N/A   33C    P0             43W /  300W |       1MiB /  81920MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d2f3cac4-bf69-4588-aca5-a97eb7d4a42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"/mnt/object_group/data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "90160599-5139-4efe-af50-0e539dfcd398",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    {\"comment_id\": 74291026, \"comment_user_login\": \"vikerman\", \"comment_body\": \"Is there a better way to avoid having this in different places?\\n\", \"comment_created_at\": \"2016-08-10T17:30:42+00:00\", \"comment_html_url\": \"https://github.com/angular/angular/pull/10620#discussion_r74291026\", \"comment_path\": \"modules/@angular/common/src/forms-deprecated/directives/abstract_control_directive.ts\", \"comment_position\": 10, \"comment_original_position\": 10, \"comment_commit_id\": \"ee8e802b7bc25eafbe54109b3924e9dbc36dce11\", \"comment_original_commit_id\": \"ee8e802b7bc25eafbe54109b3924e9dbc36dce11\", \"diff_line_content\": \"}\\n\", \"diff_line_type\": \"added\", \"diff_line_source_no\": null, \"diff_line_target_no\": 15, \"diff_hunk_header\": \"\", \"diff\": \"@@ -6,10 +6,13 @@\\n  * found in the LICENSE file at https://angular.io/license\\n  */\\n \\n-import {unimplemented} from '../../facade/exceptions';\\n+import {BaseException} from '@angular/core';\\n import {isPresent} from '../../facade/lang';\\n import {AbstractControl} from '../model';\\n \\n+function unimplemented(): any {\\n+  throw new BaseException('unimplemented');\", \"side\": \"RIGHT\", \"line_offset\": 9, \"diff_file_source\": \"a/modules/@angular/common/src/forms-deprecated/directives/abstract_control_directive.ts\", \"diff_file_target\": \"b/modules/@angular/common/src/forms-deprecated/directives/abstract_control_directive.ts\", \"repo\": \"angular/angular\"}\n",
      "    {\"comment_id\": 74305223, \"comment_user_login\": \"alexeagle\", \"comment_body\": \"we could export it in the public API but that's not nice.\\nOr we could inline the method and delete it.\\n\", \"comment_created_at\": \"2016-08-10T18:42:59+00:00\", \"comment_html_url\": \"https://github.com/angular/angular/pull/10620#discussion_r74305223\", \"comment_path\": \"modules/@angular/common/src/forms-deprecated/directives/abstract_control_directive.ts\", \"comment_position\": 10, \"comment_original_position\": 10, \"comment_commit_id\": \"ee8e802b7bc25eafbe54109b3924e9dbc36dce11\", \"comment_original_commit_id\": \"ee8e802b7bc25eafbe54109b3924e9dbc36dce11\", \"diff_line_content\": \"}\\n\", \"diff_line_type\": \"added\", \"diff_line_source_no\": null, \"diff_line_target_no\": 15, \"diff_hunk_header\": \"\", \"diff\": \"@@ -6,10 +6,13 @@\\n  * found in the LICENSE file at https://angular.io/license\\n  */\\n \\n-import {unimplemented} from '../../facade/exceptions';\\n+import {BaseException} from '@angular/core';\\n import {isPresent} from '../../facade/lang';\\n import {AbstractControl} from '../model';\\n \\n+function unimplemented(): any {\\n+  throw new BaseException('unimplemented');\", \"side\": \"RIGHT\", \"line_offset\": 9, \"diff_file_source\": \"a/modules/@angular/common/src/forms-deprecated/directives/abstract_control_directive.ts\", \"diff_file_target\": \"b/modules/@angular/common/src/forms-deprecated/directives/abstract_control_directive.ts\", \"repo\": \"angular/angular\"}\n",
      "    {\"comment_id\": 74523490, \"comment_user_login\": \"vicb\", \"comment_body\": \"const\\n\", \"comment_created_at\": \"2016-08-11T23:43:20+00:00\", \"comment_html_url\": \"https://github.com/angular/angular/pull/10622#discussion_r74523490\", \"comment_path\": \"modules/@angular/compiler-cli/integrationtest/test/basic_spec.ts\", \"comment_position\": 36, \"comment_original_position\": 36, \"comment_commit_id\": \"616c5c56924b77bc0ef539d01d1cf3596c0da87f\", \"comment_original_commit_id\": \"91f50b2f513f99cf56b7406b3a74066e128b9c7d\", \"diff_line_content\": \"    it('should support i18n for content tags', () => {\\n\", \"diff_line_type\": \"added\", \"diff_line_source_no\": null, \"diff_line_target_no\": 79, \"diff_hunk_header\": \"describe('template codegen output', () => {\", \"diff\": \"@@ -44,24 +44,40 @@ describe('template codegen output', () => {\\n   it('should support ngIf', () => {\\n     var compFixture = createComponent(BasicComp);\\n     var debugElement = compFixture.debugElement;\\n-    expect(debugElement.children.length).toBe(2);\\n+    expect(debugElement.children.length).toBe(3);\\n \\n     compFixture.componentInstance.ctxBool = true;\\n     compFixture.detectChanges();\\n-    expect(debugElement.children.length).toBe(3);\\n+    expect(debugElement.children.length).toBe(4);\\n     expect(debugElement.children[2].injector.get(MultipleComponentsMyComp)).toBeTruthy();\\n   });\\n \\n   it('should support ngFor', () => {\\n     var compFixture = createComponent(BasicComp);\\n     var debugElement = compFixture.debugElement;\\n-    expect(debugElement.children.length).toBe(2);\\n+    expect(debugElement.children.length).toBe(3);\\n \\n     // test NgFor\\n     compFixture.componentInstance.ctxArr = [1, 2];\\n     compFixture.detectChanges();\\n-    expect(debugElement.children.length).toBe(4);\\n+    expect(debugElement.children.length).toBe(5);\\n     expect(debugElement.children[2].attributes['value']).toBe('1');\\n     expect(debugElement.children[3].attributes['value']).toBe('2');\\n   });\\n+\\n+  describe('i18n', () => {\\n+    it('should support i18n for content tags', () => {\\n+      var compFixture = createComponent(BasicComp);\\n+      expect(compFixture.componentInstance.ctxLocale).toEqual('FI');\\n+    });\\n+\\n+    it('should support i18n for content tags', () => {\\n+      var compFixture = createComponent(BasicComp);\", \"side\": \"RIGHT\", \"line_offset\": 35, \"diff_file_source\": \"a/modules/@angular/compiler-cli/integrationtest/test/basic_spec.ts\", \"diff_file_target\": \"b/modules/@angular/compiler-cli/integrationtest/test/basic_spec.ts\", \"repo\": \"angular/angular\"}\n",
      "    {\"comment_id\": 74523504, \"comment_user_login\": \"vicb\", \"comment_body\": \"const\\n\", \"comment_created_at\": \"2016-08-11T23:43:27+00:00\", \"comment_html_url\": \"https://github.com/angular/angular/pull/10622#discussion_r74523504\", \"comment_path\": \"modules/@angular/compiler-cli/integrationtest/test/basic_spec.ts\", \"comment_position\": 31, \"comment_original_position\": 31, \"comment_commit_id\": \"616c5c56924b77bc0ef539d01d1cf3596c0da87f\", \"comment_original_commit_id\": \"91f50b2f513f99cf56b7406b3a74066e128b9c7d\", \"diff_line_content\": \"    it('should inject the translations format into the component', () => {\\n\", \"diff_line_type\": \"added\", \"diff_line_source_no\": null, \"diff_line_target_no\": 74, \"diff_hunk_header\": \"describe('template codegen output', () => {\", \"diff\": \"@@ -44,24 +44,40 @@ describe('template codegen output', () => {\\n   it('should support ngIf', () => {\\n     var compFixture = createComponent(BasicComp);\\n     var debugElement = compFixture.debugElement;\\n-    expect(debugElement.children.length).toBe(2);\\n+    expect(debugElement.children.length).toBe(3);\\n \\n     compFixture.componentInstance.ctxBool = true;\\n     compFixture.detectChanges();\\n-    expect(debugElement.children.length).toBe(3);\\n+    expect(debugElement.children.length).toBe(4);\\n     expect(debugElement.children[2].injector.get(MultipleComponentsMyComp)).toBeTruthy();\\n   });\\n \\n   it('should support ngFor', () => {\\n     var compFixture = createComponent(BasicComp);\\n     var debugElement = compFixture.debugElement;\\n-    expect(debugElement.children.length).toBe(2);\\n+    expect(debugElement.children.length).toBe(3);\\n \\n     // test NgFor\\n     compFixture.componentInstance.ctxArr = [1, 2];\\n     compFixture.detectChanges();\\n-    expect(debugElement.children.length).toBe(4);\\n+    expect(debugElement.children.length).toBe(5);\\n     expect(debugElement.children[2].attributes['value']).toBe('1');\\n     expect(debugElement.children[3].attributes['value']).toBe('2');\\n   });\\n+\\n+  describe('i18n', () => {\\n+    it('should support i18n for content tags', () => {\\n+      var compFixture = createComponent(BasicComp);\", \"side\": \"RIGHT\", \"line_offset\": 30, \"diff_file_source\": \"a/modules/@angular/compiler-cli/integrationtest/test/basic_spec.ts\", \"diff_file_target\": \"b/modules/@angular/compiler-cli/integrationtest/test/basic_spec.ts\", \"repo\": \"angular/angular\"}\n",
      "    {\"comment_id\": 74526501, \"comment_user_login\": \"vicb\", \"comment_body\": \"fi\\n\", \"comment_created_at\": \"2016-08-12T00:18:35+00:00\", \"comment_html_url\": \"https://github.com/angular/angular/pull/10622#discussion_r74526501\", \"comment_path\": \"modules/@angular/compiler-cli/integrationtest/test/basic_spec.ts\", \"comment_position\": 32, \"comment_original_position\": 32, \"comment_commit_id\": \"616c5c56924b77bc0ef539d01d1cf3596c0da87f\", \"comment_original_commit_id\": \"7333b4c6d9527a1a6b812ab6eea2797f228d03bd\", \"diff_line_content\": \"      const compFixture = createComponent(BasicComp);\\n\", \"diff_line_type\": \"added\", \"diff_line_source_no\": null, \"diff_line_target_no\": 75, \"diff_hunk_header\": \"describe('template codegen output', () => {\", \"diff\": \"@@ -44,24 +44,45 @@ describe('template codegen output', () => {\\n   it('should support ngIf', () => {\\n     var compFixture = createComponent(BasicComp);\\n     var debugElement = compFixture.debugElement;\\n-    expect(debugElement.children.length).toBe(2);\\n+    expect(debugElement.children.length).toBe(3);\\n \\n     compFixture.componentInstance.ctxBool = true;\\n     compFixture.detectChanges();\\n-    expect(debugElement.children.length).toBe(3);\\n+    expect(debugElement.children.length).toBe(4);\\n     expect(debugElement.children[2].injector.get(MultipleComponentsMyComp)).toBeTruthy();\\n   });\\n \\n   it('should support ngFor', () => {\\n     var compFixture = createComponent(BasicComp);\\n     var debugElement = compFixture.debugElement;\\n-    expect(debugElement.children.length).toBe(2);\\n+    expect(debugElement.children.length).toBe(3);\\n \\n     // test NgFor\\n     compFixture.componentInstance.ctxArr = [1, 2];\\n     compFixture.detectChanges();\\n-    expect(debugElement.children.length).toBe(4);\\n+    expect(debugElement.children.length).toBe(5);\\n     expect(debugElement.children[2].attributes['value']).toBe('1');\\n     expect(debugElement.children[3].attributes['value']).toBe('2');\\n   });\\n+\\n+  describe('i18n', () => {\\n+    it('should inject the locale into the component', () => {\\n+      const compFixture = createComponent(BasicComp);\\n+      expect(compFixture.componentInstance.ctxLocale).toEqual('FI');\", \"side\": \"RIGHT\", \"line_offset\": 31, \"diff_file_source\": \"a/modules/@angular/compiler-cli/integrationtest/test/basic_spec.ts\", \"diff_file_target\": \"b/modules/@angular/compiler-cli/integrationtest/test/basic_spec.ts\", \"repo\": \"angular/angular\"}\n",
      "    {\"comment_id\": 74526531, \"comment_user_login\": \"vicb\", \"comment_body\": \"remove !\\n\", \"comment_created_at\": \"2016-08-12T00:19:06+00:00\", \"comment_html_url\": \"https://github.com/angular/angular/pull/10622#discussion_r74526531\", \"comment_path\": \"modules/@angular/compiler-cli/src/codegen.ts\", \"comment_position\": 36, \"comment_original_position\": 36, \"comment_commit_id\": \"616c5c56924b77bc0ef539d01d1cf3596c0da87f\", \"comment_original_commit_id\": \"7333b4c6d9527a1a6b812ab6eea2797f228d03bd\", \"diff_line_content\": \"    const reflectorHost = new ReflectorHost(program, compilerHost, options, reflectorHostContext);\\n\", \"diff_line_type\": \"context\", \"diff_line_source_no\": 132, \"diff_line_target_no\": 142, \"diff_hunk_header\": \"export class CodeGenerator {\", \"diff\": \"@@ -88,6 +93,35 @@ export class CodeGenerator {\\n     return path.join(this.options.genDir, relativePath);\\n   }\\n \\n+  private _constructTranslationBundle(fileMetas: any[], analyzedNgModules: any): Promise<any> {\", \"side\": \"RIGHT\", \"line_offset\": 11, \"diff_file_source\": \"a/modules/@angular/compiler-cli/src/codegen.ts\", \"diff_file_target\": \"b/modules/@angular/compiler-cli/src/codegen.ts\", \"repo\": \"angular/angular\"}\n",
      "    {\"comment_id\": 74526731, \"comment_user_login\": \"vicb\", \"comment_body\": \"append at the end to make diff readable\\n\", \"comment_created_at\": \"2016-08-12T00:21:49+00:00\", \"comment_html_url\": \"https://github.com/angular/angular/pull/10622#discussion_r74526731\", \"comment_path\": \"modules/@angular/compiler-cli/src/codegen.ts\", \"comment_position\": 26, \"comment_original_position\": 26, \"comment_commit_id\": \"616c5c56924b77bc0ef539d01d1cf3596c0da87f\", \"comment_original_commit_id\": \"7333b4c6d9527a1a6b812ab6eea2797f228d03bd\", \"diff_line_content\": \"      }\\n\", \"diff_line_type\": \"context\", \"diff_line_source_no\": 129, \"diff_line_target_no\": 132, \"diff_hunk_header\": \"export class CodeGenerator {\", \"diff\": \"@@ -32,7 +35,9 @@ const PREAMBLE = `/**\\n \\n export class CodeGenerator {\\n   constructor(\\n-      private options: AngularCompilerOptions, private program: ts.Program,\\n+      private options: AngularCompilerOptions, private transContent: string,\", \"side\": \"RIGHT\", \"line_offset\": 1, \"diff_file_source\": \"a/modules/@angular/compiler-cli/src/codegen.ts\", \"diff_file_target\": \"b/modules/@angular/compiler-cli/src/codegen.ts\", \"repo\": \"angular/angular\"}\n",
      "    {\"comment_id\": 74527376, \"comment_user_login\": \"vicb\", \"comment_body\": \"This is the only line that you should keep beside loading the file\\n\", \"comment_created_at\": \"2016-08-12T00:30:39+00:00\", \"comment_html_url\": \"https://github.com/angular/angular/pull/10622#discussion_r74527376\", \"comment_path\": \"modules/@angular/compiler-cli/src/codegen.ts\", \"comment_position\": 46, \"comment_original_position\": 130, \"comment_commit_id\": \"616c5c56924b77bc0ef539d01d1cf3596c0da87f\", \"comment_original_commit_id\": \"7333b4c6d9527a1a6b812ab6eea2797f228d03bd\", \"diff_line_content\": \"        new NgModuleCompiler(), new TypeScriptEmitter(reflectorHost), cliOptions.locale,\\n\", \"diff_line_type\": \"added\", \"diff_line_source_no\": null, \"diff_line_target_no\": 164, \"diff_hunk_header\": \"export class CodeGenerator {\", \"diff\": \"@@ -132,7 +175,7 @@ export class CodeGenerator {\\n     const reflectorHost = new ReflectorHost(program, compilerHost, options, reflectorHostContext);\\n     const staticReflector = new StaticReflector(reflectorHost);\\n     StaticAndDynamicReflectionCapabilities.install(staticReflector);\\n-    const htmlParser = new compiler.i18n.HtmlParser(new HtmlParser());\\n+    const htmlParser = new compiler.i18n.HtmlParser(new HtmlParser(), transContent);\", \"side\": \"RIGHT\", \"line_offset\": 3, \"diff_file_source\": \"a/modules/@angular/compiler-cli/src/codegen.ts\", \"diff_file_target\": \"b/modules/@angular/compiler-cli/src/codegen.ts\", \"repo\": \"angular/angular\"}\n",
      "    {\"comment_id\": 74528827, \"comment_user_login\": \"vicb\", \"comment_body\": \"Id\\n\", \"comment_created_at\": \"2016-08-12T00:51:47+00:00\", \"comment_html_url\": \"https://github.com/angular/angular/pull/10622#discussion_r74528827\", \"comment_path\": \"modules/@angular/compiler-cli/integrationtest/src/basic.ts\", \"comment_position\": 15, \"comment_original_position\": 15, \"comment_commit_id\": \"616c5c56924b77bc0ef539d01d1cf3596c0da87f\", \"comment_original_commit_id\": \"e7940fc0e83c9d0a549891774db0a609ec591c19\", \"diff_line_content\": \"  }\\n\", \"diff_line_type\": \"added\", \"diff_line_source_no\": null, \"diff_line_target_no\": 30, \"diff_hunk_header\": \"export class BasicComp {\", \"diff\": \"@@ -23,5 +23,9 @@ export class BasicComp {\\n   ctxProp: string;\\n   ctxBool: boolean;\\n   ctxArr: any[] = [];\\n-  constructor() { this.ctxProp = 'initialValue'; }\\n+  constructor(\\n+      @Inject(LOCALE_ID) public localeID: string,\", \"side\": \"RIGHT\", \"line_offset\": 7, \"diff_file_source\": \"a/modules/@angular/compiler-cli/integrationtest/src/basic.ts\", \"diff_file_target\": \"b/modules/@angular/compiler-cli/integrationtest/src/basic.ts\", \"repo\": \"angular/angular\"}\n",
      "    {\"comment_id\": 74528853, \"comment_user_login\": \"vicb\", \"comment_body\": \"only add the format\\n\", \"comment_created_at\": \"2016-08-12T00:52:10+00:00\", \"comment_html_url\": \"https://github.com/angular/angular/pull/10622#discussion_r74528853\", \"comment_path\": \"modules/@angular/compiler-cli/src/extract_i18n.ts\", \"comment_position\": 6, \"comment_original_position\": 6, \"comment_commit_id\": \"616c5c56924b77bc0ef539d01d1cf3596c0da87f\", \"comment_original_commit_id\": \"e7940fc0e83c9d0a549891774db0a609ec591c19\", \"diff_line_content\": \"  const extractor = Extractor.create(ngOptions, cliOptions.i18nFormat, program, host);\\n\", \"diff_line_type\": \"added\", \"diff_line_source_no\": null, \"diff_line_target_no\": 33, \"diff_hunk_header\": \"import {StaticAndDynamicReflectionCapabilities} from './static_reflection_capabi\", \"diff\": \"@@ -28,8 +28,9 @@ import {StaticAndDynamicReflectionCapabilities} from './static_reflection_capabi\\n import {StaticReflector, StaticSymbol} from './static_reflector';\\n \\n function extract(\\n-    ngOptions: tsc.AngularCompilerOptions, program: ts.Program, host: ts.CompilerHost) {\\n-  const extractor = Extractor.create(ngOptions, program, host);\\n+    ngOptions: tsc.AngularCompilerOptions, cliOptions: tsc.CliOptions, program: ts.Program,\", \"side\": \"RIGHT\", \"line_offset\": 5, \"diff_file_source\": \"a/modules/@angular/compiler-cli/src/extract_i18n.ts\", \"diff_file_target\": \"b/modules/@angular/compiler-cli/src/extract_i18n.ts\", \"repo\": \"angular/angular\"}\n",
      "    {\"comment_id\": 74530724, \"comment_user_login\": \"vicb\", \"comment_body\": \"`, null, null` would be more semantically correct\\n\", \"comment_created_at\": \"2016-08-12T01:22:30+00:00\", \"comment_html_url\": \"https://github.com/angular/angular/pull/10622#discussion_r74530724\", \"comment_path\": \"modules/@angular/compiler-cli/src/extract_i18n.ts\", \"comment_position\": 37, \"comment_original_position\": 37, \"comment_commit_id\": \"616c5c56924b77bc0ef539d01d1cf3596c0da87f\", \"comment_original_commit_id\": \"6781377043d6eae20d6504b83a121ebde353f043\", \"diff_line_content\": \"}\\n\", \"diff_line_type\": \"added\", \"diff_line_source_no\": null, \"diff_line_target_no\": 196, \"diff_hunk_header\": \"interface FileMetadata {\", \"diff\": \"@@ -163,7 +164,7 @@ export class Extractor {\\n         config, console, elementSchemaRegistry, staticReflector);\\n     const offlineCompiler = new compiler.OfflineCompiler(\\n         resolver, normalizer, tmplParser, new StyleCompiler(urlResolver), new ViewCompiler(config),\\n-        new NgModuleCompiler(), new TypeScriptEmitter(reflectorHost));\\n+        new NgModuleCompiler(), new TypeScriptEmitter(reflectorHost), '', translationsFormat);\", \"side\": \"RIGHT\", \"line_offset\": 12, \"diff_file_source\": \"a/modules/@angular/compiler-cli/src/extract_i18n.ts\", \"diff_file_target\": \"b/modules/@angular/compiler-cli/src/extract_i18n.ts\", \"repo\": \"angular/angular\"}\n"
     ]
    }
   ],
   "source": [
    "train = \"/mnt/object_group/data/processed/train.jsonl.gz\"\n",
    "with gzip.open(train, 'rt', encoding='utf-8') as f:\n",
    "    for i, line in enumerate(f):\n",
    "        print(\"    \" + line.strip())\n",
    "        if i >= 10:\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "86df7928-62f7-4ac6-8f93-e08ce6a4b75a",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_ROOT = Path(\"/mnt/object_group/data/processed\")  # Adjust as needed\n",
    "\n",
    "def load_samples(limit_per_split=10):\n",
    "    diff_samples = []\n",
    "    for repo_dir in DATA_ROOT.iterdir():\n",
    "        if not repo_dir.is_dir():\n",
    "            continue\n",
    "        for pr_dir in (repo_dir / \"diff\").iterdir():\n",
    "            pr_id = pr_dir.name\n",
    "            diff_file = pr_dir\n",
    "            comments_file = DATA_ROOT / repo_dir.name / \"comments\" / f\"{repo_dir.name}_{pr_id}_comments.jsonl\"\n",
    "\n",
    "            if not diff_file.exists() or not comments_file.exists():\n",
    "                continue\n",
    "\n",
    "            with open(diff_file, \"r\", encoding=\"utf-8\") as df:\n",
    "                diff_content = df.read()\n",
    "\n",
    "            with open(comments_file, \"r\", encoding=\"utf-8\") as cf:\n",
    "                for line in cf:\n",
    "                    try:\n",
    "                        comment = json.loads(line)\n",
    "                        offset = comment.get(\"original_position\")\n",
    "                        side = comment.get(\"side\", \"RIGHT\")\n",
    "                        body = comment.get(\"body\", \"\").strip()\n",
    "                        if offset is not None and body:\n",
    "                            sample = {\n",
    "                                \"input\": f\"<DIFF>\\n{diff_content}\\n</DIFF>\\n<COMMENT side=\\\"{side}\\\" offset=\\\"{offset}\\\">\",\n",
    "                                \"output\": body\n",
    "                            }\n",
    "                            diff_samples.append(sample)\n",
    "                    except json.JSONDecodeError:\n",
    "                        continue\n",
    "\n",
    "    random.shuffle(diff_samples)\n",
    "    return {\n",
    "        \"train\": diff_samples[:limit_per_split],\n",
    "        \"test\": diff_samples[limit_per_split:2*limit_per_split],\n",
    "        \"eval\": diff_samples[2*limit_per_split:3*limit_per_split],\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c70b5a55-a831-4432-ae15-5b910735780f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_jsonl_gz(file_path, limit):\n",
    "    samples = []\n",
    "    with gzip.open(file_path, 'rt', encoding='utf-8') as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if i >= limit:\n",
    "                break\n",
    "            entry = json.loads(line)\n",
    "            prompt = f\"<DIFF>\\n{entry['diff']}\\n</DIFF>\\n\"\n",
    "            comment = f\"<COMMENT side=\\\"{entry['side']}\\\" offset=\\\"{entry['line_offset']}\\\">{entry['comment_body']}\"\n",
    "            samples.append({'input': prompt, 'label': comment})\n",
    "    return samples\n",
    "\n",
    "train_data = load_jsonl_gz(\"/mnt/object_group/data/processed/train.jsonl.gz\", 10)\n",
    "test_data  = load_jsonl_gz(\"/mnt/object_group/data/processed/test.jsonl.gz\", 8)\n",
    "eval_data  = load_jsonl_gz(\"/mnt/object_group/data/processed/val.jsonl.gz\", 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a0fb7e8b-f409-4557-b4e6-61fafc226c9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input', 'label'])\n",
      "<DIFF>\n",
      "@@ -6,10 +6,13 @@\n",
      "  * found in the LICENSE file at https://angular.io/license\n",
      "  */\n",
      " \n",
      "-import {unimplemented} from '../../facade/exceptions';\n",
      "+import {BaseException} from '@angular/core';\n",
      " import {isPresent} from '../../facade/lang';\n",
      " import {AbstractControl} from '../model';\n",
      " \n",
      "+function unimplemented(): any {\n",
      "+  throw new BaseException('unimplemented');\n",
      "</DIFF>\n",
      "\n",
      "<COMMENT side=\"RIGHT\" offset=\"9\">Is there a better way to avoid having this in different places?\n",
      "\n",
      "<DIFF>\n",
      "@@ -6,10 +6,13 @@\n",
      "  * found in the LICENSE file at https://angular.io/license\n",
      "  */\n",
      " \n",
      "-import {unimplemented} from '../../facade/exceptions';\n",
      "+import {BaseException} from '@angular/core';\n",
      " import {isPresent} from '../../facade/lang';\n",
      " import {AbstractControl} from '../model';\n",
      " \n",
      "+function unimplemented(): any {\n",
      "+  throw new BaseException('unimplemented');\n",
      "</DIFF>\n",
      "\n",
      "<COMMENT side=\"RIGHT\" offset=\"9\">we could export it in the public API but that's not nice.\n",
      "Or we could inline the method and delete it.\n",
      "\n",
      "<DIFF>\n",
      "@@ -44,24 +44,40 @@ describe('template codegen output', () => {\n",
      "   it('should support ngIf', () => {\n",
      "     var compFixture = createComponent(BasicComp);\n",
      "     var debugElement = compFixture.debugElement;\n",
      "-    expect(debugElement.children.length).toBe(2);\n",
      "+    expect(debugElement.children.length).toBe(3);\n",
      " \n",
      "     compFixture.componentInstance.ctxBool = true;\n",
      "     compFixture.detectChanges();\n",
      "-    expect(debugElement.children.length).toBe(3);\n",
      "+    expect(debugElement.children.length).toBe(4);\n",
      "     expect(debugElement.children[2].injector.get(MultipleComponentsMyComp)).toBeTruthy();\n",
      "   });\n",
      " \n",
      "   it('should support ngFor', () => {\n",
      "     var compFixture = createComponent(BasicComp);\n",
      "     var debugElement = compFixture.debugElement;\n",
      "-    expect(debugElement.children.length).toBe(2);\n",
      "+    expect(debugElement.children.length).toBe(3);\n",
      " \n",
      "     // test NgFor\n",
      "     compFixture.componentInstance.ctxArr = [1, 2];\n",
      "     compFixture.detectChanges();\n",
      "-    expect(debugElement.children.length).toBe(4);\n",
      "+    expect(debugElement.children.length).toBe(5);\n",
      "     expect(debugElement.children[2].attributes['value']).toBe('1');\n",
      "     expect(debugElement.children[3].attributes['value']).toBe('2');\n",
      "   });\n",
      "+\n",
      "+  describe('i18n', () => {\n",
      "+    it('should support i18n for content tags', () => {\n",
      "+      var compFixture = createComponent(BasicComp);\n",
      "+      expect(compFixture.componentInstance.ctxLocale).toEqual('FI');\n",
      "+    });\n",
      "+\n",
      "+    it('should support i18n for content tags', () => {\n",
      "+      var compFixture = createComponent(BasicComp);\n",
      "</DIFF>\n",
      "\n",
      "<COMMENT side=\"RIGHT\" offset=\"35\">const\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(train_data[0].keys())\n",
    "\n",
    "for i in range(3):\n",
    "    print(train_data[i][\"input\"])\n",
    "    print(train_data[i][\"label\"])\n",
    "# print(train_data[0][\"input\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4009389d-1604-4d45-8b29-356c1eb50ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt template\n",
    "instruction = (\n",
    "    \"You are a helpful code reviewer. \"\n",
    "    \"Given a code diff, generate all relevant review comments. \"\n",
    "    \"Each comment must be in the format: \"\n",
    "    \"<COMMENT side=\\\"RIGHT\\\" offset=\\\"X\\\">Your comment here.\\n\"\n",
    "    \"Only output comments, nothing else.\\n\\n\"\n",
    "    \"### Code Diff:\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5eb6f797-96f7-44ad-b438-ed3c12e850ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_comments(text):\n",
    "    pattern = re.compile(r'<COMMENT side=\"(?P<side>[^\"]+)\" offset=\"(?P<offset>\\d+)\">(?P<comment>.+)')\n",
    "    results = []\n",
    "    for line in text.strip().splitlines():\n",
    "        match = pattern.match(line.strip())\n",
    "        if match:\n",
    "            results.append({\n",
    "                \"side\": match.group(\"side\"),\n",
    "                \"offset\": int(match.group(\"offset\")),\n",
    "                \"comment\": match.group(\"comment\").strip()\n",
    "            })\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "bad0089a-5b53-4620-9ee6-2cb46efef9a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32016, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32016, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ec15c265-2cbe-4477-b5d4-5661257c702d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def offset_match(pred_comments, true_comments, window=3):\n",
    "    correct = 0\n",
    "    total = len(pred_comments)\n",
    "\n",
    "    gt_pairs = {(c[\"side\"], c[\"offset\"]) for c in true_comments}\n",
    "    matched = set()\n",
    "\n",
    "    for pred in pred_comments:\n",
    "        pred_side, pred_offset = pred[\"side\"], pred[\"offset\"]\n",
    "        for gt in gt_pairs:\n",
    "            gt_side, gt_offset = gt\n",
    "            if pred_side == gt_side and abs(pred_offset - gt_offset) <= window:\n",
    "                matched.add(gt)\n",
    "                correct += 1\n",
    "                break  # count each pred only once\n",
    "\n",
    "    return correct, total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e72a6268-8a4e-4601-ae46-37fea7b358fd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Sample 0 ===\n",
      "[INST] You are a helpful code reviewer. Given a code diff, generate all relevant review comments. Each comment must be in the format: <COMMENT side=\"RIGHT\" offset=\"X\">Your comment here.\n",
      "Only output comments, nothing else.\n",
      "\n",
      "### Code Diff:\n",
      "<DIFF>\n",
      "@@ -838,45 +839,58 @@ function performConcurrentWorkOnRoot(root, didTimeout) {\n",
      "       throw fatalError;\n",
      "     }\n",
      " \n",
      "-    // Check if this render may have yielded to a concurrent event, and if so,\n",
      "-    // confirm that any newly rendered stores are consistent.\n",
      "-    // TODO: It's possible that even a concurrent render may never have yielded\n",
      "-    // to the main thread, if it was fast enough, or if it expired. We could\n",
      "-    // skip the consistency check in that case, too.\n",
      "-    const renderWasConcurrent = !includesBlockingLane(root, lanes);\n",
      "-    const finishedWork: Fiber = (root.current.alternate: any);\n",
      "-    if (\n",
      "-      renderWasConcurrent &&\n",
      "-      !isRenderConsistentWithExternalStores(finishedWork)\n",
      "-    ) {\n",
      "-      // A store was mutated in an interleaved event. Render again,\n",
      "-      // synchronously, to block further mutations.\n",
      "-      exitStatus = renderRootSync(root, lanes);\n",
      "-\n",
      "-      // We need to check again if something threw\n",
      "-      if (exitStatus === RootErrored) {\n",
      "-        const errorRetryLanes = getLanesToRetrySynchronouslyOnError(root);\n",
      "-        if (errorRetryLanes !== NoLanes) {\n",
      "-          lanes = errorRetryLanes;\n",
      "-          exitStatus = recoverFromConcurrentError(root, errorRetryLanes);\n",
      "-          // We assume the tree is now consistent because we didn't yield to any\n",
      "-          // concurrent events.\n",
      "+    if (exitStatus === RootDidNotComplete) {\n",
      "</DIFF> [/INST]  <COMMENT side=\"RIGHT\" offset=\"838\">This code looks like it's checking if a render was concurrent and if it was consistent with external stores. It's also checking if the render was interrupted by an error and if so, it's retrying the render synchronously.</COMMENT>\n",
      "\n",
      "<COMMENT side=\"RIGHT\" offset=\"840\">It's possible that even a concurrent render may never have yielded to the main thread, if it was fast enough, or if it expired. We could skip the consistency check in that case, too.</COMMENT>\n",
      "\n",
      "<COMMENT side=\"RIGHT\" offset=\"842\">The code is checking if the render was concurrent and if it was consistent with external stores. If it was not consistent, it's retrying the render synchronously.</COMMENT>\n",
      "\n",
      "<COMMENT side=\"RIGHT\" offset=\"844\">It's assuming that the tree is now consistent because it didn't yield to any concurrent events.</COMMENT>\n",
      "\n",
      "<COMMENT side=\"RIGHT\" offset=\"846\">It's checking if the\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Sample 1 ===\n",
      "[INST] You are a helpful code reviewer. Given a code diff, generate all relevant review comments. Each comment must be in the format: <COMMENT side=\"RIGHT\" offset=\"X\">Your comment here.\n",
      "Only output comments, nothing else.\n",
      "\n",
      "### Code Diff:\n",
      "<DIFF>\n",
      "@@ -838,45 +839,58 @@ function performConcurrentWorkOnRoot(root, didTimeout) {\n",
      "       throw fatalError;\n",
      "     }\n",
      " \n",
      "-    // Check if this render may have yielded to a concurrent event, and if so,\n",
      "-    // confirm that any newly rendered stores are consistent.\n",
      "-    // TODO: It's possible that even a concurrent render may never have yielded\n",
      "-    // to the main thread, if it was fast enough, or if it expired. We could\n",
      "-    // skip the consistency check in that case, too.\n",
      "-    const renderWasConcurrent = !includesBlockingLane(root, lanes);\n",
      "-    const finishedWork: Fiber = (root.current.alternate: any);\n",
      "-    if (\n",
      "-      renderWasConcurrent &&\n",
      "-      !isRenderConsistentWithExternalStores(finishedWork)\n",
      "-    ) {\n",
      "-      // A store was mutated in an interleaved event. Render again,\n",
      "-      // synchronously, to block further mutations.\n",
      "-      exitStatus = renderRootSync(root, lanes);\n",
      "-\n",
      "-      // We need to check again if something threw\n",
      "-      if (exitStatus === RootErrored) {\n",
      "-        const errorRetryLanes = getLanesToRetrySynchronouslyOnError(root);\n",
      "-        if (errorRetryLanes !== NoLanes) {\n",
      "-          lanes = errorRetryLanes;\n",
      "-          exitStatus = recoverFromConcurrentError(root, errorRetryLanes);\n",
      "-          // We assume the tree is now consistent because we didn't yield to any\n",
      "-          // concurrent events.\n",
      "+    if (exitStatus === RootDidNotComplete) {\n",
      "+      // The render unwound without completing the tree. This happens in special\n",
      "+      // cases where need to exit the current render without producing a\n",
      "+      // consistent tree or committing.\n",
      "+      //\n",
      "+      // This should only happen during a concurrent render, not a discrete or\n",
      "+      // synchronous update. We should have already checked for this when we\n",
      "+      // unwound the stack.\n",
      "</DIFF> [/INST]  <COMMENT side=\"RIGHT\" offset=\"838\">This code looks like it's checking if a render was concurrent and if it was consistent with external stores. However, it's not clear why this check is necessary or what the implications of this check are.</COMMENT>\n",
      "\n",
      "<COMMENT side=\"RIGHT\" offset=\"845\">This code is checking if a render was concurrent and if it was consistent with external stores. If the render was concurrent and the stores were not consistent, it will re-render the tree synchronously to block further mutations.</COMMENT>\n",
      "\n",
      "<COMMENT side=\"RIGHT\" offset=\"853\">This code is checking if a render was concurrent and if it was consistent with external stores. If the render was concurrent and the stores were not consistent, it will re-render the tree synchronously to block further mutations. However, it's not clear why this check is necessary or what the implications of this check are.</COMMENT>\n",
      "\n",
      "<COMMENT side=\"RIGHT\" offset=\"861\">This code is checking if a render was concurrent and if it was consistent with external stores. If the render\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Sample 2 ===\n",
      "[INST] You are a helpful code reviewer. Given a code diff, generate all relevant review comments. Each comment must be in the format: <COMMENT side=\"RIGHT\" offset=\"X\">Your comment here.\n",
      "Only output comments, nothing else.\n",
      "\n",
      "### Code Diff:\n",
      "<DIFF>\n",
      "@@ -1149,6 +1163,10 @@ function performSyncWorkOnRoot(root) {\n",
      "     throw fatalError;\n",
      "   }\n",
      " \n",
      "+  if (exitStatus === RootDidNotComplete) {\n",
      "</DIFF> [/INST]  <COMMENT side=\"RIGHT\" offset=\"1150\">This line is not needed. The `throw` statement will already cause the function to exit with an error. It's not necessary to check the `exitStatus` variable and throw a new error if it's not `RootCompleted`.</COMMENT>\n",
      "\n",
      "<COMMENT side=\"RIGHT\" offset=\"1164\">This line is not needed. The `if` statement is already checking if the `exitStatus` variable is equal to `RootDidNotComplete`. It's not necessary to check the `exitStatus` variable again in the `if` statement's body.</COMMENT>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Sample 3 ===\n",
      "[INST] You are a helpful code reviewer. Given a code diff, generate all relevant review comments. Each comment must be in the format: <COMMENT side=\"RIGHT\" offset=\"X\">Your comment here.\n",
      "Only output comments, nothing else.\n",
      "\n",
      "### Code Diff:\n",
      "<DIFF>\n",
      "@@ -739,14 +739,20 @@ describe('ReactCache', () => {\n",
      "     await act(async () => {\n",
      "       refresh();\n",
      "     });\n",
      "-    expect(Scheduler).toHaveYielded(['Cache miss! [A]', 'Loading...']);\n",
      "+    expect(Scheduler).toHaveYielded([\n",
      "+      'Cache miss! [A]',\n",
      "+      'Loading...',\n",
      "+      // TODO: This happens too early, because we don't retain the refreshed\n",
      "+      // cache until it commits. Will fix in next step.\n",
      "+      'Cache cleanup: A [v1]',\n",
      "</DIFF> [/INST]  <COMMENT side=\"RIGHT\" offset=\"10\">This comment is suggesting that the cache cleanup is happening too early, before the refreshed cache has committed. This could be fixed by waiting for the cache to commit before cleaning up.</COMMENT>\n",
      "\n",
      "<COMMENT side=\"RIGHT\" offset=\"15\">This comment is suggesting that the cache cleanup is happening too early, before the refreshed cache has committed. This could be fixed by waiting for the cache to commit before cleaning up.</COMMENT>\n",
      "\n",
      "<COMMENT side=\"RIGHT\" offset=\"18\">This comment is suggesting that the cache cleanup is happening too early, before the refreshed cache has committed. This could be fixed by waiting for the cache to commit before cleaning up.</COMMENT>\n",
      "\n",
      "<COMMENT side=\"RIGHT\" offset=\"21\">This comment is suggesting that the cache cleanup is happening too early, before the refreshed cache has committed. This could be fixed by waiting for the cache to commit before cleaning up.</COMMENT>\n",
      "\n",
      "<COMMENT side=\"RIGHT\" offset=\"24\">This comment is suggesting that the cache cleanup\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Sample 4 ===\n",
      "[INST] You are a helpful code reviewer. Given a code diff, generate all relevant review comments. Each comment must be in the format: <COMMENT side=\"RIGHT\" offset=\"X\">Your comment here.\n",
      "Only output comments, nothing else.\n",
      "\n",
      "### Code Diff:\n",
      "<DIFF>\n",
      "@@ -739,14 +739,20 @@ describe('ReactCache', () => {\n",
      "     await act(async () => {\n",
      "       refresh();\n",
      "     });\n",
      "-    expect(Scheduler).toHaveYielded(['Cache miss! [A]', 'Loading...']);\n",
      "+    expect(Scheduler).toHaveYielded([\n",
      "+      'Cache miss! [A]',\n",
      "+      'Loading...',\n",
      "+      // TODO: This happens too early, because we don't retain the refreshed\n",
      "+      // cache until it commits. Will fix in next step.\n",
      "+      'Cache cleanup: A [v1]',\n",
      "</DIFF> [/INST]  <COMMENT side=\"RIGHT\" offset=\"10\">This comment is suggesting that the cache cleanup is happening too early, before the refreshed cache has committed. This could be fixed by waiting for the cache to commit before cleaning up.</COMMENT>\n",
      "\n",
      "<COMMENT side=\"RIGHT\" offset=\"15\">This comment is suggesting that the cache cleanup is happening too early, before the refreshed cache has committed. This could be fixed by waiting for the cache to commit before cleaning up.</COMMENT>\n",
      "\n",
      "<COMMENT side=\"RIGHT\" offset=\"18\">This comment is suggesting that the cache cleanup is happening too early, before the refreshed cache has committed. This could be fixed by waiting for the cache to commit before cleaning up.</COMMENT>\n",
      "\n",
      "<COMMENT side=\"RIGHT\" offset=\"21\">This comment is suggesting that the cache cleanup is happening too early, before the refreshed cache has committed. This could be fixed by waiting for the cache to commit before cleaning up.</COMMENT>\n",
      "\n",
      "<COMMENT side=\"RIGHT\" offset=\"24\">This comment is suggesting that the cache cleanup\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Sample 5 ===\n",
      "[INST] You are a helpful code reviewer. Given a code diff, generate all relevant review comments. Each comment must be in the format: <COMMENT side=\"RIGHT\" offset=\"X\">Your comment here.\n",
      "Only output comments, nothing else.\n",
      "\n",
      "### Code Diff:\n",
      "<DIFF>\n",
      "@@ -739,14 +739,20 @@ describe('ReactCache', () => {\n",
      "     await act(async () => {\n",
      "       refresh();\n",
      "     });\n",
      "-    expect(Scheduler).toHaveYielded(['Cache miss! [A]', 'Loading...']);\n",
      "+    expect(Scheduler).toHaveYielded([\n",
      "+      'Cache miss! [A]',\n",
      "+      'Loading...',\n",
      "+      // TODO: This happens too early, because we don't retain the refreshed\n",
      "+      // cache until it commits. Will fix in next step.\n",
      "+      'Cache cleanup: A [v1]',\n",
      "</DIFF> [/INST]  <COMMENT side=\"RIGHT\" offset=\"10\">This comment is suggesting that the cache cleanup is happening too early, before the refreshed cache has committed. This could be fixed by waiting for the cache to commit before cleaning up.</COMMENT>\n",
      "\n",
      "<COMMENT side=\"RIGHT\" offset=\"15\">This comment is suggesting that the cache cleanup is happening too early, before the refreshed cache has committed. This could be fixed by waiting for the cache to commit before cleaning up.</COMMENT>\n",
      "\n",
      "<COMMENT side=\"RIGHT\" offset=\"18\">This comment is suggesting that the cache cleanup is happening too early, before the refreshed cache has committed. This could be fixed by waiting for the cache to commit before cleaning up.</COMMENT>\n",
      "\n",
      "<COMMENT side=\"RIGHT\" offset=\"21\">This comment is suggesting that the cache cleanup is happening too early, before the refreshed cache has committed. This could be fixed by waiting for the cache to commit before cleaning up.</COMMENT>\n",
      "\n",
      "<COMMENT side=\"RIGHT\" offset=\"24\">This comment is suggesting that the cache cleanup\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Sample 6 ===\n",
      "[INST] You are a helpful code reviewer. Given a code diff, generate all relevant review comments. Each comment must be in the format: <COMMENT side=\"RIGHT\" offset=\"X\">Your comment here.\n",
      "Only output comments, nothing else.\n",
      "\n",
      "### Code Diff:\n",
      "<DIFF>\n",
      "@@ -0,0 +1,197 @@\n",
      "+/**\n",
      "+ * Copyright (c) Facebook, Inc. and its affiliates.\n",
      "+ *\n",
      "+ * This source code is licensed under the MIT license found in the\n",
      "+ * LICENSE file in the root directory of this source tree.\n",
      "+ *\n",
      "+ * @emails react-core\n",
      "+ */\n",
      "+\n",
      "+let JSDOM;\n",
      "+let React;\n",
      "+let ReactDOM;\n",
      "+let Scheduler;\n",
      "+let clientAct;\n",
      "+let ReactDOMFizzServer;\n",
      "+let Stream;\n",
      "+let document;\n",
      "+let writable;\n",
      "+let container;\n",
      "+let buffer = '';\n",
      "+let hasErrored = false;\n",
      "+let fatalError = undefined;\n",
      "+let textCache;\n",
      "+\n",
      "+describe('useId', () => {\n",
      "+  beforeEach(() => {\n",
      "+    jest.resetModules();\n",
      "+    JSDOM = require('jsdom').JSDOM;\n",
      "+    React = require('react');\n",
      "+    ReactDOM = require('react-dom');\n",
      "+    Scheduler = require('scheduler');\n",
      "+    clientAct = require('jest-react').act;\n",
      "+    ReactDOMFizzServer = require('react-dom/server');\n",
      "+    Stream = require('stream');\n",
      "+\n",
      "+    textCache = new Map();\n",
      "+\n",
      "+    // Test Environment\n",
      "+    const jsdom = new JSDOM(\n",
      "+      '<!DOCTYPE html><html><head></head><body><div id=\"container\">',\n",
      "+      {\n",
      "+        runScripts: 'dangerously',\n",
      "</DIFF> [/INST]  <COMMENT side=\"RIGHT\" offset=\"1\">This code looks like it's setting up a test environment for a React component. It's using JSDOM to create a virtual DOM, and it's requiring several other libraries that are used in React.</COMMENT>\n",
      "\n",
      "  <COMMENT side=\"RIGHT\" offset=\"10\">The `textCache` variable is being initialized as a new Map, which is a data structure that maps keys to values. It's not clear what this variable is used for, but it's likely that it's being used to cache some data or results.</COMMENT>\n",
      "\n",
      "  <COMMENT side=\"RIGHT\" offset=\"15\">The `describe` function is being used to define a test suite. It's not clear what this test suite is testing, but it's likely that it's testing some functionality related to the `useId` hook.</COMMENT>\n",
      "\n",
      "  <COMMENT side=\"RIGHT\" offset=\"20\">The `beforeEach` function is being used to set up some variables that are used in the test suite. It's not clear what these variables are used for\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:58<00:00,  7.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Sample 7 ===\n",
      "[INST] You are a helpful code reviewer. Given a code diff, generate all relevant review comments. Each comment must be in the format: <COMMENT side=\"RIGHT\" offset=\"X\">Your comment here.\n",
      "Only output comments, nothing else.\n",
      "\n",
      "### Code Diff:\n",
      "<DIFF>\n",
      "@@ -389,17 +389,6 @@ describe('ReactSuspense', () => {\n",
      "     expect(root).toMatchRenderedOutput('Hi');\n",
      "   });\n",
      " \n",
      "-  it('throws if tree suspends and none of the Suspense ancestors have a boundary', () => {\n",
      "</DIFF> [/INST]  <COMMENT side=\"RIGHT\" offset=\"389\">This test seems to be testing the behavior of the `ReactSuspense` component when it is used in a situation where it is not expected to be used. The test is checking that an error is thrown when the component suspends and none of the Suspense ancestors have a boundary. However, this test is not actually testing the behavior of the `ReactSuspense` component in this situation. It would be more useful to test the behavior of the component in a real-world scenario where it is actually used.\n",
      "\n",
      "Additionally, the test is not actually testing the error message that is thrown when the component suspends and none of the Suspense ancestors have a boundary. It would be more useful to test that the correct error message is thrown in this situation.\n",
      "\n",
      "Overall, this test seems to be testing the behavior of the `ReactSuspense` component in a way that is not actually relevant to its intended use case. It would be more useful to test the component in a more realistic scenario.\n",
      "</COMMENT>\n",
      "\n",
      "<COMMENT side=\"RIGHT\" offset=\"390\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "references = []\n",
    "candidates = []\n",
    "offset_match_total = 0\n",
    "offset_match_correct = 0\n",
    "\n",
    "for i, sample in enumerate(tqdm(test_data[:100])):  # Full test set if needed\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        [{\"role\": \"user\", \"content\": instruction + sample[\"input\"]}],\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "    inputs = tokenizer(\n",
    "        prompt,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=4096\n",
    "    ).to(\"cuda:0\")\n",
    "\n",
    "    true_comments = extract_comments(sample[\"label\"])\n",
    "\n",
    "    # Measure inference latency\n",
    "    with torch.no_grad():\n",
    "        start = time.time()\n",
    "        output_ids = model.generate(**inputs, max_new_tokens=256, do_sample=False)\n",
    "        latencies.append(time.time() - start)\n",
    "\n",
    "    # Decode and evaluate\n",
    "    generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True).strip()\n",
    "    generated_text = generated_text.split(\"Setting `pad_token_id\")[0].strip()\n",
    "    print(f\"\\n=== Sample {i} ===\\n{generated_text}\")\n",
    "\n",
    "    pred_comments = extract_comments(generated_text)\n",
    "\n",
    "    for gt in true_comments:\n",
    "        references.append(gt[\"comment\"])\n",
    "        best_pred = max(pred_comments, key=lambda p: p[\"comment\"], default={\"comment\": \"\"})\n",
    "        candidates.append(best_pred[\"comment\"] if best_pred else \"\")\n",
    "\n",
    "    correct, total = offset_match(pred_comments, true_comments, window=3)\n",
    "    offset_match_correct += correct\n",
    "    offset_match_total += total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "49490094-41c4-4d0a-8e51-cb463e34c143",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Inference Latency Stats ===\n",
      "Median latency:        7757.82 ms\n",
      "95th percentile:       7990.19 ms\n",
      "99th percentile:       8111.77 ms\n",
      "Throughput (1 sample): 0.13 samples/sec\n"
     ]
    }
   ],
   "source": [
    "# Latency stats\n",
    "print(\"\\n=== Inference Latency Stats ===\")\n",
    "print(f\"Median latency:        {np.percentile(latencies, 50) * 1000:.2f} ms\")\n",
    "print(f\"95th percentile:       {np.percentile(latencies, 95) * 1000:.2f} ms\")\n",
    "print(f\"99th percentile:       {np.percentile(latencies, 99) * 1000:.2f} ms\")\n",
    "print(f\"Throughput (1 sample): {len(latencies) / np.sum(latencies):.2f} samples/sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b0323dc1-c576-47ab-bbc0-7c930423c226",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ded12b530986495da51b52e1c454a9e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3d77327e53a4a4faaefd6e2a83edf4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/482 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cb10eacaf814f9a8e243681bfaff08e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bba71d0866324ad9b8c8f5c5552e3b56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84288e69da2140eab7714604c4bc8e70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ed8b6878f4641488e94a62857d9a78d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.42G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fe514997ef649c2adbdf3afeaa59fe6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49dc0a4d64694fb1bef9cdaf0324dee1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 0.10 seconds, 83.01 sentences/sec\n",
      "\n",
      "Evaluation Results\n",
      "Avg BERTScore F1: 0.7252\n",
      "Offset Precision: 0/23 = 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n"
     ]
    }
   ],
   "source": [
    "# BERTScore\n",
    "P, R, F1 = bert_score(candidates, references, lang=\"en\", verbose=True)\n",
    "print(\"\\nEvaluation Results\")\n",
    "print(f\"Avg BERTScore F1: {F1.mean().item():.4f}\")\n",
    "print(f\"Offset Precision: {offset_match_correct}/{offset_match_total} = {offset_match_correct/offset_match_total:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a49350b7-5cf3-458d-84ae-367a972e5511",
   "metadata": {},
   "source": [
    "Batch throughput test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "30882c1b-72a9-4340-a50e-fc29d763e714",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "batch_size = 4\n",
    "num_batches = 10\n",
    "batch_inputs = [test_data[i][\"input\"] for i in range(batch_size)]\n",
    "batch_prompts = [\n",
    "    tokenizer.apply_chat_template(\n",
    "        [{\"role\": \"user\", \"content\": instruction + inp}],\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    for inp in batch_inputs\n",
    "]\n",
    "\n",
    "encoded = tokenizer(\n",
    "    batch_prompts,\n",
    "    return_tensors=\"pt\",\n",
    "    padding=\"max_length\",\n",
    "    truncation=True,\n",
    "    max_length=4096\n",
    ").to(\"cuda:0\")\n",
    "\n",
    "# Warm-up\n",
    "with torch.no_grad():\n",
    "    _ = model.generate(**encoded, max_new_tokens=256, do_sample=False)\n",
    "\n",
    "batch_times = []\n",
    "with torch.no_grad():\n",
    "    for _ in range(num_batches):\n",
    "        start = time.time()\n",
    "        _ = model.generate(**encoded, max_new_tokens=256, do_sample=False)\n",
    "        batch_times.append(time.time() - start)\n",
    "\n",
    "batch_fps = (batch_size * num_batches) / np.sum(batch_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "cb5cd99a-3643-4300-8087-f23fbf8940ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Final Evaluation Summary ===\n",
      "Model Size on Disk:               12.55\n",
      "BERTScore F1 (avg):               0.7252\n",
      "Location Precision:               0/23 = 0.0000\n",
      "Inference Latency (median):       7757.82 ms\n",
      "Inference Latency (95th pct):     7990.19 ms\n",
      "Inference Latency (99th pct):     8111.77 ms\n",
      "Inference Throughput (1 sample):  0.13 samples/sec\n",
      "Batch Throughput (4):  0.26 samples/sec\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== Final Evaluation Summary ===\")\n",
    "print(f\"Model Size on Disk:               {get_model_size(model):.2f}\")\n",
    "print(f\"BERTScore F1 (avg):               {F1.mean().item():.4f}\")\n",
    "print(f\"Location Precision:               {offset_match_correct}/{offset_match_total} = {offset_match_correct / offset_match_total:.4f}\")\n",
    "print(f\"Inference Latency (median):       {np.percentile(latencies, 50) * 1000:.2f} ms\")\n",
    "print(f\"Inference Latency (95th pct):     {np.percentile(latencies, 95) * 1000:.2f} ms\")\n",
    "print(f\"Inference Latency (99th pct):     {np.percentile(latencies, 99) * 1000:.2f} ms\")\n",
    "print(f\"Inference Throughput (1 sample):  {len(latencies) / np.sum(latencies):.2f} samples/sec\")\n",
    "print(f\"Batch Throughput ({batch_size}):  {batch_fps:.2f} samples/sec\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "911505f5-d4bd-4f7b-a8f5-e699dcec3743",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun May 11 12:01:55 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 560.35.05              Driver Version: 560.35.05      CUDA Version: 12.6     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA A100 80GB PCIe          Off |   00000000:27:00.0 Off |                    0 |\n",
      "| N/A   65C    P0             89W /  300W |   52771MiB /  81920MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a1157d-4706-4d00-93fc-a054dcd5f1ea",
   "metadata": {},
   "source": [
    "### Compiled model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "50d13d22-c3ca-449c-ac9c-4c6c24f441c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a408cd1b-9629-42c0-aea3-0d176bc4c52e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Sample 0 ===\n",
      "[INST] You are a helpful code reviewer. Given a code diff, generate all relevant review comments. Each comment must be in the format: <COMMENT side=\"RIGHT\" offset=\"X\">Your comment here.\n",
      "Only output comments, nothing else.\n",
      "\n",
      "### Code Diff:\n",
      "<DIFF>\n",
      "@@ -838,45 +839,58 @@ function performConcurrentWorkOnRoot(root, didTimeout) {\n",
      "       throw fatalError;\n",
      "     }\n",
      " \n",
      "-    // Check if this render may have yielded to a concurrent event, and if so,\n",
      "-    // confirm that any newly rendered stores are consistent.\n",
      "-    // TODO: It's possible that even a concurrent render may never have yielded\n",
      "-    // to the main thread, if it was fast enough, or if it expired. We could\n",
      "-    // skip the consistency check in that case, too.\n",
      "-    const renderWasConcurrent = !includesBlockingLane(root, lanes);\n",
      "-    const finishedWork: Fiber = (root.current.alternate: any);\n",
      "-    if (\n",
      "-      renderWasConcurrent &&\n",
      "-      !isRenderConsistentWithExternalStores(finishedWork)\n",
      "-    ) {\n",
      "-      // A store was mutated in an interleaved event. Render again,\n",
      "-      // synchronously, to block further mutations.\n",
      "-      exitStatus = renderRootSync(root, lanes);\n",
      "-\n",
      "-      // We need to check again if something threw\n",
      "-      if (exitStatus === RootErrored) {\n",
      "-        const errorRetryLanes = getLanesToRetrySynchronouslyOnError(root);\n",
      "-        if (errorRetryLanes !== NoLanes) {\n",
      "-          lanes = errorRetryLanes;\n",
      "-          exitStatus = recoverFromConcurrentError(root, errorRetryLanes);\n",
      "-          // We assume the tree is now consistent because we didn't yield to any\n",
      "-          // concurrent events.\n",
      "+    if (exitStatus === RootDidNotComplete) {\n",
      "</DIFF> [/INST]  <COMMENT side=\"RIGHT\" offset=\"838\">This code looks like it's checking if a render was concurrent and if it was consistent with external stores. It's also checking if the render was interrupted by an error and if so, it's retrying the render synchronously.</COMMENT>\n",
      "\n",
      "<COMMENT side=\"RIGHT\" offset=\"840\">It's possible that even a concurrent render may never have yielded to the main thread, if it was fast enough, or if it expired. We could skip the consistency check in that case, too.</COMMENT>\n",
      "\n",
      "<COMMENT side=\"RIGHT\" offset=\"842\">The code is checking if the render was concurrent and if it was consistent with external stores. If it was not consistent, it's retrying the render synchronously.</COMMENT>\n",
      "\n",
      "<COMMENT side=\"RIGHT\" offset=\"844\">It's assuming that the tree is now consistent because it didn't yield to any concurrent events.</COMMENT>\n",
      "\n",
      "<COMMENT side=\"RIGHT\" offset=\"846\">It's checking if the\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Sample 1 ===\n",
      "[INST] You are a helpful code reviewer. Given a code diff, generate all relevant review comments. Each comment must be in the format: <COMMENT side=\"RIGHT\" offset=\"X\">Your comment here.\n",
      "Only output comments, nothing else.\n",
      "\n",
      "### Code Diff:\n",
      "<DIFF>\n",
      "@@ -838,45 +839,58 @@ function performConcurrentWorkOnRoot(root, didTimeout) {\n",
      "       throw fatalError;\n",
      "     }\n",
      " \n",
      "-    // Check if this render may have yielded to a concurrent event, and if so,\n",
      "-    // confirm that any newly rendered stores are consistent.\n",
      "-    // TODO: It's possible that even a concurrent render may never have yielded\n",
      "-    // to the main thread, if it was fast enough, or if it expired. We could\n",
      "-    // skip the consistency check in that case, too.\n",
      "-    const renderWasConcurrent = !includesBlockingLane(root, lanes);\n",
      "-    const finishedWork: Fiber = (root.current.alternate: any);\n",
      "-    if (\n",
      "-      renderWasConcurrent &&\n",
      "-      !isRenderConsistentWithExternalStores(finishedWork)\n",
      "-    ) {\n",
      "-      // A store was mutated in an interleaved event. Render again,\n",
      "-      // synchronously, to block further mutations.\n",
      "-      exitStatus = renderRootSync(root, lanes);\n",
      "-\n",
      "-      // We need to check again if something threw\n",
      "-      if (exitStatus === RootErrored) {\n",
      "-        const errorRetryLanes = getLanesToRetrySynchronouslyOnError(root);\n",
      "-        if (errorRetryLanes !== NoLanes) {\n",
      "-          lanes = errorRetryLanes;\n",
      "-          exitStatus = recoverFromConcurrentError(root, errorRetryLanes);\n",
      "-          // We assume the tree is now consistent because we didn't yield to any\n",
      "-          // concurrent events.\n",
      "+    if (exitStatus === RootDidNotComplete) {\n",
      "+      // The render unwound without completing the tree. This happens in special\n",
      "+      // cases where need to exit the current render without producing a\n",
      "+      // consistent tree or committing.\n",
      "+      //\n",
      "+      // This should only happen during a concurrent render, not a discrete or\n",
      "+      // synchronous update. We should have already checked for this when we\n",
      "+      // unwound the stack.\n",
      "</DIFF> [/INST]  <COMMENT side=\"RIGHT\" offset=\"838\">This code looks like it's checking if a render was concurrent and if it was consistent with external stores. However, it's not clear why this check is necessary or what the implications of this check are.</COMMENT>\n",
      "\n",
      "<COMMENT side=\"RIGHT\" offset=\"845\">This code is checking if a render was concurrent and if it was consistent with external stores. If the render was concurrent and the stores were not consistent, it will re-render the tree synchronously to block further mutations.</COMMENT>\n",
      "\n",
      "<COMMENT side=\"RIGHT\" offset=\"853\">This code is checking if a render was concurrent and if it was consistent with external stores. If the render was concurrent and the stores were not consistent, it will re-render the tree synchronously to block further mutations. However, it's not clear why this check is necessary or what the implications of this check are.</COMMENT>\n",
      "\n",
      "<COMMENT side=\"RIGHT\" offset=\"861\">This code is checking if a render was concurrent and if it was consistent with external stores. If the render\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Sample 2 ===\n",
      "[INST] You are a helpful code reviewer. Given a code diff, generate all relevant review comments. Each comment must be in the format: <COMMENT side=\"RIGHT\" offset=\"X\">Your comment here.\n",
      "Only output comments, nothing else.\n",
      "\n",
      "### Code Diff:\n",
      "<DIFF>\n",
      "@@ -1149,6 +1163,10 @@ function performSyncWorkOnRoot(root) {\n",
      "     throw fatalError;\n",
      "   }\n",
      " \n",
      "+  if (exitStatus === RootDidNotComplete) {\n",
      "</DIFF> [/INST]  <COMMENT side=\"RIGHT\" offset=\"1150\">This line is not needed. The `throw` statement will already cause the function to exit with an error. It's not necessary to check the `exitStatus` variable and throw a new error if it's not `RootCompleted`.</COMMENT>\n",
      "\n",
      "<COMMENT side=\"RIGHT\" offset=\"1164\">This line is not needed. The `if` statement is already checking if the `exitStatus` variable is equal to `RootDidNotComplete`. It's not necessary to check the `exitStatus` variable again in the `if` statement's body.</COMMENT>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Sample 3 ===\n",
      "[INST] You are a helpful code reviewer. Given a code diff, generate all relevant review comments. Each comment must be in the format: <COMMENT side=\"RIGHT\" offset=\"X\">Your comment here.\n",
      "Only output comments, nothing else.\n",
      "\n",
      "### Code Diff:\n",
      "<DIFF>\n",
      "@@ -739,14 +739,20 @@ describe('ReactCache', () => {\n",
      "     await act(async () => {\n",
      "       refresh();\n",
      "     });\n",
      "-    expect(Scheduler).toHaveYielded(['Cache miss! [A]', 'Loading...']);\n",
      "+    expect(Scheduler).toHaveYielded([\n",
      "+      'Cache miss! [A]',\n",
      "+      'Loading...',\n",
      "+      // TODO: This happens too early, because we don't retain the refreshed\n",
      "+      // cache until it commits. Will fix in next step.\n",
      "+      'Cache cleanup: A [v1]',\n",
      "</DIFF> [/INST]  <COMMENT side=\"RIGHT\" offset=\"10\">This comment is suggesting that the cache cleanup is happening too early, before the refreshed cache has committed. This could be fixed by waiting for the cache to commit before cleaning up.</COMMENT>\n",
      "\n",
      "<COMMENT side=\"RIGHT\" offset=\"15\">This comment is suggesting that the cache cleanup is happening too early, before the refreshed cache has committed. This could be fixed by waiting for the cache to commit before cleaning up.</COMMENT>\n",
      "\n",
      "<COMMENT side=\"RIGHT\" offset=\"18\">This comment is suggesting that the cache cleanup is happening too early, before the refreshed cache has committed. This could be fixed by waiting for the cache to commit before cleaning up.</COMMENT>\n",
      "\n",
      "<COMMENT side=\"RIGHT\" offset=\"21\">This comment is suggesting that the cache cleanup is happening too early, before the refreshed cache has committed. This could be fixed by waiting for the cache to commit before cleaning up.</COMMENT>\n",
      "\n",
      "<COMMENT side=\"RIGHT\" offset=\"24\">This comment is suggesting that the cache cleanup\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:35<00:00,  7.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Sample 4 ===\n",
      "[INST] You are a helpful code reviewer. Given a code diff, generate all relevant review comments. Each comment must be in the format: <COMMENT side=\"RIGHT\" offset=\"X\">Your comment here.\n",
      "Only output comments, nothing else.\n",
      "\n",
      "### Code Diff:\n",
      "<DIFF>\n",
      "@@ -739,14 +739,20 @@ describe('ReactCache', () => {\n",
      "     await act(async () => {\n",
      "       refresh();\n",
      "     });\n",
      "-    expect(Scheduler).toHaveYielded(['Cache miss! [A]', 'Loading...']);\n",
      "+    expect(Scheduler).toHaveYielded([\n",
      "+      'Cache miss! [A]',\n",
      "+      'Loading...',\n",
      "+      // TODO: This happens too early, because we don't retain the refreshed\n",
      "+      // cache until it commits. Will fix in next step.\n",
      "+      'Cache cleanup: A [v1]',\n",
      "</DIFF> [/INST]  <COMMENT side=\"RIGHT\" offset=\"10\">This comment is suggesting that the cache cleanup is happening too early, before the refreshed cache has committed. This could be fixed by waiting for the cache to commit before cleaning up.</COMMENT>\n",
      "\n",
      "<COMMENT side=\"RIGHT\" offset=\"15\">This comment is suggesting that the cache cleanup is happening too early, before the refreshed cache has committed. This could be fixed by waiting for the cache to commit before cleaning up.</COMMENT>\n",
      "\n",
      "<COMMENT side=\"RIGHT\" offset=\"18\">This comment is suggesting that the cache cleanup is happening too early, before the refreshed cache has committed. This could be fixed by waiting for the cache to commit before cleaning up.</COMMENT>\n",
      "\n",
      "<COMMENT side=\"RIGHT\" offset=\"21\">This comment is suggesting that the cache cleanup is happening too early, before the refreshed cache has committed. This could be fixed by waiting for the cache to commit before cleaning up.</COMMENT>\n",
      "\n",
      "<COMMENT side=\"RIGHT\" offset=\"24\">This comment is suggesting that the cache cleanup\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "references = []\n",
    "candidates = []\n",
    "offset_match_total = 0\n",
    "offset_match_correct = 0\n",
    "\n",
    "for i, sample in enumerate(tqdm(test_data[:5])):  # Full test set if needed\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        [{\"role\": \"user\", \"content\": instruction + sample[\"input\"]}],\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "    inputs = tokenizer(\n",
    "        prompt,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=4096\n",
    "    ).to(\"cuda:0\")\n",
    "\n",
    "    true_comments = extract_comments(sample[\"label\"])\n",
    "\n",
    "    # Measure inference latency\n",
    "    with torch.no_grad():\n",
    "        start = time.time()\n",
    "        output_ids = model.generate(**inputs, max_new_tokens=256, do_sample=False)\n",
    "        latencies.append(time.time() - start)\n",
    "\n",
    "    # Decode and evaluate\n",
    "    generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True).strip()\n",
    "    generated_text = generated_text.split(\"Setting `pad_token_id\")[0].strip()\n",
    "    print(f\"\\n=== Sample {i} ===\\n{generated_text}\")\n",
    "\n",
    "    pred_comments = extract_comments(generated_text)\n",
    "\n",
    "    for gt in true_comments:\n",
    "        references.append(gt[\"comment\"])\n",
    "        best_pred = max(pred_comments, key=lambda p: p[\"comment\"], default={\"comment\": \"\"})\n",
    "        candidates.append(best_pred[\"comment\"] if best_pred else \"\")\n",
    "\n",
    "    correct, total = offset_match(pred_comments, true_comments, window=3)\n",
    "    offset_match_correct += correct\n",
    "    offset_match_total += total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0ef32e3d-1306-4f65-9237-96ad4a7f3dcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "batch_size = 4\n",
    "num_batches = 10\n",
    "batch_inputs = [test_data[i][\"input\"] for i in range(batch_size)]\n",
    "batch_prompts = [\n",
    "    tokenizer.apply_chat_template(\n",
    "        [{\"role\": \"user\", \"content\": instruction + inp}],\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    for inp in batch_inputs\n",
    "]\n",
    "\n",
    "encoded = tokenizer(\n",
    "    batch_prompts,\n",
    "    return_tensors=\"pt\",\n",
    "    padding=\"max_length\",\n",
    "    truncation=True,\n",
    "    max_length=4096\n",
    ").to(\"cuda:0\")\n",
    "\n",
    "# Warm-up\n",
    "with torch.no_grad():\n",
    "    _ = model.generate(**encoded, max_new_tokens=256, do_sample=False)\n",
    "\n",
    "batch_times = []\n",
    "with torch.no_grad():\n",
    "    for _ in range(num_batches):\n",
    "        start = time.time()\n",
    "        _ = model.generate(**encoded, max_new_tokens=256, do_sample=False)\n",
    "        batch_times.append(time.time() - start)\n",
    "\n",
    "batch_fps = (batch_size * num_batches) / np.sum(batch_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "befdf788-8e05-446f-9ecd-1e1fb07bb09b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Final Evaluation Summary ===\n",
      "Model Size on Disk:               12.55\n",
      "BERTScore F1 (avg):               0.7252\n",
      "Location Precision:               0/16 = 0.0000\n",
      "Inference Latency (median):       7761.92 ms\n",
      "Inference Latency (95th pct):     8123.67 ms\n",
      "Inference Latency (99th pct):     159196.10 ms\n",
      "Inference Throughput (1 sample):  0.06 samples/sec\n",
      "Batch Throughput (4):  0.26 samples/sec\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== Final Evaluation Summary ===\")\n",
    "print(f\"Model Size on Disk:               {get_model_size(model):.2f}\")\n",
    "print(f\"BERTScore F1 (avg):               {F1.mean().item():.4f}\")\n",
    "print(f\"Location Precision:               {offset_match_correct}/{offset_match_total} = {offset_match_correct / offset_match_total:.4f}\")\n",
    "print(f\"Inference Latency (median):       {np.percentile(latencies, 50) * 1000:.2f} ms\")\n",
    "print(f\"Inference Latency (95th pct):     {np.percentile(latencies, 95) * 1000:.2f} ms\")\n",
    "print(f\"Inference Latency (99th pct):     {np.percentile(latencies, 99) * 1000:.2f} ms\")\n",
    "print(f\"Inference Throughput (1 sample):  {len(latencies) / np.sum(latencies):.2f} samples/sec\")\n",
    "print(f\"Batch Throughput ({batch_size}):  {batch_fps:.2f} samples/sec\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6280cc2-c9ba-4531-bf1a-8eceb0a8a5cc",
   "metadata": {},
   "source": [
    "### Converting to ONNX"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f30c06-36cf-4efe-ae85-19b92cd7fa60",
   "metadata": {},
   "source": [
    "The newer versions don't support Optimum, and with ORTModelForCausalLM, it exports the onnx model too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8eebd2ae-281f-40d4-9132-389fb4e39507",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cae0131b99f14bc4bd8b9041496589a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.12/site-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.12/site-packages/transformers/modeling_attn_mask_utils.py:114: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if (input_shape[-1] > 1 or self.sliding_window is not None) and self.is_causal:\n",
      "/opt/conda/lib/python3.12/site-packages/optimum/exporters/onnx/model_patcher.py:454: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if past_key_values_length > 0:\n",
      "/opt/conda/lib/python3.12/site-packages/transformers/models/llama/modeling_llama.py:150: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if seq_len > self.max_seq_len_cached:\n",
      "/opt/conda/lib/python3.12/site-packages/transformers/models/llama/modeling_llama.py:716: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if attention_mask.size() != (bsz, 1, q_len, kv_seq_len):\n",
      "In-place op on output of tensor.shape. See https://pytorch.org/docs/main/onnx.html#avoid-inplace-operations-when-using-tensor-shape-in-tracing-mode\n",
      "In-place op on output of tensor.shape. See https://pytorch.org/docs/main/onnx.html#avoid-inplace-operations-when-using-tensor-shape-in-tracing-mode\n",
      "In-place op on output of tensor.shape. See https://pytorch.org/docs/main/onnx.html#avoid-inplace-operations-when-using-tensor-shape-in-tracing-mode\n",
      "In-place op on output of tensor.shape. See https://pytorch.org/docs/main/onnx.html#avoid-inplace-operations-when-using-tensor-shape-in-tracing-mode\n",
      "In-place op on output of tensor.shape. See https://pytorch.org/docs/main/onnx.html#avoid-inplace-operations-when-using-tensor-shape-in-tracing-mode\n",
      "In-place op on output of tensor.shape. See https://pytorch.org/docs/main/onnx.html#avoid-inplace-operations-when-using-tensor-shape-in-tracing-mode\n",
      "In-place op on output of tensor.shape. See https://pytorch.org/docs/main/onnx.html#avoid-inplace-operations-when-using-tensor-shape-in-tracing-mode\n",
      "In-place op on output of tensor.shape. See https://pytorch.org/docs/main/onnx.html#avoid-inplace-operations-when-using-tensor-shape-in-tracing-mode\n",
      "In-place op on output of tensor.shape. See https://pytorch.org/docs/main/onnx.html#avoid-inplace-operations-when-using-tensor-shape-in-tracing-mode\n",
      "In-place op on output of tensor.shape. See https://pytorch.org/docs/main/onnx.html#avoid-inplace-operations-when-using-tensor-shape-in-tracing-mode\n",
      "In-place op on output of tensor.shape. See https://pytorch.org/docs/main/onnx.html#avoid-inplace-operations-when-using-tensor-shape-in-tracing-mode\n",
      "In-place op on output of tensor.shape. See https://pytorch.org/docs/main/onnx.html#avoid-inplace-operations-when-using-tensor-shape-in-tracing-mode\n",
      "In-place op on output of tensor.shape. See https://pytorch.org/docs/main/onnx.html#avoid-inplace-operations-when-using-tensor-shape-in-tracing-mode\n",
      "In-place op on output of tensor.shape. See https://pytorch.org/docs/main/onnx.html#avoid-inplace-operations-when-using-tensor-shape-in-tracing-mode\n",
      "In-place op on output of tensor.shape. See https://pytorch.org/docs/main/onnx.html#avoid-inplace-operations-when-using-tensor-shape-in-tracing-mode\n",
      "In-place op on output of tensor.shape. See https://pytorch.org/docs/main/onnx.html#avoid-inplace-operations-when-using-tensor-shape-in-tracing-mode\n",
      "In-place op on output of tensor.shape. See https://pytorch.org/docs/main/onnx.html#avoid-inplace-operations-when-using-tensor-shape-in-tracing-mode\n",
      "In-place op on output of tensor.shape. See https://pytorch.org/docs/main/onnx.html#avoid-inplace-operations-when-using-tensor-shape-in-tracing-mode\n",
      "In-place op on output of tensor.shape. See https://pytorch.org/docs/main/onnx.html#avoid-inplace-operations-when-using-tensor-shape-in-tracing-mode\n",
      "In-place op on output of tensor.shape. See https://pytorch.org/docs/main/onnx.html#avoid-inplace-operations-when-using-tensor-shape-in-tracing-mode\n",
      "In-place op on output of tensor.shape. See https://pytorch.org/docs/main/onnx.html#avoid-inplace-operations-when-using-tensor-shape-in-tracing-mode\n",
      "In-place op on output of tensor.shape. See https://pytorch.org/docs/main/onnx.html#avoid-inplace-operations-when-using-tensor-shape-in-tracing-mode\n",
      "In-place op on output of tensor.shape. See https://pytorch.org/docs/main/onnx.html#avoid-inplace-operations-when-using-tensor-shape-in-tracing-mode\n",
      "In-place op on output of tensor.shape. See https://pytorch.org/docs/main/onnx.html#avoid-inplace-operations-when-using-tensor-shape-in-tracing-mode\n",
      "In-place op on output of tensor.shape. See https://pytorch.org/docs/main/onnx.html#avoid-inplace-operations-when-using-tensor-shape-in-tracing-mode\n",
      "In-place op on output of tensor.shape. See https://pytorch.org/docs/main/onnx.html#avoid-inplace-operations-when-using-tensor-shape-in-tracing-mode\n",
      "In-place op on output of tensor.shape. See https://pytorch.org/docs/main/onnx.html#avoid-inplace-operations-when-using-tensor-shape-in-tracing-mode\n",
      "In-place op on output of tensor.shape. See https://pytorch.org/docs/main/onnx.html#avoid-inplace-operations-when-using-tensor-shape-in-tracing-mode\n",
      "In-place op on output of tensor.shape. See https://pytorch.org/docs/main/onnx.html#avoid-inplace-operations-when-using-tensor-shape-in-tracing-mode\n",
      "In-place op on output of tensor.shape. See https://pytorch.org/docs/main/onnx.html#avoid-inplace-operations-when-using-tensor-shape-in-tracing-mode\n",
      "In-place op on output of tensor.shape. See https://pytorch.org/docs/main/onnx.html#avoid-inplace-operations-when-using-tensor-shape-in-tracing-mode\n",
      "In-place op on output of tensor.shape. See https://pytorch.org/docs/main/onnx.html#avoid-inplace-operations-when-using-tensor-shape-in-tracing-mode\n",
      "/opt/conda/lib/python3.12/site-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONNX model saved to: models/llama_onnx//model.onnx\n"
     ]
    }
   ],
   "source": [
    "model_id = \"codellama/CodeLLaMA-7b-Instruct-hf\"  \n",
    "onnx_dir = \"models/llama_onnx/\"\n",
    "\n",
    "model = ORTModelForCausalLM.from_pretrained(\n",
    "    model_id, \n",
    "    export=True, \n",
    "    cache_dir=onnx_dir\n",
    ")\n",
    "\n",
    "model.save_pretrained(onnx_dir)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.save_pretrained(onnx_dir)\n",
    "\n",
    "print(f\"ONNX model saved to: {onnx_dir}/model.onnx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "73ea5c19-2dec-46ca-955e-dda896876d59",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: onnxruntime 1.22.0\n",
      "Uninstalling onnxruntime-1.22.0:\n",
      "  Successfully uninstalled onnxruntime-1.22.0\n",
      "Requirement already satisfied: onnxruntime-gpu in /opt/conda/lib/python3.12/site-packages (1.22.0)\n",
      "Requirement already satisfied: coloredlogs in /opt/conda/lib/python3.12/site-packages (from onnxruntime-gpu) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in /opt/conda/lib/python3.12/site-packages (from onnxruntime-gpu) (25.2.10)\n",
      "Requirement already satisfied: numpy>=1.21.6 in /opt/conda/lib/python3.12/site-packages (from onnxruntime-gpu) (1.26.4)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.12/site-packages (from onnxruntime-gpu) (24.2)\n",
      "Requirement already satisfied: protobuf in /opt/conda/lib/python3.12/site-packages (from onnxruntime-gpu) (5.28.3)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.12/site-packages (from onnxruntime-gpu) (1.13.1)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /opt/conda/lib/python3.12/site-packages (from coloredlogs->onnxruntime-gpu) (10.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.12/site-packages (from sympy->onnxruntime-gpu) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "# !pip uninstall onnxruntime -y\n",
    "# !pip install onnxruntime-gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4496440a-b983-4149-818d-800558015034",
   "metadata": {},
   "outputs": [],
   "source": [
    "ort_session = ort.InferenceSession(\"models/llama_onnx/model.onnx\", providers=[\"CUDAExecutionProvider\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1ec6690c-1daf-401c-906e-71013696e2ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AzureExecutionProvider', 'CPUExecutionProvider']\n"
     ]
    }
   ],
   "source": [
    "# ort_session.get_providers()\n",
    "import onnxruntime as ort\n",
    "print(ort.get_available_providers())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1cdd910a-db5e-4214-8101-7ba87822fbc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun May 11 13:39:52 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 560.35.05              Driver Version: 560.35.05      CUDA Version: 12.6     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA A100 80GB PCIe          Off |   00000000:27:00.0 Off |                    0 |\n",
      "| N/A   45C    P0             48W /  300W |       1MiB /  81920MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5e945b84-8a3b-437d-a875-6a8e11e9e14c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "AttributeError: module 'onnxruntime' has no attribute 'get_available_providers'\n"
     ]
    }
   ],
   "source": [
    "!python -c \"import onnxruntime as ort; print(ort.get_available_providers())\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "98b5147f-0cb4-473f-b8a8-90f00259a1ae",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Asked to use CUDAExecutionProvider as an ONNX Runtime execution provider, but the available execution providers are ['AzureExecutionProvider', 'CPUExecutionProvider'].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m onnx_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodels/llama_onnx/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Load the ONNX model\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mORTModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43monnx_dir\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Load tokenizer\u001b[39;00m\n\u001b[1;32m      7\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(onnx_dir)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/optimum/onnxruntime/modeling_ort.py:335\u001b[0m, in \u001b[0;36mORTModel.to\u001b[0;34m(self, device)\u001b[0m\n\u001b[1;32m    332\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n\u001b[1;32m    334\u001b[0m provider \u001b[38;5;241m=\u001b[39m get_provider_for_device(device)\n\u001b[0;32m--> 335\u001b[0m \u001b[43mvalidate_provider_availability\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprovider\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# raise error if the provider is not available\u001b[39;00m\n\u001b[1;32m    337\u001b[0m \u001b[38;5;66;03m# IOBinding is only supported for CPU and CUDA Execution Providers.\u001b[39;00m\n\u001b[1;32m    338\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m device\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_use_io_binding \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m provider \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCUDAExecutionProvider\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/optimum/onnxruntime/utils.py:287\u001b[0m, in \u001b[0;36mvalidate_provider_availability\u001b[0;34m(provider)\u001b[0m\n\u001b[1;32m    285\u001b[0m available_providers \u001b[38;5;241m=\u001b[39m ort\u001b[38;5;241m.\u001b[39mget_available_providers()\n\u001b[1;32m    286\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m provider \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m available_providers:\n\u001b[0;32m--> 287\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    288\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAsked to use \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprovider\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m as an ONNX Runtime execution provider, but the available execution providers are \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mavailable_providers\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    289\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Asked to use CUDAExecutionProvider as an ONNX Runtime execution provider, but the available execution providers are ['AzureExecutionProvider', 'CPUExecutionProvider']."
     ]
    }
   ],
   "source": [
    "onnx_dir = \"models/llama_onnx/\"\n",
    "\n",
    "# Load the ONNX model\n",
    "model = ORTModelForCausalLM.from_pretrained(onnx_dir).to(\"cuda\")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(onnx_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b828ecf-f7d0-49c8-9b54-23b0330ccaa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "code_prompt = \"\"\"\n",
    "def fibonacci(n):\n",
    "    if n <= 0:\n",
    "        return []\n",
    "    elif n == 1:\n",
    "        return [0]\n",
    "    elif n == 2:\n",
    "        return [0, 1]\n",
    "    else:\n",
    "        seq = [0, 1]\n",
    "        for i in range(2, n):\n",
    "            seq.append(seq[-1] + seq[-2])\n",
    "        return seq\n",
    "\"\"\"\n",
    "\n",
    "prompt = f\"Given the code diff below, generate relevant code review comments in the format:\\n<COMMENT side=\\\"RIGHT\\\" offset=\\\"X\\\">...\\n\\n{code_prompt}\"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=4096).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f1438a8-d30e-42b6-ba35-c60ed7a25bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    output_ids = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=256,\n",
    "        do_sample=False,\n",
    "        pad_token_id=tokenizer.pad_token_id\n",
    "    )\n",
    "\n",
    "generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "print(\"Generated Review Comments:\\n\", generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f233aab9-549b-4f55-8e32-796434b855b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa7d283d-dfc9-483c-9fbc-3b4366f2b6c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_session(ort_session, input_array, batch_input):\n",
    "    print(f\"\\nExecution provider: {ort_session.get_providers()}\")\n",
    "    input_name = ort_session.get_inputs()[0].name\n",
    "\n",
    "    # Inference latency (single input)\n",
    "    num_trials = 10\n",
    "    ort_session.run(None, {input_name: input_array})  # Warm-up\n",
    "    latencies = []\n",
    "    for _ in range(num_trials):\n",
    "        start = time.time()\n",
    "        ort_session.run(None, {input_name: input_array})\n",
    "        latencies.append(time.time() - start)\n",
    "\n",
    "    print(f\"Inference Latency (median): {np.percentile(latencies, 50) * 1000:.2f} ms\")\n",
    "    print(f\"Inference Latency (95th pct): {np.percentile(latencies, 95) * 1000:.2f} ms\")\n",
    "    print(f\"Inference Latency (99th pct): {np.percentile(latencies, 99) * 1000:.2f} ms\")\n",
    "    print(f\"Inference Throughput (1 sample): {num_trials / np.sum(latencies):.2f} samples/sec\")\n",
    "\n",
    "    # Batch throughput\n",
    "    num_batches = 5\n",
    "    ort_session.run(None, {input_name: batch_input})  # Warm-up\n",
    "    batch_times = []\n",
    "    for _ in range(num_batches):\n",
    "        start = time.time()\n",
    "        ort_session.run(None, {input_name: batch_input})\n",
    "        batch_times.append(time.time() - start)\n",
    "    batch_fps = (batch_input.shape[0] * num_batches) / np.sum(batch_times)\n",
    "    print(f\"Batch Throughput ({batch_input.shape[0]}): {batch_fps:.2f} samples/sec\")\n",
    "\n",
    "    # Model size\n",
    "    model_size = os.path.getsize(ort_session._model_path)\n",
    "    print(f\"Model Size on Disk: {model_size / 1e6:.2f} MB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053ff48b-148a-46d4-91ec-4abb232173c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"Given the code diff below, generate review comments:{train_data[0]['input']}\"\n",
    "\n",
    "# Tokenize it\n",
    "inputs = tokenizer(prompt, return_tensors=\"np\", padding=\"max_length\", truncation=True, max_length=128)\n",
    "\n",
    "# Extract NumPy input\n",
    "input_array = inputs[\"input_ids\"]\n",
    "batch_input = np.repeat(input_array, repeats=4, axis=0)  # batch of 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "074062fd-87af-435d-9e7b-563b2b73d92a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e16b971-73df-48ff-9712-49c210202ec0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c1cab21e-d1c0-4336-8fa0-388de2da71d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: onnxruntime-gpu\n",
      "Version: 1.22.0\n",
      "Summary: ONNX Runtime is a runtime accelerator for Machine Learning models\n",
      "Home-page: https://onnxruntime.ai\n",
      "Author: Microsoft Corporation\n",
      "Author-email: onnxruntime@microsoft.com\n",
      "License: MIT License\n",
      "Location: /opt/conda/lib/python3.12/site-packages\n",
      "Requires: coloredlogs, flatbuffers, numpy, packaging, protobuf, sympy\n",
      "Required-by: \n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip show onnxruntime-gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c5213f7-05ad-477f-9f2f-c62818856e77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1f15b7-89e7-4ce1-98c3-81e0ff8f4a55",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
