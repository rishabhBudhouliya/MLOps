{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fcc6e203-7aa8-4dee-82e6-28be0a766a24",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acd5f6802af4470cb83bf391145b263a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "Failed to import transformers.trainer because of the following error (look up to see its traceback):\ncannot import name 'EncoderDecoderCache' from 'transformers' (/opt/conda/lib/python3.12/site-packages/transformers/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/transformers/utils/import_utils.py:1364\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1363\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1364\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1365\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/importlib/__init__.py:90\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m     89\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 90\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1387\u001b[0m, in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1360\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1331\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:935\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:999\u001b[0m, in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:488\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/transformers/trainer.py:190\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_peft_available():\n\u001b[0;32m--> 190\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpeft\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PeftModel\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_accelerate_available():\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/peft/__init__.py:17\u001b[0m\n\u001b[1;32m     15\u001b[0m __version__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0.15.2\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mauto\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     18\u001b[0m     MODEL_TYPE_TO_PEFT_MODEL_MAPPING,\n\u001b[1;32m     19\u001b[0m     AutoPeftModel,\n\u001b[1;32m     20\u001b[0m     AutoPeftModelForCausalLM,\n\u001b[1;32m     21\u001b[0m     AutoPeftModelForFeatureExtraction,\n\u001b[1;32m     22\u001b[0m     AutoPeftModelForQuestionAnswering,\n\u001b[1;32m     23\u001b[0m     AutoPeftModelForSeq2SeqLM,\n\u001b[1;32m     24\u001b[0m     AutoPeftModelForSequenceClassification,\n\u001b[1;32m     25\u001b[0m     AutoPeftModelForTokenClassification,\n\u001b[1;32m     26\u001b[0m )\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PeftConfig, PromptLearningConfig\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/peft/auto.py:32\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PeftConfig\n\u001b[0;32m---> 32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpeft_model\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     33\u001b[0m     PeftModel,\n\u001b[1;32m     34\u001b[0m     PeftModelForCausalLM,\n\u001b[1;32m     35\u001b[0m     PeftModelForFeatureExtraction,\n\u001b[1;32m     36\u001b[0m     PeftModelForQuestionAnswering,\n\u001b[1;32m     37\u001b[0m     PeftModelForSeq2SeqLM,\n\u001b[1;32m     38\u001b[0m     PeftModelForSequenceClassification,\n\u001b[1;32m     39\u001b[0m     PeftModelForTokenClassification,\n\u001b[1;32m     40\u001b[0m )\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconstants\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TOKENIZER_CONFIG_NAME\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/peft/peft_model.py:37\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n\u001b[0;32m---> 37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Cache, DynamicCache, EncoderDecoderCache, PreTrainedModel\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodeling_outputs\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m QuestionAnsweringModelOutput, SequenceClassifierOutput, TokenClassifierOutput\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'EncoderDecoderCache' from 'transformers' (/opt/conda/lib/python3.12/site-packages/transformers/__init__.py)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpathlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Path\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdatasets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Dataset, load_dataset\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      8\u001b[0m     AutoTokenizer, \n\u001b[1;32m      9\u001b[0m     AutoModelForCausalLM, \n\u001b[1;32m     10\u001b[0m     TrainingArguments, \n\u001b[1;32m     11\u001b[0m     Trainer, \n\u001b[1;32m     12\u001b[0m     DataCollatorForLanguageModeling,\n\u001b[1;32m     13\u001b[0m     BitsAndBytesConfig\n\u001b[1;32m     14\u001b[0m )\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpeft\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_peft_model, LoraConfig, TaskType, PeftModel, PeftConfig\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mbert_score\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m score\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1412\u001b[0m, in \u001b[0;36m_handle_fromlist\u001b[0;34m(module, fromlist, import_, recursive)\u001b[0m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/transformers/utils/import_utils.py:1354\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1352\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_module(name)\n\u001b[1;32m   1353\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m-> 1354\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_class_to_module\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1355\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(module, name)\n\u001b[1;32m   1356\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/transformers/utils/import_utils.py:1366\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1364\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m module_name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m   1365\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m-> 1366\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1367\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to import \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m because of the following error (look up to see its\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1368\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m traceback):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1369\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Failed to import transformers.trainer because of the following error (look up to see its traceback):\ncannot import name 'EncoderDecoderCache' from 'transformers' (/opt/conda/lib/python3.12/site-packages/transformers/__init__.py)"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gzip, json, time\n",
    "import torch \n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from datasets import Dataset, load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForCausalLM, \n",
    "    TrainingArguments, \n",
    "    Trainer, \n",
    "    DataCollatorForLanguageModeling,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "from peft import get_peft_model, LoraConfig, TaskType, PeftModel, PeftConfig\n",
    "from bert_score import score\n",
    "import onnxruntime as ort\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import onnx\n",
    "import onnxruntime as ort\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "from bert_score import score as bert_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ba2b28-2e9a-4820-97f3-da7cc81706c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from optimum.onnxruntime import ORTModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "215b13b5-089b-420e-8fe4-3ad6f9fe30c3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers==4.37.2\n",
      "  Downloading transformers-4.37.2-py3-none-any.whl.metadata (129 kB)\n",
      "Collecting optimum==1.12.0 (from optimum[onnxruntime]==1.12.0)\n",
      "  Downloading optimum-1.12.0-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.12/site-packages (from transformers==4.37.2) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /opt/conda/lib/python3.12/site-packages (from transformers==4.37.2) (0.31.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.12/site-packages (from transformers==4.37.2) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.12/site-packages (from transformers==4.37.2) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.12/site-packages (from transformers==4.37.2) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.12/site-packages (from transformers==4.37.2) (2024.11.6)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.12/site-packages (from transformers==4.37.2) (2.32.3)\n",
      "Collecting tokenizers<0.19,>=0.14 (from transformers==4.37.2)\n",
      "  Downloading tokenizers-0.15.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.12/site-packages (from transformers==4.37.2) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.12/site-packages (from transformers==4.37.2) (4.67.1)\n",
      "Requirement already satisfied: coloredlogs in /opt/conda/lib/python3.12/site-packages (from optimum==1.12.0->optimum[onnxruntime]==1.12.0) (15.0.1)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.12/site-packages (from optimum==1.12.0->optimum[onnxruntime]==1.12.0) (1.13.1)\n",
      "Requirement already satisfied: torch>=1.9 in /opt/conda/lib/python3.12/site-packages (from optimum==1.12.0->optimum[onnxruntime]==1.12.0) (2.5.1+cu124)\n",
      "Requirement already satisfied: datasets in /opt/conda/lib/python3.12/site-packages (from optimum==1.12.0->optimum[onnxruntime]==1.12.0) (3.6.0)\n",
      "Requirement already satisfied: onnx in /opt/conda/lib/python3.12/site-packages (from optimum[onnxruntime]==1.12.0) (1.17.0)\n",
      "Requirement already satisfied: onnxruntime>=1.11.0 in /opt/conda/lib/python3.12/site-packages (from optimum[onnxruntime]==1.12.0) (1.22.0)\n",
      "Requirement already satisfied: evaluate in /opt/conda/lib/python3.12/site-packages (from optimum[onnxruntime]==1.12.0) (0.4.3)\n",
      "Requirement already satisfied: protobuf>=3.20.1 in /opt/conda/lib/python3.12/site-packages (from optimum[onnxruntime]==1.12.0) (5.28.3)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.12/site-packages (from datasets->optimum==1.12.0->optimum[onnxruntime]==1.12.0) (19.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.12/site-packages (from datasets->optimum==1.12.0->optimum[onnxruntime]==1.12.0) (0.3.8)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.12/site-packages (from datasets->optimum==1.12.0->optimum[onnxruntime]==1.12.0) (2.2.3)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.12/site-packages (from datasets->optimum==1.12.0->optimum[onnxruntime]==1.12.0) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /opt/conda/lib/python3.12/site-packages (from datasets->optimum==1.12.0->optimum[onnxruntime]==1.12.0) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /opt/conda/lib/python3.12/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets->optimum==1.12.0->optimum[onnxruntime]==1.12.0) (2024.12.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.37.2) (4.12.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /opt/conda/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.37.2) (1.1.0)\n",
      "Requirement already satisfied: flatbuffers in /opt/conda/lib/python3.12/site-packages (from onnxruntime>=1.11.0->optimum[onnxruntime]==1.12.0) (25.2.10)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.12/site-packages (from requests->transformers==4.37.2) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.12/site-packages (from requests->transformers==4.37.2) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.12/site-packages (from requests->transformers==4.37.2) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.12/site-packages (from requests->transformers==4.37.2) (2024.12.14)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.12/site-packages (from torch>=1.9->optimum==1.12.0->optimum[onnxruntime]==1.12.0) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.12/site-packages (from torch>=1.9->optimum==1.12.0->optimum[onnxruntime]==1.12.0) (3.1.5)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /opt/conda/lib/python3.12/site-packages (from torch>=1.9->optimum==1.12.0->optimum[onnxruntime]==1.12.0) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /opt/conda/lib/python3.12/site-packages (from torch>=1.9->optimum==1.12.0->optimum[onnxruntime]==1.12.0) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /opt/conda/lib/python3.12/site-packages (from torch>=1.9->optimum==1.12.0->optimum[onnxruntime]==1.12.0) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /opt/conda/lib/python3.12/site-packages (from torch>=1.9->optimum==1.12.0->optimum[onnxruntime]==1.12.0) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /opt/conda/lib/python3.12/site-packages (from torch>=1.9->optimum==1.12.0->optimum[onnxruntime]==1.12.0) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /opt/conda/lib/python3.12/site-packages (from torch>=1.9->optimum==1.12.0->optimum[onnxruntime]==1.12.0) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /opt/conda/lib/python3.12/site-packages (from torch>=1.9->optimum==1.12.0->optimum[onnxruntime]==1.12.0) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /opt/conda/lib/python3.12/site-packages (from torch>=1.9->optimum==1.12.0->optimum[onnxruntime]==1.12.0) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /opt/conda/lib/python3.12/site-packages (from torch>=1.9->optimum==1.12.0->optimum[onnxruntime]==1.12.0) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /opt/conda/lib/python3.12/site-packages (from torch>=1.9->optimum==1.12.0->optimum[onnxruntime]==1.12.0) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /opt/conda/lib/python3.12/site-packages (from torch>=1.9->optimum==1.12.0->optimum[onnxruntime]==1.12.0) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /opt/conda/lib/python3.12/site-packages (from torch>=1.9->optimum==1.12.0->optimum[onnxruntime]==1.12.0) (12.4.127)\n",
      "Requirement already satisfied: triton==3.1.0 in /opt/conda/lib/python3.12/site-packages (from torch>=1.9->optimum==1.12.0->optimum[onnxruntime]==1.12.0) (3.1.0)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.12/site-packages (from torch>=1.9->optimum==1.12.0->optimum[onnxruntime]==1.12.0) (75.8.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.12/site-packages (from sympy->optimum==1.12.0->optimum[onnxruntime]==1.12.0) (1.3.0)\n",
      "Collecting sentencepiece!=0.1.92,>=0.1.91 (from transformers[sentencepiece]>=4.26.0->optimum==1.12.0->optimum[onnxruntime]==1.12.0)\n",
      "  Downloading sentencepiece-0.2.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /opt/conda/lib/python3.12/site-packages (from coloredlogs->optimum==1.12.0->optimum[onnxruntime]==1.12.0) (10.0)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /opt/conda/lib/python3.12/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets->optimum==1.12.0->optimum[onnxruntime]==1.12.0) (3.11.18)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.12/site-packages (from jinja2->torch>=1.9->optimum==1.12.0->optimum[onnxruntime]==1.12.0) (3.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.12/site-packages (from pandas->datasets->optimum==1.12.0->optimum[onnxruntime]==1.12.0) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.12/site-packages (from pandas->datasets->optimum==1.12.0->optimum[onnxruntime]==1.12.0) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.12/site-packages (from pandas->datasets->optimum==1.12.0->optimum[onnxruntime]==1.12.0) (2025.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/conda/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->optimum==1.12.0->optimum[onnxruntime]==1.12.0) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->optimum==1.12.0->optimum[onnxruntime]==1.12.0) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->optimum==1.12.0->optimum[onnxruntime]==1.12.0) (25.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->optimum==1.12.0->optimum[onnxruntime]==1.12.0) (1.6.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->optimum==1.12.0->optimum[onnxruntime]==1.12.0) (6.4.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/conda/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->optimum==1.12.0->optimum[onnxruntime]==1.12.0) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/conda/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->optimum==1.12.0->optimum[onnxruntime]==1.12.0) (1.20.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets->optimum==1.12.0->optimum[onnxruntime]==1.12.0) (1.17.0)\n",
      "Downloading transformers-4.37.2-py3-none-any.whl (8.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0mm\n",
      "Downloading optimum-1.12.0-py3-none-any.whl (380 kB)\n",
      "Downloading tokenizers-0.15.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "Downloading sentencepiece-0.2.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Installing collected packages: sentencepiece, tokenizers, transformers, optimum\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.21.1\n",
      "    Uninstalling tokenizers-0.21.1:\n",
      "      Successfully uninstalled tokenizers-0.21.1\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.48.3\n",
      "    Uninstalling transformers-4.48.3:\n",
      "      Successfully uninstalled transformers-4.48.3\n",
      "  Attempting uninstall: optimum\n",
      "    Found existing installation: optimum 1.24.0\n",
      "    Uninstalling optimum-1.24.0:\n",
      "      Successfully uninstalled optimum-1.24.0\n",
      "Successfully installed optimum-1.12.0 sentencepiece-0.2.0 tokenizers-0.15.2 transformers-4.37.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install \"transformers==4.37.2\" \"optimum[onnxruntime]==1.12.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ef634522-7782-488e-b7db-9087277cbc2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "354"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "import gc\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "99102de2-dcbf-4073-acfd-7bfc824d4508",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed2389032fd44bdf86b8a5837443c6c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/646 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "891c3a5755ea4c97a71eaf8dbed06f34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/25.1k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93d04158a85a4d25b7fc65d873dcc663",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98c54aa4d299452ba20c0a2929e0c408",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/9.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8750744d258d4d6387a6811c84e7e9d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/3.50G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4480de01751b4184a7b66bc84e636675",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce83db0a884448ba9c5d3dc89d3c1521",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"codellama/CodeLlama-7b-Instruct-hf\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model = AutoModelForCausalLM.from_pretrained(\"codellama/CodeLlama-7b-Instruct-hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7259a2ba-f3b9-4713-b0f0-fac43d8e294b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(32016, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32016, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1d01799a-d78c-44f4-8d25-484671ed776d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_size(model):\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    dtype_size = torch.finfo(model.dtype).bits // 8\n",
    "    \n",
    "    model_size_gb = total_params * dtype_size / (1024 ** 3)\n",
    "    # print(f\"Model size: {model_size_gb:2f} GB\")\n",
    "    return model_size_gb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a8390be8-f2f0-44d2-a514-31c9f5646a43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size: 25.103043 GB\n"
     ]
    }
   ],
   "source": [
    "get_model_size(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c6ba147-ff75-4ae3-a934-0fed0478d870",
   "metadata": {},
   "source": [
    "### Floating Point 15 params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "121c0545-e9ff-4873-a42e-3bcabf34231f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90fc795087b74fd0b2663ea7a6f0932f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"codellama/CodeLlama-7b-Instruct-hf\",\n",
    "    torch_dtype=torch.float16,\n",
    "    # device_map={\"\": 0},\n",
    "    # low_cpu_mem_usage=True\n",
    "    device_map=\"cuda:0\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c5b9097e-bc5f-482c-b5f6-8ec29b6977e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size: 12.551521 GB\n"
     ]
    }
   ],
   "source": [
    "get_model_size(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "77cc5210-f4f3-4842-a142-1a9ddb7f00ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun May 11 11:42:31 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 560.35.05              Driver Version: 560.35.05      CUDA Version: 12.6     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA A100 80GB PCIe          Off |   00000000:27:00.0 Off |                    0 |\n",
      "| N/A   57C    P0             80W /  300W |   14139MiB /  81920MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d2f3cac4-bf69-4588-aca5-a97eb7d4a42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"/mnt/object_group/data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "90160599-5139-4efe-af50-0e539dfcd398",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    {\"comment_id\": 74291026, \"comment_user_login\": \"vikerman\", \"comment_body\": \"Is there a better way to avoid having this in different places?\\n\", \"comment_created_at\": \"2016-08-10T17:30:42+00:00\", \"comment_html_url\": \"https://github.com/angular/angular/pull/10620#discussion_r74291026\", \"comment_path\": \"modules/@angular/common/src/forms-deprecated/directives/abstract_control_directive.ts\", \"comment_position\": 10, \"comment_original_position\": 10, \"comment_commit_id\": \"ee8e802b7bc25eafbe54109b3924e9dbc36dce11\", \"comment_original_commit_id\": \"ee8e802b7bc25eafbe54109b3924e9dbc36dce11\", \"diff_line_content\": \"}\\n\", \"diff_line_type\": \"added\", \"diff_line_source_no\": null, \"diff_line_target_no\": 15, \"diff_hunk_header\": \"\", \"diff\": \"@@ -6,10 +6,13 @@\\n  * found in the LICENSE file at https://angular.io/license\\n  */\\n \\n-import {unimplemented} from '../../facade/exceptions';\\n+import {BaseException} from '@angular/core';\\n import {isPresent} from '../../facade/lang';\\n import {AbstractControl} from '../model';\\n \\n+function unimplemented(): any {\\n+  throw new BaseException('unimplemented');\", \"side\": \"RIGHT\", \"line_offset\": 9, \"diff_file_source\": \"a/modules/@angular/common/src/forms-deprecated/directives/abstract_control_directive.ts\", \"diff_file_target\": \"b/modules/@angular/common/src/forms-deprecated/directives/abstract_control_directive.ts\", \"repo\": \"angular/angular\"}\n",
      "    {\"comment_id\": 74305223, \"comment_user_login\": \"alexeagle\", \"comment_body\": \"we could export it in the public API but that's not nice.\\nOr we could inline the method and delete it.\\n\", \"comment_created_at\": \"2016-08-10T18:42:59+00:00\", \"comment_html_url\": \"https://github.com/angular/angular/pull/10620#discussion_r74305223\", \"comment_path\": \"modules/@angular/common/src/forms-deprecated/directives/abstract_control_directive.ts\", \"comment_position\": 10, \"comment_original_position\": 10, \"comment_commit_id\": \"ee8e802b7bc25eafbe54109b3924e9dbc36dce11\", \"comment_original_commit_id\": \"ee8e802b7bc25eafbe54109b3924e9dbc36dce11\", \"diff_line_content\": \"}\\n\", \"diff_line_type\": \"added\", \"diff_line_source_no\": null, \"diff_line_target_no\": 15, \"diff_hunk_header\": \"\", \"diff\": \"@@ -6,10 +6,13 @@\\n  * found in the LICENSE file at https://angular.io/license\\n  */\\n \\n-import {unimplemented} from '../../facade/exceptions';\\n+import {BaseException} from '@angular/core';\\n import {isPresent} from '../../facade/lang';\\n import {AbstractControl} from '../model';\\n \\n+function unimplemented(): any {\\n+  throw new BaseException('unimplemented');\", \"side\": \"RIGHT\", \"line_offset\": 9, \"diff_file_source\": \"a/modules/@angular/common/src/forms-deprecated/directives/abstract_control_directive.ts\", \"diff_file_target\": \"b/modules/@angular/common/src/forms-deprecated/directives/abstract_control_directive.ts\", \"repo\": \"angular/angular\"}\n",
      "    {\"comment_id\": 74523490, \"comment_user_login\": \"vicb\", \"comment_body\": \"const\\n\", \"comment_created_at\": \"2016-08-11T23:43:20+00:00\", \"comment_html_url\": \"https://github.com/angular/angular/pull/10622#discussion_r74523490\", \"comment_path\": \"modules/@angular/compiler-cli/integrationtest/test/basic_spec.ts\", \"comment_position\": 36, \"comment_original_position\": 36, \"comment_commit_id\": \"616c5c56924b77bc0ef539d01d1cf3596c0da87f\", \"comment_original_commit_id\": \"91f50b2f513f99cf56b7406b3a74066e128b9c7d\", \"diff_line_content\": \"    it('should support i18n for content tags', () => {\\n\", \"diff_line_type\": \"added\", \"diff_line_source_no\": null, \"diff_line_target_no\": 79, \"diff_hunk_header\": \"describe('template codegen output', () => {\", \"diff\": \"@@ -44,24 +44,40 @@ describe('template codegen output', () => {\\n   it('should support ngIf', () => {\\n     var compFixture = createComponent(BasicComp);\\n     var debugElement = compFixture.debugElement;\\n-    expect(debugElement.children.length).toBe(2);\\n+    expect(debugElement.children.length).toBe(3);\\n \\n     compFixture.componentInstance.ctxBool = true;\\n     compFixture.detectChanges();\\n-    expect(debugElement.children.length).toBe(3);\\n+    expect(debugElement.children.length).toBe(4);\\n     expect(debugElement.children[2].injector.get(MultipleComponentsMyComp)).toBeTruthy();\\n   });\\n \\n   it('should support ngFor', () => {\\n     var compFixture = createComponent(BasicComp);\\n     var debugElement = compFixture.debugElement;\\n-    expect(debugElement.children.length).toBe(2);\\n+    expect(debugElement.children.length).toBe(3);\\n \\n     // test NgFor\\n     compFixture.componentInstance.ctxArr = [1, 2];\\n     compFixture.detectChanges();\\n-    expect(debugElement.children.length).toBe(4);\\n+    expect(debugElement.children.length).toBe(5);\\n     expect(debugElement.children[2].attributes['value']).toBe('1');\\n     expect(debugElement.children[3].attributes['value']).toBe('2');\\n   });\\n+\\n+  describe('i18n', () => {\\n+    it('should support i18n for content tags', () => {\\n+      var compFixture = createComponent(BasicComp);\\n+      expect(compFixture.componentInstance.ctxLocale).toEqual('FI');\\n+    });\\n+\\n+    it('should support i18n for content tags', () => {\\n+      var compFixture = createComponent(BasicComp);\", \"side\": \"RIGHT\", \"line_offset\": 35, \"diff_file_source\": \"a/modules/@angular/compiler-cli/integrationtest/test/basic_spec.ts\", \"diff_file_target\": \"b/modules/@angular/compiler-cli/integrationtest/test/basic_spec.ts\", \"repo\": \"angular/angular\"}\n",
      "    {\"comment_id\": 74523504, \"comment_user_login\": \"vicb\", \"comment_body\": \"const\\n\", \"comment_created_at\": \"2016-08-11T23:43:27+00:00\", \"comment_html_url\": \"https://github.com/angular/angular/pull/10622#discussion_r74523504\", \"comment_path\": \"modules/@angular/compiler-cli/integrationtest/test/basic_spec.ts\", \"comment_position\": 31, \"comment_original_position\": 31, \"comment_commit_id\": \"616c5c56924b77bc0ef539d01d1cf3596c0da87f\", \"comment_original_commit_id\": \"91f50b2f513f99cf56b7406b3a74066e128b9c7d\", \"diff_line_content\": \"    it('should inject the translations format into the component', () => {\\n\", \"diff_line_type\": \"added\", \"diff_line_source_no\": null, \"diff_line_target_no\": 74, \"diff_hunk_header\": \"describe('template codegen output', () => {\", \"diff\": \"@@ -44,24 +44,40 @@ describe('template codegen output', () => {\\n   it('should support ngIf', () => {\\n     var compFixture = createComponent(BasicComp);\\n     var debugElement = compFixture.debugElement;\\n-    expect(debugElement.children.length).toBe(2);\\n+    expect(debugElement.children.length).toBe(3);\\n \\n     compFixture.componentInstance.ctxBool = true;\\n     compFixture.detectChanges();\\n-    expect(debugElement.children.length).toBe(3);\\n+    expect(debugElement.children.length).toBe(4);\\n     expect(debugElement.children[2].injector.get(MultipleComponentsMyComp)).toBeTruthy();\\n   });\\n \\n   it('should support ngFor', () => {\\n     var compFixture = createComponent(BasicComp);\\n     var debugElement = compFixture.debugElement;\\n-    expect(debugElement.children.length).toBe(2);\\n+    expect(debugElement.children.length).toBe(3);\\n \\n     // test NgFor\\n     compFixture.componentInstance.ctxArr = [1, 2];\\n     compFixture.detectChanges();\\n-    expect(debugElement.children.length).toBe(4);\\n+    expect(debugElement.children.length).toBe(5);\\n     expect(debugElement.children[2].attributes['value']).toBe('1');\\n     expect(debugElement.children[3].attributes['value']).toBe('2');\\n   });\\n+\\n+  describe('i18n', () => {\\n+    it('should support i18n for content tags', () => {\\n+      var compFixture = createComponent(BasicComp);\", \"side\": \"RIGHT\", \"line_offset\": 30, \"diff_file_source\": \"a/modules/@angular/compiler-cli/integrationtest/test/basic_spec.ts\", \"diff_file_target\": \"b/modules/@angular/compiler-cli/integrationtest/test/basic_spec.ts\", \"repo\": \"angular/angular\"}\n",
      "    {\"comment_id\": 74526501, \"comment_user_login\": \"vicb\", \"comment_body\": \"fi\\n\", \"comment_created_at\": \"2016-08-12T00:18:35+00:00\", \"comment_html_url\": \"https://github.com/angular/angular/pull/10622#discussion_r74526501\", \"comment_path\": \"modules/@angular/compiler-cli/integrationtest/test/basic_spec.ts\", \"comment_position\": 32, \"comment_original_position\": 32, \"comment_commit_id\": \"616c5c56924b77bc0ef539d01d1cf3596c0da87f\", \"comment_original_commit_id\": \"7333b4c6d9527a1a6b812ab6eea2797f228d03bd\", \"diff_line_content\": \"      const compFixture = createComponent(BasicComp);\\n\", \"diff_line_type\": \"added\", \"diff_line_source_no\": null, \"diff_line_target_no\": 75, \"diff_hunk_header\": \"describe('template codegen output', () => {\", \"diff\": \"@@ -44,24 +44,45 @@ describe('template codegen output', () => {\\n   it('should support ngIf', () => {\\n     var compFixture = createComponent(BasicComp);\\n     var debugElement = compFixture.debugElement;\\n-    expect(debugElement.children.length).toBe(2);\\n+    expect(debugElement.children.length).toBe(3);\\n \\n     compFixture.componentInstance.ctxBool = true;\\n     compFixture.detectChanges();\\n-    expect(debugElement.children.length).toBe(3);\\n+    expect(debugElement.children.length).toBe(4);\\n     expect(debugElement.children[2].injector.get(MultipleComponentsMyComp)).toBeTruthy();\\n   });\\n \\n   it('should support ngFor', () => {\\n     var compFixture = createComponent(BasicComp);\\n     var debugElement = compFixture.debugElement;\\n-    expect(debugElement.children.length).toBe(2);\\n+    expect(debugElement.children.length).toBe(3);\\n \\n     // test NgFor\\n     compFixture.componentInstance.ctxArr = [1, 2];\\n     compFixture.detectChanges();\\n-    expect(debugElement.children.length).toBe(4);\\n+    expect(debugElement.children.length).toBe(5);\\n     expect(debugElement.children[2].attributes['value']).toBe('1');\\n     expect(debugElement.children[3].attributes['value']).toBe('2');\\n   });\\n+\\n+  describe('i18n', () => {\\n+    it('should inject the locale into the component', () => {\\n+      const compFixture = createComponent(BasicComp);\\n+      expect(compFixture.componentInstance.ctxLocale).toEqual('FI');\", \"side\": \"RIGHT\", \"line_offset\": 31, \"diff_file_source\": \"a/modules/@angular/compiler-cli/integrationtest/test/basic_spec.ts\", \"diff_file_target\": \"b/modules/@angular/compiler-cli/integrationtest/test/basic_spec.ts\", \"repo\": \"angular/angular\"}\n",
      "    {\"comment_id\": 74526531, \"comment_user_login\": \"vicb\", \"comment_body\": \"remove !\\n\", \"comment_created_at\": \"2016-08-12T00:19:06+00:00\", \"comment_html_url\": \"https://github.com/angular/angular/pull/10622#discussion_r74526531\", \"comment_path\": \"modules/@angular/compiler-cli/src/codegen.ts\", \"comment_position\": 36, \"comment_original_position\": 36, \"comment_commit_id\": \"616c5c56924b77bc0ef539d01d1cf3596c0da87f\", \"comment_original_commit_id\": \"7333b4c6d9527a1a6b812ab6eea2797f228d03bd\", \"diff_line_content\": \"    const reflectorHost = new ReflectorHost(program, compilerHost, options, reflectorHostContext);\\n\", \"diff_line_type\": \"context\", \"diff_line_source_no\": 132, \"diff_line_target_no\": 142, \"diff_hunk_header\": \"export class CodeGenerator {\", \"diff\": \"@@ -88,6 +93,35 @@ export class CodeGenerator {\\n     return path.join(this.options.genDir, relativePath);\\n   }\\n \\n+  private _constructTranslationBundle(fileMetas: any[], analyzedNgModules: any): Promise<any> {\", \"side\": \"RIGHT\", \"line_offset\": 11, \"diff_file_source\": \"a/modules/@angular/compiler-cli/src/codegen.ts\", \"diff_file_target\": \"b/modules/@angular/compiler-cli/src/codegen.ts\", \"repo\": \"angular/angular\"}\n",
      "    {\"comment_id\": 74526731, \"comment_user_login\": \"vicb\", \"comment_body\": \"append at the end to make diff readable\\n\", \"comment_created_at\": \"2016-08-12T00:21:49+00:00\", \"comment_html_url\": \"https://github.com/angular/angular/pull/10622#discussion_r74526731\", \"comment_path\": \"modules/@angular/compiler-cli/src/codegen.ts\", \"comment_position\": 26, \"comment_original_position\": 26, \"comment_commit_id\": \"616c5c56924b77bc0ef539d01d1cf3596c0da87f\", \"comment_original_commit_id\": \"7333b4c6d9527a1a6b812ab6eea2797f228d03bd\", \"diff_line_content\": \"      }\\n\", \"diff_line_type\": \"context\", \"diff_line_source_no\": 129, \"diff_line_target_no\": 132, \"diff_hunk_header\": \"export class CodeGenerator {\", \"diff\": \"@@ -32,7 +35,9 @@ const PREAMBLE = `/**\\n \\n export class CodeGenerator {\\n   constructor(\\n-      private options: AngularCompilerOptions, private program: ts.Program,\\n+      private options: AngularCompilerOptions, private transContent: string,\", \"side\": \"RIGHT\", \"line_offset\": 1, \"diff_file_source\": \"a/modules/@angular/compiler-cli/src/codegen.ts\", \"diff_file_target\": \"b/modules/@angular/compiler-cli/src/codegen.ts\", \"repo\": \"angular/angular\"}\n",
      "    {\"comment_id\": 74527376, \"comment_user_login\": \"vicb\", \"comment_body\": \"This is the only line that you should keep beside loading the file\\n\", \"comment_created_at\": \"2016-08-12T00:30:39+00:00\", \"comment_html_url\": \"https://github.com/angular/angular/pull/10622#discussion_r74527376\", \"comment_path\": \"modules/@angular/compiler-cli/src/codegen.ts\", \"comment_position\": 46, \"comment_original_position\": 130, \"comment_commit_id\": \"616c5c56924b77bc0ef539d01d1cf3596c0da87f\", \"comment_original_commit_id\": \"7333b4c6d9527a1a6b812ab6eea2797f228d03bd\", \"diff_line_content\": \"        new NgModuleCompiler(), new TypeScriptEmitter(reflectorHost), cliOptions.locale,\\n\", \"diff_line_type\": \"added\", \"diff_line_source_no\": null, \"diff_line_target_no\": 164, \"diff_hunk_header\": \"export class CodeGenerator {\", \"diff\": \"@@ -132,7 +175,7 @@ export class CodeGenerator {\\n     const reflectorHost = new ReflectorHost(program, compilerHost, options, reflectorHostContext);\\n     const staticReflector = new StaticReflector(reflectorHost);\\n     StaticAndDynamicReflectionCapabilities.install(staticReflector);\\n-    const htmlParser = new compiler.i18n.HtmlParser(new HtmlParser());\\n+    const htmlParser = new compiler.i18n.HtmlParser(new HtmlParser(), transContent);\", \"side\": \"RIGHT\", \"line_offset\": 3, \"diff_file_source\": \"a/modules/@angular/compiler-cli/src/codegen.ts\", \"diff_file_target\": \"b/modules/@angular/compiler-cli/src/codegen.ts\", \"repo\": \"angular/angular\"}\n",
      "    {\"comment_id\": 74528827, \"comment_user_login\": \"vicb\", \"comment_body\": \"Id\\n\", \"comment_created_at\": \"2016-08-12T00:51:47+00:00\", \"comment_html_url\": \"https://github.com/angular/angular/pull/10622#discussion_r74528827\", \"comment_path\": \"modules/@angular/compiler-cli/integrationtest/src/basic.ts\", \"comment_position\": 15, \"comment_original_position\": 15, \"comment_commit_id\": \"616c5c56924b77bc0ef539d01d1cf3596c0da87f\", \"comment_original_commit_id\": \"e7940fc0e83c9d0a549891774db0a609ec591c19\", \"diff_line_content\": \"  }\\n\", \"diff_line_type\": \"added\", \"diff_line_source_no\": null, \"diff_line_target_no\": 30, \"diff_hunk_header\": \"export class BasicComp {\", \"diff\": \"@@ -23,5 +23,9 @@ export class BasicComp {\\n   ctxProp: string;\\n   ctxBool: boolean;\\n   ctxArr: any[] = [];\\n-  constructor() { this.ctxProp = 'initialValue'; }\\n+  constructor(\\n+      @Inject(LOCALE_ID) public localeID: string,\", \"side\": \"RIGHT\", \"line_offset\": 7, \"diff_file_source\": \"a/modules/@angular/compiler-cli/integrationtest/src/basic.ts\", \"diff_file_target\": \"b/modules/@angular/compiler-cli/integrationtest/src/basic.ts\", \"repo\": \"angular/angular\"}\n",
      "    {\"comment_id\": 74528853, \"comment_user_login\": \"vicb\", \"comment_body\": \"only add the format\\n\", \"comment_created_at\": \"2016-08-12T00:52:10+00:00\", \"comment_html_url\": \"https://github.com/angular/angular/pull/10622#discussion_r74528853\", \"comment_path\": \"modules/@angular/compiler-cli/src/extract_i18n.ts\", \"comment_position\": 6, \"comment_original_position\": 6, \"comment_commit_id\": \"616c5c56924b77bc0ef539d01d1cf3596c0da87f\", \"comment_original_commit_id\": \"e7940fc0e83c9d0a549891774db0a609ec591c19\", \"diff_line_content\": \"  const extractor = Extractor.create(ngOptions, cliOptions.i18nFormat, program, host);\\n\", \"diff_line_type\": \"added\", \"diff_line_source_no\": null, \"diff_line_target_no\": 33, \"diff_hunk_header\": \"import {StaticAndDynamicReflectionCapabilities} from './static_reflection_capabi\", \"diff\": \"@@ -28,8 +28,9 @@ import {StaticAndDynamicReflectionCapabilities} from './static_reflection_capabi\\n import {StaticReflector, StaticSymbol} from './static_reflector';\\n \\n function extract(\\n-    ngOptions: tsc.AngularCompilerOptions, program: ts.Program, host: ts.CompilerHost) {\\n-  const extractor = Extractor.create(ngOptions, program, host);\\n+    ngOptions: tsc.AngularCompilerOptions, cliOptions: tsc.CliOptions, program: ts.Program,\", \"side\": \"RIGHT\", \"line_offset\": 5, \"diff_file_source\": \"a/modules/@angular/compiler-cli/src/extract_i18n.ts\", \"diff_file_target\": \"b/modules/@angular/compiler-cli/src/extract_i18n.ts\", \"repo\": \"angular/angular\"}\n",
      "    {\"comment_id\": 74530724, \"comment_user_login\": \"vicb\", \"comment_body\": \"`, null, null` would be more semantically correct\\n\", \"comment_created_at\": \"2016-08-12T01:22:30+00:00\", \"comment_html_url\": \"https://github.com/angular/angular/pull/10622#discussion_r74530724\", \"comment_path\": \"modules/@angular/compiler-cli/src/extract_i18n.ts\", \"comment_position\": 37, \"comment_original_position\": 37, \"comment_commit_id\": \"616c5c56924b77bc0ef539d01d1cf3596c0da87f\", \"comment_original_commit_id\": \"6781377043d6eae20d6504b83a121ebde353f043\", \"diff_line_content\": \"}\\n\", \"diff_line_type\": \"added\", \"diff_line_source_no\": null, \"diff_line_target_no\": 196, \"diff_hunk_header\": \"interface FileMetadata {\", \"diff\": \"@@ -163,7 +164,7 @@ export class Extractor {\\n         config, console, elementSchemaRegistry, staticReflector);\\n     const offlineCompiler = new compiler.OfflineCompiler(\\n         resolver, normalizer, tmplParser, new StyleCompiler(urlResolver), new ViewCompiler(config),\\n-        new NgModuleCompiler(), new TypeScriptEmitter(reflectorHost));\\n+        new NgModuleCompiler(), new TypeScriptEmitter(reflectorHost), '', translationsFormat);\", \"side\": \"RIGHT\", \"line_offset\": 12, \"diff_file_source\": \"a/modules/@angular/compiler-cli/src/extract_i18n.ts\", \"diff_file_target\": \"b/modules/@angular/compiler-cli/src/extract_i18n.ts\", \"repo\": \"angular/angular\"}\n"
     ]
    }
   ],
   "source": [
    "train = \"/mnt/object_group/data/processed/train.jsonl.gz\"\n",
    "with gzip.open(train, 'rt', encoding='utf-8') as f:\n",
    "    for i, line in enumerate(f):\n",
    "        print(\"    \" + line.strip())\n",
    "        if i >= 10:\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "86df7928-62f7-4ac6-8f93-e08ce6a4b75a",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_ROOT = Path(\"/mnt/object_group/data/processed\")  # Adjust as needed\n",
    "\n",
    "def load_samples(limit_per_split=10):\n",
    "    diff_samples = []\n",
    "    for repo_dir in DATA_ROOT.iterdir():\n",
    "        if not repo_dir.is_dir():\n",
    "            continue\n",
    "        for pr_dir in (repo_dir / \"diff\").iterdir():\n",
    "            pr_id = pr_dir.name\n",
    "            diff_file = pr_dir\n",
    "            comments_file = DATA_ROOT / repo_dir.name / \"comments\" / f\"{repo_dir.name}_{pr_id}_comments.jsonl\"\n",
    "\n",
    "            if not diff_file.exists() or not comments_file.exists():\n",
    "                continue\n",
    "\n",
    "            with open(diff_file, \"r\", encoding=\"utf-8\") as df:\n",
    "                diff_content = df.read()\n",
    "\n",
    "            with open(comments_file, \"r\", encoding=\"utf-8\") as cf:\n",
    "                for line in cf:\n",
    "                    try:\n",
    "                        comment = json.loads(line)\n",
    "                        offset = comment.get(\"original_position\")\n",
    "                        side = comment.get(\"side\", \"RIGHT\")\n",
    "                        body = comment.get(\"body\", \"\").strip()\n",
    "                        if offset is not None and body:\n",
    "                            sample = {\n",
    "                                \"input\": f\"<DIFF>\\n{diff_content}\\n</DIFF>\\n<COMMENT side=\\\"{side}\\\" offset=\\\"{offset}\\\">\",\n",
    "                                \"output\": body\n",
    "                            }\n",
    "                            diff_samples.append(sample)\n",
    "                    except json.JSONDecodeError:\n",
    "                        continue\n",
    "\n",
    "    random.shuffle(diff_samples)\n",
    "    return {\n",
    "        \"train\": diff_samples[:limit_per_split],\n",
    "        \"test\": diff_samples[limit_per_split:2*limit_per_split],\n",
    "        \"eval\": diff_samples[2*limit_per_split:3*limit_per_split],\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c70b5a55-a831-4432-ae15-5b910735780f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_jsonl_gz(file_path, limit):\n",
    "    samples = []\n",
    "    with gzip.open(file_path, 'rt', encoding='utf-8') as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if i >= limit:\n",
    "                break\n",
    "            entry = json.loads(line)\n",
    "            prompt = f\"<DIFF>\\n{entry['diff']}\\n</DIFF>\\n\"\n",
    "            comment = f\"<COMMENT side=\\\"{entry['side']}\\\" offset=\\\"{entry['line_offset']}\\\">{entry['comment_body']}\"\n",
    "            samples.append({'input': prompt, 'label': comment})\n",
    "    return samples\n",
    "\n",
    "train_data = load_jsonl_gz(\"/mnt/object_group/data/processed/train.jsonl.gz\", 10)\n",
    "test_data  = load_jsonl_gz(\"/mnt/object_group/data/processed/test.jsonl.gz\", 8)\n",
    "eval_data  = load_jsonl_gz(\"/mnt/object_group/data/processed/val.jsonl.gz\", 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a0fb7e8b-f409-4557-b4e6-61fafc226c9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input', 'label'])\n",
      "<DIFF>\n",
      "@@ -6,10 +6,13 @@\n",
      "  * found in the LICENSE file at https://angular.io/license\n",
      "  */\n",
      " \n",
      "-import {unimplemented} from '../../facade/exceptions';\n",
      "+import {BaseException} from '@angular/core';\n",
      " import {isPresent} from '../../facade/lang';\n",
      " import {AbstractControl} from '../model';\n",
      " \n",
      "+function unimplemented(): any {\n",
      "+  throw new BaseException('unimplemented');\n",
      "</DIFF>\n",
      "\n",
      "<COMMENT side=\"RIGHT\" offset=\"9\">Is there a better way to avoid having this in different places?\n",
      "\n",
      "<DIFF>\n",
      "@@ -6,10 +6,13 @@\n",
      "  * found in the LICENSE file at https://angular.io/license\n",
      "  */\n",
      " \n",
      "-import {unimplemented} from '../../facade/exceptions';\n",
      "+import {BaseException} from '@angular/core';\n",
      " import {isPresent} from '../../facade/lang';\n",
      " import {AbstractControl} from '../model';\n",
      " \n",
      "+function unimplemented(): any {\n",
      "+  throw new BaseException('unimplemented');\n",
      "</DIFF>\n",
      "\n",
      "<COMMENT side=\"RIGHT\" offset=\"9\">we could export it in the public API but that's not nice.\n",
      "Or we could inline the method and delete it.\n",
      "\n",
      "<DIFF>\n",
      "@@ -44,24 +44,40 @@ describe('template codegen output', () => {\n",
      "   it('should support ngIf', () => {\n",
      "     var compFixture = createComponent(BasicComp);\n",
      "     var debugElement = compFixture.debugElement;\n",
      "-    expect(debugElement.children.length).toBe(2);\n",
      "+    expect(debugElement.children.length).toBe(3);\n",
      " \n",
      "     compFixture.componentInstance.ctxBool = true;\n",
      "     compFixture.detectChanges();\n",
      "-    expect(debugElement.children.length).toBe(3);\n",
      "+    expect(debugElement.children.length).toBe(4);\n",
      "     expect(debugElement.children[2].injector.get(MultipleComponentsMyComp)).toBeTruthy();\n",
      "   });\n",
      " \n",
      "   it('should support ngFor', () => {\n",
      "     var compFixture = createComponent(BasicComp);\n",
      "     var debugElement = compFixture.debugElement;\n",
      "-    expect(debugElement.children.length).toBe(2);\n",
      "+    expect(debugElement.children.length).toBe(3);\n",
      " \n",
      "     // test NgFor\n",
      "     compFixture.componentInstance.ctxArr = [1, 2];\n",
      "     compFixture.detectChanges();\n",
      "-    expect(debugElement.children.length).toBe(4);\n",
      "+    expect(debugElement.children.length).toBe(5);\n",
      "     expect(debugElement.children[2].attributes['value']).toBe('1');\n",
      "     expect(debugElement.children[3].attributes['value']).toBe('2');\n",
      "   });\n",
      "+\n",
      "+  describe('i18n', () => {\n",
      "+    it('should support i18n for content tags', () => {\n",
      "+      var compFixture = createComponent(BasicComp);\n",
      "+      expect(compFixture.componentInstance.ctxLocale).toEqual('FI');\n",
      "+    });\n",
      "+\n",
      "+    it('should support i18n for content tags', () => {\n",
      "+      var compFixture = createComponent(BasicComp);\n",
      "</DIFF>\n",
      "\n",
      "<COMMENT side=\"RIGHT\" offset=\"35\">const\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(train_data[0].keys())\n",
    "\n",
    "for i in range(3):\n",
    "    print(train_data[i][\"input\"])\n",
    "    print(train_data[i][\"label\"])\n",
    "# print(train_data[0][\"input\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4009389d-1604-4d45-8b29-356c1eb50ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt template\n",
    "instruction = (\n",
    "    \"You are a helpful code reviewer. \"\n",
    "    \"Given a code diff, generate all relevant review comments. \"\n",
    "    \"Each comment must be in the format: \"\n",
    "    \"<COMMENT side=\\\"RIGHT\\\" offset=\\\"X\\\">Your comment here.\\n\"\n",
    "    \"Only output comments, nothing else.\\n\\n\"\n",
    "    \"### Code Diff:\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5eb6f797-96f7-44ad-b438-ed3c12e850ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_comments(text):\n",
    "    pattern = re.compile(r'<COMMENT side=\"(?P<side>[^\"]+)\" offset=\"(?P<offset>\\d+)\">(?P<comment>.+)')\n",
    "    results = []\n",
    "    for line in text.strip().splitlines():\n",
    "        match = pattern.match(line.strip())\n",
    "        if match:\n",
    "            results.append({\n",
    "                \"side\": match.group(\"side\"),\n",
    "                \"offset\": int(match.group(\"offset\")),\n",
    "                \"comment\": match.group(\"comment\").strip()\n",
    "            })\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "bad0089a-5b53-4620-9ee6-2cb46efef9a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32016, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32016, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ec15c265-2cbe-4477-b5d4-5661257c702d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def offset_match(pred_comments, true_comments, window=3):\n",
    "    correct = 0\n",
    "    total = len(pred_comments)\n",
    "\n",
    "    gt_pairs = {(c[\"side\"], c[\"offset\"]) for c in true_comments}\n",
    "    matched = set()\n",
    "\n",
    "    for pred in pred_comments:\n",
    "        pred_side, pred_offset = pred[\"side\"], pred[\"offset\"]\n",
    "        for gt in gt_pairs:\n",
    "            gt_side, gt_offset = gt\n",
    "            if pred_side == gt_side and abs(pred_offset - gt_offset) <= window:\n",
    "                matched.add(gt)\n",
    "                correct += 1\n",
    "                break  # count each pred only once\n",
    "\n",
    "    return correct, total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e72a6268-8a4e-4601-ae46-37fea7b358fd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Sample 0 ===\n",
      "[INST] You are a helpful code reviewer. Given a code diff, generate all relevant review comments. Each comment must be in the format: <COMMENT side=\"RIGHT\" offset=\"X\">Your comment here.\n",
      "Only output comments, nothing else.\n",
      "\n",
      "### Code Diff:\n",
      "<DIFF>\n",
      "@@ -838,45 +839,58 @@ function performConcurrentWorkOnRoot(root, didTimeout) {\n",
      "       throw fatalError;\n",
      "     }\n",
      " \n",
      "-    // Check if this render may have yielded to a concurrent event, and if so,\n",
      "-    // confirm that any newly rendered stores are consistent.\n",
      "-    // TODO: It's possible that even a concurrent render may never have yielded\n",
      "-    // to the main thread, if it was fast enough, or if it expired. We could\n",
      "-    // skip the consistency check in that case, too.\n",
      "-    const renderWasConcurrent = !includesBlockingLane(root, lanes);\n",
      "-    const finishedWork: Fiber = (root.current.alternate: any);\n",
      "-    if (\n",
      "-      renderWasConcurrent &&\n",
      "-      !isRenderConsistentWithExternalStores(finishedWork)\n",
      "-    ) {\n",
      "-      // A store was mutated in an interleaved event. Render again,\n",
      "-      // synchronously, to block further mutations.\n",
      "-      exitStatus = renderRootSync(root, lanes);\n",
      "-\n",
      "-      // We need to check again if something threw\n",
      "-      if (exitStatus === RootErrored) {\n",
      "-        const errorRetryLanes = getLanesToRetrySynchronouslyOnError(root);\n",
      "-        if (errorRetryLanes !== NoLanes) {\n",
      "-          lanes = errorRetryLanes;\n",
      "-          exitStatus = recoverFromConcurrentError(root, errorRetryLanes);\n",
      "-          // We assume the tree is now consistent because we didn't yield to any\n",
      "-          // concurrent events.\n",
      "+    if (exitStatus === RootDidNotComplete) {\n",
      "</DIFF> [/INST]  <COMMENT side=\"RIGHT\" offset=\"838\">This code looks like it's checking if a render was concurrent and if it was consistent with external stores. It's also checking if the render was interrupted by an error and if so, it's retrying the render synchronously.</COMMENT>\n",
      "\n",
      "<COMMENT side=\"RIGHT\" offset=\"840\">It's possible that even a concurrent render may never have yielded to the main thread, if it was fast enough, or if it expired. We could skip the consistency check in that case, too.</COMMENT>\n",
      "\n",
      "<COMMENT side=\"RIGHT\" offset=\"842\">The code is checking if the render was concurrent and if it was consistent with external stores. If it was not consistent, it's retrying the render synchronously.</COMMENT>\n",
      "\n",
      "<COMMENT side=\"RIGHT\" offset=\"844\">It's assuming that the tree is now consistent because it didn't yield to any concurrent events.</COMMENT>\n",
      "\n",
      "<COMMENT side=\"RIGHT\" offset=\"846\">It's checking if the\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Sample 1 ===\n",
      "[INST] You are a helpful code reviewer. Given a code diff, generate all relevant review comments. Each comment must be in the format: <COMMENT side=\"RIGHT\" offset=\"X\">Your comment here.\n",
      "Only output comments, nothing else.\n",
      "\n",
      "### Code Diff:\n",
      "<DIFF>\n",
      "@@ -838,45 +839,58 @@ function performConcurrentWorkOnRoot(root, didTimeout) {\n",
      "       throw fatalError;\n",
      "     }\n",
      " \n",
      "-    // Check if this render may have yielded to a concurrent event, and if so,\n",
      "-    // confirm that any newly rendered stores are consistent.\n",
      "-    // TODO: It's possible that even a concurrent render may never have yielded\n",
      "-    // to the main thread, if it was fast enough, or if it expired. We could\n",
      "-    // skip the consistency check in that case, too.\n",
      "-    const renderWasConcurrent = !includesBlockingLane(root, lanes);\n",
      "-    const finishedWork: Fiber = (root.current.alternate: any);\n",
      "-    if (\n",
      "-      renderWasConcurrent &&\n",
      "-      !isRenderConsistentWithExternalStores(finishedWork)\n",
      "-    ) {\n",
      "-      // A store was mutated in an interleaved event. Render again,\n",
      "-      // synchronously, to block further mutations.\n",
      "-      exitStatus = renderRootSync(root, lanes);\n",
      "-\n",
      "-      // We need to check again if something threw\n",
      "-      if (exitStatus === RootErrored) {\n",
      "-        const errorRetryLanes = getLanesToRetrySynchronouslyOnError(root);\n",
      "-        if (errorRetryLanes !== NoLanes) {\n",
      "-          lanes = errorRetryLanes;\n",
      "-          exitStatus = recoverFromConcurrentError(root, errorRetryLanes);\n",
      "-          // We assume the tree is now consistent because we didn't yield to any\n",
      "-          // concurrent events.\n",
      "+    if (exitStatus === RootDidNotComplete) {\n",
      "+      // The render unwound without completing the tree. This happens in special\n",
      "+      // cases where need to exit the current render without producing a\n",
      "+      // consistent tree or committing.\n",
      "+      //\n",
      "+      // This should only happen during a concurrent render, not a discrete or\n",
      "+      // synchronous update. We should have already checked for this when we\n",
      "+      // unwound the stack.\n",
      "</DIFF> [/INST]  <COMMENT side=\"RIGHT\" offset=\"838\">This code looks like it's checking if a render was concurrent and if it was consistent with external stores. However, it's not clear why this check is necessary or what the implications of this check are.</COMMENT>\n",
      "\n",
      "<COMMENT side=\"RIGHT\" offset=\"845\">This code is checking if a render was concurrent and if it was consistent with external stores. If the render was concurrent and the stores were not consistent, it will re-render the tree synchronously to block further mutations.</COMMENT>\n",
      "\n",
      "<COMMENT side=\"RIGHT\" offset=\"853\">This code is checking if a render was concurrent and if it was consistent with external stores. If the render was concurrent and the stores were not consistent, it will re-render the tree synchronously to block further mutations. However, it's not clear why this check is necessary or what the implications of this check are.</COMMENT>\n",
      "\n",
      "<COMMENT side=\"RIGHT\" offset=\"861\">This code is checking if a render was concurrent and if it was consistent with external stores. If the render\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Sample 2 ===\n",
      "[INST] You are a helpful code reviewer. Given a code diff, generate all relevant review comments. Each comment must be in the format: <COMMENT side=\"RIGHT\" offset=\"X\">Your comment here.\n",
      "Only output comments, nothing else.\n",
      "\n",
      "### Code Diff:\n",
      "<DIFF>\n",
      "@@ -1149,6 +1163,10 @@ function performSyncWorkOnRoot(root) {\n",
      "     throw fatalError;\n",
      "   }\n",
      " \n",
      "+  if (exitStatus === RootDidNotComplete) {\n",
      "</DIFF> [/INST]  <COMMENT side=\"RIGHT\" offset=\"1150\">This line is not needed. The `throw` statement will already cause the function to exit with an error. It's not necessary to check the `exitStatus` variable and throw a new error if it's not `RootCompleted`.</COMMENT>\n",
      "\n",
      "<COMMENT side=\"RIGHT\" offset=\"1164\">This line is not needed. The `if` statement is already checking if the `exitStatus` variable is equal to `RootDidNotComplete`. It's not necessary to check the `exitStatus` variable again in the `if` statement's body.</COMMENT>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Sample 3 ===\n",
      "[INST] You are a helpful code reviewer. Given a code diff, generate all relevant review comments. Each comment must be in the format: <COMMENT side=\"RIGHT\" offset=\"X\">Your comment here.\n",
      "Only output comments, nothing else.\n",
      "\n",
      "### Code Diff:\n",
      "<DIFF>\n",
      "@@ -739,14 +739,20 @@ describe('ReactCache', () => {\n",
      "     await act(async () => {\n",
      "       refresh();\n",
      "     });\n",
      "-    expect(Scheduler).toHaveYielded(['Cache miss! [A]', 'Loading...']);\n",
      "+    expect(Scheduler).toHaveYielded([\n",
      "+      'Cache miss! [A]',\n",
      "+      'Loading...',\n",
      "+      // TODO: This happens too early, because we don't retain the refreshed\n",
      "+      // cache until it commits. Will fix in next step.\n",
      "+      'Cache cleanup: A [v1]',\n",
      "</DIFF> [/INST]  <COMMENT side=\"RIGHT\" offset=\"10\">This comment is suggesting that the cache cleanup is happening too early, before the refreshed cache has committed. This could be fixed by waiting for the cache to commit before cleaning up.</COMMENT>\n",
      "\n",
      "<COMMENT side=\"RIGHT\" offset=\"15\">This comment is suggesting that the cache cleanup is happening too early, before the refreshed cache has committed. This could be fixed by waiting for the cache to commit before cleaning up.</COMMENT>\n",
      "\n",
      "<COMMENT side=\"RIGHT\" offset=\"18\">This comment is suggesting that the cache cleanup is happening too early, before the refreshed cache has committed. This could be fixed by waiting for the cache to commit before cleaning up.</COMMENT>\n",
      "\n",
      "<COMMENT side=\"RIGHT\" offset=\"21\">This comment is suggesting that the cache cleanup is happening too early, before the refreshed cache has committed. This could be fixed by waiting for the cache to commit before cleaning up.</COMMENT>\n",
      "\n",
      "<COMMENT side=\"RIGHT\" offset=\"24\">This comment is suggesting that the cache cleanup\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Sample 4 ===\n",
      "[INST] You are a helpful code reviewer. Given a code diff, generate all relevant review comments. Each comment must be in the format: <COMMENT side=\"RIGHT\" offset=\"X\">Your comment here.\n",
      "Only output comments, nothing else.\n",
      "\n",
      "### Code Diff:\n",
      "<DIFF>\n",
      "@@ -739,14 +739,20 @@ describe('ReactCache', () => {\n",
      "     await act(async () => {\n",
      "       refresh();\n",
      "     });\n",
      "-    expect(Scheduler).toHaveYielded(['Cache miss! [A]', 'Loading...']);\n",
      "+    expect(Scheduler).toHaveYielded([\n",
      "+      'Cache miss! [A]',\n",
      "+      'Loading...',\n",
      "+      // TODO: This happens too early, because we don't retain the refreshed\n",
      "+      // cache until it commits. Will fix in next step.\n",
      "+      'Cache cleanup: A [v1]',\n",
      "</DIFF> [/INST]  <COMMENT side=\"RIGHT\" offset=\"10\">This comment is suggesting that the cache cleanup is happening too early, before the refreshed cache has committed. This could be fixed by waiting for the cache to commit before cleaning up.</COMMENT>\n",
      "\n",
      "<COMMENT side=\"RIGHT\" offset=\"15\">This comment is suggesting that the cache cleanup is happening too early, before the refreshed cache has committed. This could be fixed by waiting for the cache to commit before cleaning up.</COMMENT>\n",
      "\n",
      "<COMMENT side=\"RIGHT\" offset=\"18\">This comment is suggesting that the cache cleanup is happening too early, before the refreshed cache has committed. This could be fixed by waiting for the cache to commit before cleaning up.</COMMENT>\n",
      "\n",
      "<COMMENT side=\"RIGHT\" offset=\"21\">This comment is suggesting that the cache cleanup is happening too early, before the refreshed cache has committed. This could be fixed by waiting for the cache to commit before cleaning up.</COMMENT>\n",
      "\n",
      "<COMMENT side=\"RIGHT\" offset=\"24\">This comment is suggesting that the cache cleanup\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Sample 5 ===\n",
      "[INST] You are a helpful code reviewer. Given a code diff, generate all relevant review comments. Each comment must be in the format: <COMMENT side=\"RIGHT\" offset=\"X\">Your comment here.\n",
      "Only output comments, nothing else.\n",
      "\n",
      "### Code Diff:\n",
      "<DIFF>\n",
      "@@ -739,14 +739,20 @@ describe('ReactCache', () => {\n",
      "     await act(async () => {\n",
      "       refresh();\n",
      "     });\n",
      "-    expect(Scheduler).toHaveYielded(['Cache miss! [A]', 'Loading...']);\n",
      "+    expect(Scheduler).toHaveYielded([\n",
      "+      'Cache miss! [A]',\n",
      "+      'Loading...',\n",
      "+      // TODO: This happens too early, because we don't retain the refreshed\n",
      "+      // cache until it commits. Will fix in next step.\n",
      "+      'Cache cleanup: A [v1]',\n",
      "</DIFF> [/INST]  <COMMENT side=\"RIGHT\" offset=\"10\">This comment is suggesting that the cache cleanup is happening too early, before the refreshed cache has committed. This could be fixed by waiting for the cache to commit before cleaning up.</COMMENT>\n",
      "\n",
      "<COMMENT side=\"RIGHT\" offset=\"15\">This comment is suggesting that the cache cleanup is happening too early, before the refreshed cache has committed. This could be fixed by waiting for the cache to commit before cleaning up.</COMMENT>\n",
      "\n",
      "<COMMENT side=\"RIGHT\" offset=\"18\">This comment is suggesting that the cache cleanup is happening too early, before the refreshed cache has committed. This could be fixed by waiting for the cache to commit before cleaning up.</COMMENT>\n",
      "\n",
      "<COMMENT side=\"RIGHT\" offset=\"21\">This comment is suggesting that the cache cleanup is happening too early, before the refreshed cache has committed. This could be fixed by waiting for the cache to commit before cleaning up.</COMMENT>\n",
      "\n",
      "<COMMENT side=\"RIGHT\" offset=\"24\">This comment is suggesting that the cache cleanup\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Sample 6 ===\n",
      "[INST] You are a helpful code reviewer. Given a code diff, generate all relevant review comments. Each comment must be in the format: <COMMENT side=\"RIGHT\" offset=\"X\">Your comment here.\n",
      "Only output comments, nothing else.\n",
      "\n",
      "### Code Diff:\n",
      "<DIFF>\n",
      "@@ -0,0 +1,197 @@\n",
      "+/**\n",
      "+ * Copyright (c) Facebook, Inc. and its affiliates.\n",
      "+ *\n",
      "+ * This source code is licensed under the MIT license found in the\n",
      "+ * LICENSE file in the root directory of this source tree.\n",
      "+ *\n",
      "+ * @emails react-core\n",
      "+ */\n",
      "+\n",
      "+let JSDOM;\n",
      "+let React;\n",
      "+let ReactDOM;\n",
      "+let Scheduler;\n",
      "+let clientAct;\n",
      "+let ReactDOMFizzServer;\n",
      "+let Stream;\n",
      "+let document;\n",
      "+let writable;\n",
      "+let container;\n",
      "+let buffer = '';\n",
      "+let hasErrored = false;\n",
      "+let fatalError = undefined;\n",
      "+let textCache;\n",
      "+\n",
      "+describe('useId', () => {\n",
      "+  beforeEach(() => {\n",
      "+    jest.resetModules();\n",
      "+    JSDOM = require('jsdom').JSDOM;\n",
      "+    React = require('react');\n",
      "+    ReactDOM = require('react-dom');\n",
      "+    Scheduler = require('scheduler');\n",
      "+    clientAct = require('jest-react').act;\n",
      "+    ReactDOMFizzServer = require('react-dom/server');\n",
      "+    Stream = require('stream');\n",
      "+\n",
      "+    textCache = new Map();\n",
      "+\n",
      "+    // Test Environment\n",
      "+    const jsdom = new JSDOM(\n",
      "+      '<!DOCTYPE html><html><head></head><body><div id=\"container\">',\n",
      "+      {\n",
      "+        runScripts: 'dangerously',\n",
      "</DIFF> [/INST]  <COMMENT side=\"RIGHT\" offset=\"1\">This code looks like it's setting up a test environment for a React component. It's using JSDOM to create a virtual DOM, and it's requiring several other libraries that are used in React.</COMMENT>\n",
      "\n",
      "  <COMMENT side=\"RIGHT\" offset=\"10\">The `textCache` variable is being initialized as a new Map, which is a data structure that maps keys to values. It's not clear what this variable is used for, but it's likely that it's being used to cache some data or results.</COMMENT>\n",
      "\n",
      "  <COMMENT side=\"RIGHT\" offset=\"15\">The `describe` function is being used to define a test suite. It's not clear what this test suite is testing, but it's likely that it's testing some functionality related to the `useId` hook.</COMMENT>\n",
      "\n",
      "  <COMMENT side=\"RIGHT\" offset=\"20\">The `beforeEach` function is being used to set up some variables that are used in the test suite. It's not clear what these variables are used for\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:58<00:00,  7.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Sample 7 ===\n",
      "[INST] You are a helpful code reviewer. Given a code diff, generate all relevant review comments. Each comment must be in the format: <COMMENT side=\"RIGHT\" offset=\"X\">Your comment here.\n",
      "Only output comments, nothing else.\n",
      "\n",
      "### Code Diff:\n",
      "<DIFF>\n",
      "@@ -389,17 +389,6 @@ describe('ReactSuspense', () => {\n",
      "     expect(root).toMatchRenderedOutput('Hi');\n",
      "   });\n",
      " \n",
      "-  it('throws if tree suspends and none of the Suspense ancestors have a boundary', () => {\n",
      "</DIFF> [/INST]  <COMMENT side=\"RIGHT\" offset=\"389\">This test seems to be testing the behavior of the `ReactSuspense` component when it is used in a situation where it is not expected to be used. The test is checking that an error is thrown when the component suspends and none of the Suspense ancestors have a boundary. However, this test is not actually testing the behavior of the `ReactSuspense` component in this situation. It would be more useful to test the behavior of the component in a real-world scenario where it is actually used.\n",
      "\n",
      "Additionally, the test is not actually testing the error message that is thrown when the component suspends and none of the Suspense ancestors have a boundary. It would be more useful to test that the correct error message is thrown in this situation.\n",
      "\n",
      "Overall, this test seems to be testing the behavior of the `ReactSuspense` component in a way that is not actually relevant to its intended use case. It would be more useful to test the component in a more realistic scenario.\n",
      "</COMMENT>\n",
      "\n",
      "<COMMENT side=\"RIGHT\" offset=\"390\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "references = []\n",
    "candidates = []\n",
    "offset_match_total = 0\n",
    "offset_match_correct = 0\n",
    "\n",
    "for i, sample in enumerate(tqdm(test_data[:100])):  # Full test set if needed\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        [{\"role\": \"user\", \"content\": instruction + sample[\"input\"]}],\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "    inputs = tokenizer(\n",
    "        prompt,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=4096\n",
    "    ).to(\"cuda:0\")\n",
    "\n",
    "    true_comments = extract_comments(sample[\"label\"])\n",
    "\n",
    "    # Measure inference latency\n",
    "    with torch.no_grad():\n",
    "        start = time.time()\n",
    "        output_ids = model.generate(**inputs, max_new_tokens=256, do_sample=False)\n",
    "        latencies.append(time.time() - start)\n",
    "\n",
    "    # Decode and evaluate\n",
    "    generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True).strip()\n",
    "    generated_text = generated_text.split(\"Setting `pad_token_id\")[0].strip()\n",
    "    print(f\"\\n=== Sample {i} ===\\n{generated_text}\")\n",
    "\n",
    "    pred_comments = extract_comments(generated_text)\n",
    "\n",
    "    for gt in true_comments:\n",
    "        references.append(gt[\"comment\"])\n",
    "        best_pred = max(pred_comments, key=lambda p: p[\"comment\"], default={\"comment\": \"\"})\n",
    "        candidates.append(best_pred[\"comment\"] if best_pred else \"\")\n",
    "\n",
    "    correct, total = offset_match(pred_comments, true_comments, window=3)\n",
    "    offset_match_correct += correct\n",
    "    offset_match_total += total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "49490094-41c4-4d0a-8e51-cb463e34c143",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Inference Latency Stats ===\n",
      "Median latency:        7757.82 ms\n",
      "95th percentile:       7990.19 ms\n",
      "99th percentile:       8111.77 ms\n",
      "Throughput (1 sample): 0.13 samples/sec\n"
     ]
    }
   ],
   "source": [
    "# Latency stats\n",
    "print(\"\\n=== Inference Latency Stats ===\")\n",
    "print(f\"Median latency:        {np.percentile(latencies, 50) * 1000:.2f} ms\")\n",
    "print(f\"95th percentile:       {np.percentile(latencies, 95) * 1000:.2f} ms\")\n",
    "print(f\"99th percentile:       {np.percentile(latencies, 99) * 1000:.2f} ms\")\n",
    "print(f\"Throughput (1 sample): {len(latencies) / np.sum(latencies):.2f} samples/sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b0323dc1-c576-47ab-bbc0-7c930423c226",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ded12b530986495da51b52e1c454a9e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3d77327e53a4a4faaefd6e2a83edf4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/482 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cb10eacaf814f9a8e243681bfaff08e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bba71d0866324ad9b8c8f5c5552e3b56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84288e69da2140eab7714604c4bc8e70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ed8b6878f4641488e94a62857d9a78d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.42G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fe514997ef649c2adbdf3afeaa59fe6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49dc0a4d64694fb1bef9cdaf0324dee1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 0.10 seconds, 83.01 sentences/sec\n",
      "\n",
      "Evaluation Results\n",
      "Avg BERTScore F1: 0.7252\n",
      "Offset Precision: 0/23 = 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n"
     ]
    }
   ],
   "source": [
    "# BERTScore\n",
    "P, R, F1 = bert_score(candidates, references, lang=\"en\", verbose=True)\n",
    "print(\"\\nEvaluation Results\")\n",
    "print(f\"Avg BERTScore F1: {F1.mean().item():.4f}\")\n",
    "print(f\"Offset Precision: {offset_match_correct}/{offset_match_total} = {offset_match_correct/offset_match_total:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a49350b7-5cf3-458d-84ae-367a972e5511",
   "metadata": {},
   "source": [
    "Batch throughput test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "30882c1b-72a9-4340-a50e-fc29d763e714",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "batch_size = 4\n",
    "num_batches = 10\n",
    "batch_inputs = [test_data[i][\"input\"] for i in range(batch_size)]\n",
    "batch_prompts = [\n",
    "    tokenizer.apply_chat_template(\n",
    "        [{\"role\": \"user\", \"content\": instruction + inp}],\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    for inp in batch_inputs\n",
    "]\n",
    "\n",
    "encoded = tokenizer(\n",
    "    batch_prompts,\n",
    "    return_tensors=\"pt\",\n",
    "    padding=\"max_length\",\n",
    "    truncation=True,\n",
    "    max_length=4096\n",
    ").to(\"cuda:0\")\n",
    "\n",
    "# Warm-up\n",
    "with torch.no_grad():\n",
    "    _ = model.generate(**encoded, max_new_tokens=256, do_sample=False)\n",
    "\n",
    "batch_times = []\n",
    "with torch.no_grad():\n",
    "    for _ in range(num_batches):\n",
    "        start = time.time()\n",
    "        _ = model.generate(**encoded, max_new_tokens=256, do_sample=False)\n",
    "        batch_times.append(time.time() - start)\n",
    "\n",
    "batch_fps = (batch_size * num_batches) / np.sum(batch_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "cb5cd99a-3643-4300-8087-f23fbf8940ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Final Evaluation Summary ===\n",
      "Model Size on Disk:               12.55\n",
      "BERTScore F1 (avg):               0.7252\n",
      "Location Precision:               0/23 = 0.0000\n",
      "Inference Latency (median):       7757.82 ms\n",
      "Inference Latency (95th pct):     7990.19 ms\n",
      "Inference Latency (99th pct):     8111.77 ms\n",
      "Inference Throughput (1 sample):  0.13 samples/sec\n",
      "Batch Throughput (4):  0.26 samples/sec\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== Final Evaluation Summary ===\")\n",
    "print(f\"Model Size on Disk:               {get_model_size(model):.2f}\")\n",
    "print(f\"BERTScore F1 (avg):               {F1.mean().item():.4f}\")\n",
    "print(f\"Location Precision:               {offset_match_correct}/{offset_match_total} = {offset_match_correct / offset_match_total:.4f}\")\n",
    "print(f\"Inference Latency (median):       {np.percentile(latencies, 50) * 1000:.2f} ms\")\n",
    "print(f\"Inference Latency (95th pct):     {np.percentile(latencies, 95) * 1000:.2f} ms\")\n",
    "print(f\"Inference Latency (99th pct):     {np.percentile(latencies, 99) * 1000:.2f} ms\")\n",
    "print(f\"Inference Throughput (1 sample):  {len(latencies) / np.sum(latencies):.2f} samples/sec\")\n",
    "print(f\"Batch Throughput ({batch_size}):  {batch_fps:.2f} samples/sec\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "911505f5-d4bd-4f7b-a8f5-e699dcec3743",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun May 11 12:01:55 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 560.35.05              Driver Version: 560.35.05      CUDA Version: 12.6     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA A100 80GB PCIe          Off |   00000000:27:00.0 Off |                    0 |\n",
      "| N/A   65C    P0             89W /  300W |   52771MiB /  81920MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a1157d-4706-4d00-93fc-a054dcd5f1ea",
   "metadata": {},
   "source": [
    "### Compiled model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "50d13d22-c3ca-449c-ac9c-4c6c24f441c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a408cd1b-9629-42c0-aea3-0d176bc4c52e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Sample 0 ===\n",
      "[INST] You are a helpful code reviewer. Given a code diff, generate all relevant review comments. Each comment must be in the format: <COMMENT side=\"RIGHT\" offset=\"X\">Your comment here.\n",
      "Only output comments, nothing else.\n",
      "\n",
      "### Code Diff:\n",
      "<DIFF>\n",
      "@@ -838,45 +839,58 @@ function performConcurrentWorkOnRoot(root, didTimeout) {\n",
      "       throw fatalError;\n",
      "     }\n",
      " \n",
      "-    // Check if this render may have yielded to a concurrent event, and if so,\n",
      "-    // confirm that any newly rendered stores are consistent.\n",
      "-    // TODO: It's possible that even a concurrent render may never have yielded\n",
      "-    // to the main thread, if it was fast enough, or if it expired. We could\n",
      "-    // skip the consistency check in that case, too.\n",
      "-    const renderWasConcurrent = !includesBlockingLane(root, lanes);\n",
      "-    const finishedWork: Fiber = (root.current.alternate: any);\n",
      "-    if (\n",
      "-      renderWasConcurrent &&\n",
      "-      !isRenderConsistentWithExternalStores(finishedWork)\n",
      "-    ) {\n",
      "-      // A store was mutated in an interleaved event. Render again,\n",
      "-      // synchronously, to block further mutations.\n",
      "-      exitStatus = renderRootSync(root, lanes);\n",
      "-\n",
      "-      // We need to check again if something threw\n",
      "-      if (exitStatus === RootErrored) {\n",
      "-        const errorRetryLanes = getLanesToRetrySynchronouslyOnError(root);\n",
      "-        if (errorRetryLanes !== NoLanes) {\n",
      "-          lanes = errorRetryLanes;\n",
      "-          exitStatus = recoverFromConcurrentError(root, errorRetryLanes);\n",
      "-          // We assume the tree is now consistent because we didn't yield to any\n",
      "-          // concurrent events.\n",
      "+    if (exitStatus === RootDidNotComplete) {\n",
      "</DIFF> [/INST]  <COMMENT side=\"RIGHT\" offset=\"838\">This code looks like it's checking if a render was concurrent and if it was consistent with external stores. It's also checking if the render was interrupted by an error and if so, it's retrying the render synchronously.</COMMENT>\n",
      "\n",
      "<COMMENT side=\"RIGHT\" offset=\"840\">It's possible that even a concurrent render may never have yielded to the main thread, if it was fast enough, or if it expired. We could skip the consistency check in that case, too.</COMMENT>\n",
      "\n",
      "<COMMENT side=\"RIGHT\" offset=\"842\">The code is checking if the render was concurrent and if it was consistent with external stores. If it was not consistent, it's retrying the render synchronously.</COMMENT>\n",
      "\n",
      "<COMMENT side=\"RIGHT\" offset=\"844\">It's assuming that the tree is now consistent because it didn't yield to any concurrent events.</COMMENT>\n",
      "\n",
      "<COMMENT side=\"RIGHT\" offset=\"846\">It's checking if the\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Sample 1 ===\n",
      "[INST] You are a helpful code reviewer. Given a code diff, generate all relevant review comments. Each comment must be in the format: <COMMENT side=\"RIGHT\" offset=\"X\">Your comment here.\n",
      "Only output comments, nothing else.\n",
      "\n",
      "### Code Diff:\n",
      "<DIFF>\n",
      "@@ -838,45 +839,58 @@ function performConcurrentWorkOnRoot(root, didTimeout) {\n",
      "       throw fatalError;\n",
      "     }\n",
      " \n",
      "-    // Check if this render may have yielded to a concurrent event, and if so,\n",
      "-    // confirm that any newly rendered stores are consistent.\n",
      "-    // TODO: It's possible that even a concurrent render may never have yielded\n",
      "-    // to the main thread, if it was fast enough, or if it expired. We could\n",
      "-    // skip the consistency check in that case, too.\n",
      "-    const renderWasConcurrent = !includesBlockingLane(root, lanes);\n",
      "-    const finishedWork: Fiber = (root.current.alternate: any);\n",
      "-    if (\n",
      "-      renderWasConcurrent &&\n",
      "-      !isRenderConsistentWithExternalStores(finishedWork)\n",
      "-    ) {\n",
      "-      // A store was mutated in an interleaved event. Render again,\n",
      "-      // synchronously, to block further mutations.\n",
      "-      exitStatus = renderRootSync(root, lanes);\n",
      "-\n",
      "-      // We need to check again if something threw\n",
      "-      if (exitStatus === RootErrored) {\n",
      "-        const errorRetryLanes = getLanesToRetrySynchronouslyOnError(root);\n",
      "-        if (errorRetryLanes !== NoLanes) {\n",
      "-          lanes = errorRetryLanes;\n",
      "-          exitStatus = recoverFromConcurrentError(root, errorRetryLanes);\n",
      "-          // We assume the tree is now consistent because we didn't yield to any\n",
      "-          // concurrent events.\n",
      "+    if (exitStatus === RootDidNotComplete) {\n",
      "+      // The render unwound without completing the tree. This happens in special\n",
      "+      // cases where need to exit the current render without producing a\n",
      "+      // consistent tree or committing.\n",
      "+      //\n",
      "+      // This should only happen during a concurrent render, not a discrete or\n",
      "+      // synchronous update. We should have already checked for this when we\n",
      "+      // unwound the stack.\n",
      "</DIFF> [/INST]  <COMMENT side=\"RIGHT\" offset=\"838\">This code looks like it's checking if a render was concurrent and if it was consistent with external stores. However, it's not clear why this check is necessary or what the implications of this check are.</COMMENT>\n",
      "\n",
      "<COMMENT side=\"RIGHT\" offset=\"845\">This code is checking if a render was concurrent and if it was consistent with external stores. If the render was concurrent and the stores were not consistent, it will re-render the tree synchronously to block further mutations.</COMMENT>\n",
      "\n",
      "<COMMENT side=\"RIGHT\" offset=\"853\">This code is checking if a render was concurrent and if it was consistent with external stores. If the render was concurrent and the stores were not consistent, it will re-render the tree synchronously to block further mutations. However, it's not clear why this check is necessary or what the implications of this check are.</COMMENT>\n",
      "\n",
      "<COMMENT side=\"RIGHT\" offset=\"861\">This code is checking if a render was concurrent and if it was consistent with external stores. If the render\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Sample 2 ===\n",
      "[INST] You are a helpful code reviewer. Given a code diff, generate all relevant review comments. Each comment must be in the format: <COMMENT side=\"RIGHT\" offset=\"X\">Your comment here.\n",
      "Only output comments, nothing else.\n",
      "\n",
      "### Code Diff:\n",
      "<DIFF>\n",
      "@@ -1149,6 +1163,10 @@ function performSyncWorkOnRoot(root) {\n",
      "     throw fatalError;\n",
      "   }\n",
      " \n",
      "+  if (exitStatus === RootDidNotComplete) {\n",
      "</DIFF> [/INST]  <COMMENT side=\"RIGHT\" offset=\"1150\">This line is not needed. The `throw` statement will already cause the function to exit with an error. It's not necessary to check the `exitStatus` variable and throw a new error if it's not `RootCompleted`.</COMMENT>\n",
      "\n",
      "<COMMENT side=\"RIGHT\" offset=\"1164\">This line is not needed. The `if` statement is already checking if the `exitStatus` variable is equal to `RootDidNotComplete`. It's not necessary to check the `exitStatus` variable again in the `if` statement's body.</COMMENT>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Sample 3 ===\n",
      "[INST] You are a helpful code reviewer. Given a code diff, generate all relevant review comments. Each comment must be in the format: <COMMENT side=\"RIGHT\" offset=\"X\">Your comment here.\n",
      "Only output comments, nothing else.\n",
      "\n",
      "### Code Diff:\n",
      "<DIFF>\n",
      "@@ -739,14 +739,20 @@ describe('ReactCache', () => {\n",
      "     await act(async () => {\n",
      "       refresh();\n",
      "     });\n",
      "-    expect(Scheduler).toHaveYielded(['Cache miss! [A]', 'Loading...']);\n",
      "+    expect(Scheduler).toHaveYielded([\n",
      "+      'Cache miss! [A]',\n",
      "+      'Loading...',\n",
      "+      // TODO: This happens too early, because we don't retain the refreshed\n",
      "+      // cache until it commits. Will fix in next step.\n",
      "+      'Cache cleanup: A [v1]',\n",
      "</DIFF> [/INST]  <COMMENT side=\"RIGHT\" offset=\"10\">This comment is suggesting that the cache cleanup is happening too early, before the refreshed cache has committed. This could be fixed by waiting for the cache to commit before cleaning up.</COMMENT>\n",
      "\n",
      "<COMMENT side=\"RIGHT\" offset=\"15\">This comment is suggesting that the cache cleanup is happening too early, before the refreshed cache has committed. This could be fixed by waiting for the cache to commit before cleaning up.</COMMENT>\n",
      "\n",
      "<COMMENT side=\"RIGHT\" offset=\"18\">This comment is suggesting that the cache cleanup is happening too early, before the refreshed cache has committed. This could be fixed by waiting for the cache to commit before cleaning up.</COMMENT>\n",
      "\n",
      "<COMMENT side=\"RIGHT\" offset=\"21\">This comment is suggesting that the cache cleanup is happening too early, before the refreshed cache has committed. This could be fixed by waiting for the cache to commit before cleaning up.</COMMENT>\n",
      "\n",
      "<COMMENT side=\"RIGHT\" offset=\"24\">This comment is suggesting that the cache cleanup\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:35<00:00,  7.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Sample 4 ===\n",
      "[INST] You are a helpful code reviewer. Given a code diff, generate all relevant review comments. Each comment must be in the format: <COMMENT side=\"RIGHT\" offset=\"X\">Your comment here.\n",
      "Only output comments, nothing else.\n",
      "\n",
      "### Code Diff:\n",
      "<DIFF>\n",
      "@@ -739,14 +739,20 @@ describe('ReactCache', () => {\n",
      "     await act(async () => {\n",
      "       refresh();\n",
      "     });\n",
      "-    expect(Scheduler).toHaveYielded(['Cache miss! [A]', 'Loading...']);\n",
      "+    expect(Scheduler).toHaveYielded([\n",
      "+      'Cache miss! [A]',\n",
      "+      'Loading...',\n",
      "+      // TODO: This happens too early, because we don't retain the refreshed\n",
      "+      // cache until it commits. Will fix in next step.\n",
      "+      'Cache cleanup: A [v1]',\n",
      "</DIFF> [/INST]  <COMMENT side=\"RIGHT\" offset=\"10\">This comment is suggesting that the cache cleanup is happening too early, before the refreshed cache has committed. This could be fixed by waiting for the cache to commit before cleaning up.</COMMENT>\n",
      "\n",
      "<COMMENT side=\"RIGHT\" offset=\"15\">This comment is suggesting that the cache cleanup is happening too early, before the refreshed cache has committed. This could be fixed by waiting for the cache to commit before cleaning up.</COMMENT>\n",
      "\n",
      "<COMMENT side=\"RIGHT\" offset=\"18\">This comment is suggesting that the cache cleanup is happening too early, before the refreshed cache has committed. This could be fixed by waiting for the cache to commit before cleaning up.</COMMENT>\n",
      "\n",
      "<COMMENT side=\"RIGHT\" offset=\"21\">This comment is suggesting that the cache cleanup is happening too early, before the refreshed cache has committed. This could be fixed by waiting for the cache to commit before cleaning up.</COMMENT>\n",
      "\n",
      "<COMMENT side=\"RIGHT\" offset=\"24\">This comment is suggesting that the cache cleanup\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "references = []\n",
    "candidates = []\n",
    "offset_match_total = 0\n",
    "offset_match_correct = 0\n",
    "\n",
    "for i, sample in enumerate(tqdm(test_data[:5])):  # Full test set if needed\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        [{\"role\": \"user\", \"content\": instruction + sample[\"input\"]}],\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "    inputs = tokenizer(\n",
    "        prompt,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=4096\n",
    "    ).to(\"cuda:0\")\n",
    "\n",
    "    true_comments = extract_comments(sample[\"label\"])\n",
    "\n",
    "    # Measure inference latency\n",
    "    with torch.no_grad():\n",
    "        start = time.time()\n",
    "        output_ids = model.generate(**inputs, max_new_tokens=256, do_sample=False)\n",
    "        latencies.append(time.time() - start)\n",
    "\n",
    "    # Decode and evaluate\n",
    "    generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True).strip()\n",
    "    generated_text = generated_text.split(\"Setting `pad_token_id\")[0].strip()\n",
    "    print(f\"\\n=== Sample {i} ===\\n{generated_text}\")\n",
    "\n",
    "    pred_comments = extract_comments(generated_text)\n",
    "\n",
    "    for gt in true_comments:\n",
    "        references.append(gt[\"comment\"])\n",
    "        best_pred = max(pred_comments, key=lambda p: p[\"comment\"], default={\"comment\": \"\"})\n",
    "        candidates.append(best_pred[\"comment\"] if best_pred else \"\")\n",
    "\n",
    "    correct, total = offset_match(pred_comments, true_comments, window=3)\n",
    "    offset_match_correct += correct\n",
    "    offset_match_total += total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0ef32e3d-1306-4f65-9237-96ad4a7f3dcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "batch_size = 4\n",
    "num_batches = 10\n",
    "batch_inputs = [test_data[i][\"input\"] for i in range(batch_size)]\n",
    "batch_prompts = [\n",
    "    tokenizer.apply_chat_template(\n",
    "        [{\"role\": \"user\", \"content\": instruction + inp}],\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    for inp in batch_inputs\n",
    "]\n",
    "\n",
    "encoded = tokenizer(\n",
    "    batch_prompts,\n",
    "    return_tensors=\"pt\",\n",
    "    padding=\"max_length\",\n",
    "    truncation=True,\n",
    "    max_length=4096\n",
    ").to(\"cuda:0\")\n",
    "\n",
    "# Warm-up\n",
    "with torch.no_grad():\n",
    "    _ = model.generate(**encoded, max_new_tokens=256, do_sample=False)\n",
    "\n",
    "batch_times = []\n",
    "with torch.no_grad():\n",
    "    for _ in range(num_batches):\n",
    "        start = time.time()\n",
    "        _ = model.generate(**encoded, max_new_tokens=256, do_sample=False)\n",
    "        batch_times.append(time.time() - start)\n",
    "\n",
    "batch_fps = (batch_size * num_batches) / np.sum(batch_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "befdf788-8e05-446f-9ecd-1e1fb07bb09b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Final Evaluation Summary ===\n",
      "Model Size on Disk:               12.55\n",
      "BERTScore F1 (avg):               0.7252\n",
      "Location Precision:               0/16 = 0.0000\n",
      "Inference Latency (median):       7761.92 ms\n",
      "Inference Latency (95th pct):     8123.67 ms\n",
      "Inference Latency (99th pct):     159196.10 ms\n",
      "Inference Throughput (1 sample):  0.06 samples/sec\n",
      "Batch Throughput (4):  0.26 samples/sec\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== Final Evaluation Summary ===\")\n",
    "print(f\"Model Size on Disk:               {get_model_size(model):.2f}\")\n",
    "print(f\"BERTScore F1 (avg):               {F1.mean().item():.4f}\")\n",
    "print(f\"Location Precision:               {offset_match_correct}/{offset_match_total} = {offset_match_correct / offset_match_total:.4f}\")\n",
    "print(f\"Inference Latency (median):       {np.percentile(latencies, 50) * 1000:.2f} ms\")\n",
    "print(f\"Inference Latency (95th pct):     {np.percentile(latencies, 95) * 1000:.2f} ms\")\n",
    "print(f\"Inference Latency (99th pct):     {np.percentile(latencies, 99) * 1000:.2f} ms\")\n",
    "print(f\"Inference Throughput (1 sample):  {len(latencies) / np.sum(latencies):.2f} samples/sec\")\n",
    "print(f\"Batch Throughput ({batch_size}):  {batch_fps:.2f} samples/sec\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6280cc2-c9ba-4531-bf1a-8eceb0a8a5cc",
   "metadata": {},
   "source": [
    "### Converting to ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4b828ecf-f7d0-49c8-9b54-23b0330ccaa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "code_prompt = \"\"\"\n",
    "def fibonacci(n):\n",
    "    if n <= 0:\n",
    "        return []\n",
    "    elif n == 1:\n",
    "        return [0]\n",
    "    elif n == 2:\n",
    "        return [0, 1]\n",
    "    else:\n",
    "        seq = [0, 1]\n",
    "        for i in range(2, n):\n",
    "            seq.append(seq[-1] + seq[-2])\n",
    "        return seq\n",
    "\"\"\"\n",
    "\n",
    "prompt = f\"Given the code diff below, generate relevant code review comments in the format:\\n<COMMENT side=\\\"RIGHT\\\" offset=\\\"X\\\">...\\n\\n{code_prompt}\"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=4096).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "8eebd2ae-281f-40d4-9132-389fb4e39507",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ORTModelForCausalLM' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[60], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m model_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcodellama/CodeLLaMA-7b-Instruct-hf\u001b[39m\u001b[38;5;124m\"\u001b[39m  \n\u001b[0;32m----> 3\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mORTModelForCausalLM\u001b[49m\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_id, export\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, save_dir\u001b[38;5;241m=\u001b[39monnx_dir)\n\u001b[1;32m      4\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_id)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mpad_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ORTModelForCausalLM' is not defined"
     ]
    }
   ],
   "source": [
    "model_id = \"codellama/CodeLLaMA-7b-Instruct-hf\"  \n",
    "\n",
    "model = ORTModelForCausalLM.from_pretrained(model_id, export=True, save_dir=onnx_dir)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(f\"ONNX model saved to: {onnx_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b5147f-0cb4-473f-b8a8-90f00259a1ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f1438a8-d30e-42b6-ba35-c60ed7a25bbe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f233aab9-549b-4f55-8e32-796434b855b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f76345cf-8926-4636-a1ca-f31166ffc93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_path = \"models/llama_onnx/model.onnx\"\n",
    "os.makedirs(os.path.dirname(onnx_path), exist_ok=True)\n",
    "\n",
    "# Export the model\n",
    "with torch.no_grad():\n",
    "    torch.onnx.export(\n",
    "        model,\n",
    "        args=(inputs[\"input_ids\"],),\n",
    "        f=onnx_path,\n",
    "        input_names=[\"input_ids\"],\n",
    "        output_names=[\"logits\"],\n",
    "        dynamic_axes={\"input_ids\": {0: \"batch_size\", 1: \"seq_len\"}, \"logits\": {0: \"batch_size\"}},\n",
    "        opset_version=17,\n",
    "        do_constant_folding=True\n",
    "    )\n",
    "\n",
    "print(f\"ONNX model saved to: {onnx_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa7d283d-dfc9-483c-9fbc-3b4366f2b6c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_session(ort_session, input_array, batch_input):\n",
    "    print(f\"\\nExecution provider: {ort_session.get_providers()}\")\n",
    "\n",
    "    # Latency\n",
    "    num_trials = 100\n",
    "    ort_session.run(None, {\"input_ids\": input_array})  # warm-up\n",
    "    latencies = []\n",
    "    for _ in range(num_trials):\n",
    "        start = time.time()\n",
    "        ort_session.run(None, {\"input_ids\": input_array})\n",
    "        latencies.append(time.time() - start)\n",
    "    print(f\"Inference Latency (median): {np.percentile(latencies, 50) * 1000:.2f} ms\")\n",
    "    print(f\"Inference Latency (95th pct): {np.percentile(latencies, 95) * 1000:.2f} ms\")\n",
    "    print(f\"Inference Latency (99th pct): {np.percentile(latencies, 99) * 1000:.2f} ms\")\n",
    "    print(f\"Inference Throughput (1 sample): {num_trials / np.sum(latencies):.2f} samples/sec\")\n",
    "\n",
    "    # Batch throughput\n",
    "    num_batches = 50\n",
    "    ort_session.run(None, {\"input_ids\": batch_input})  # warm-up\n",
    "    batch_times = []\n",
    "    for _ in range(num_batches):\n",
    "        start = time.time()\n",
    "        ort_session.run(None, {\"input_ids\": batch_input})\n",
    "        batch_times.append(time.time() - start)\n",
    "    batch_fps = (batch_input.shape[0] * num_batches) / np.sum(batch_times)\n",
    "    print(f\"Batch Throughput ({batch_input.shape[0]}): {batch_fps:.2f} samples/sec\")\n",
    "\n",
    "    # Disk size\n",
    "    model_size = os.path.getsize(ort_session._model_path)\n",
    "    print(f\"Model Size on Disk: {model_size / 1e6:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4496440a-b983-4149-818d-800558015034",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
