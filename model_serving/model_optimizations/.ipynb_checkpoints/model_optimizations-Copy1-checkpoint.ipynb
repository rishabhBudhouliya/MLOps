{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "755593ec-5202-45ec-a1e9-4bf48536be4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gzip, json, time\n",
    "import torch \n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from datasets import Dataset, load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForCausalLM, \n",
    "    TrainingArguments, \n",
    "    Trainer, \n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "from bert_score import score\n",
    "import onnxruntime as ort\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0874e8ae-b6ed-4294-b7bb-a48615cf3ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "# !pip install accelerate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "91a5e99b-69b3-41ca-8372-669999796b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"/mnt/object/my_model.pth\"\n",
    "data_path = \"/mnt/object_group/data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9752de0a-5d13-4dcd-9004-d5a709a69ac9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Size on Disk: 31415.38 MB\n"
     ]
    }
   ],
   "source": [
    "model_size = os.path.getsize(model_path) \n",
    "print(f\"Model Size on Disk: {model_size/ (1e6) :.2f} MB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "13518000-fe32-4aad-b7b9-19173c43f17b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "/mnt/object_group/data\n",
      "\n",
      "/mnt/object_group/data/metadata\n",
      "  └── processed_prs.log\n",
      "     --- File preview ---\n",
      "     https://github.com/angular/angular/pull/10609\n",
      "     https://github.com/angular/angular/pull/10616\n",
      "     ---------------------\n",
      "\n",
      "/mnt/object_group/data/processed\n",
      "  └── dataset_card.md\n",
      "     --- File preview ---\n",
      "     # Dataset Card for v1\n",
      "     \n",
      "     ---------------------\n",
      "  └── split_map.yml\n",
      "     --- File preview ---\n",
      "     test:\n",
      "     - facebook_react\n",
      "     ---------------------\n",
      "  └── test.jsonl.gz\n",
      "     --- File preview ---\n",
      "     [Binary or non-text file — skipped]\n",
      "  └── train.jsonl.gz\n",
      "     --- File preview ---\n",
      "     [Binary or non-text file — skipped]\n",
      "  └── val.jsonl.gz\n",
      "     --- File preview ---\n",
      "     [Binary or non-text file — skipped]\n",
      "\n",
      "/mnt/object_group/data/raw\n",
      "\n",
      "/mnt/object_group/data/raw/angular\n",
      "\n",
      "/mnt/object_group/data/raw/angular/angular\n",
      "  └── angular_angular_10609.diff\n",
      "     --- File preview ---\n",
      "     diff --git a/modules/@angular/compiler-cli/package.json b/modules/@angular/compiler-cli/package.json\n",
      "     index 11760c5c75f69..6b06d2ce052af 100644\n",
      "     ---------------------\n",
      "  └── angular_angular_10609_comments.jsonl\n",
      "     --- File preview ---\n",
      "     ---------------------\n",
      "  └── angular_angular_10616.diff\n",
      "     --- File preview ---\n",
      "     diff --git a/modules/@angular/common/src/common_directives.ts b/modules/@angular/common/src/common_directives.ts\n",
      "     index 194e4393311f9..3d35ff51d05e4 100644\n",
      "     ---------------------\n",
      "  └── angular_angular_10616_comments.jsonl\n",
      "     --- File preview ---\n",
      "     ---------------------\n",
      "  └── angular_angular_10620.diff\n",
      "     --- File preview ---\n",
      "     diff --git a/modules/@angular/common/src/directives/ng_for.ts b/modules/@angular/common/src/directives/ng_for.ts\n",
      "     index baf82e3e64c0e..0f4768c879e9d 100644\n",
      "     ---------------------\n",
      "  ... (more files hidden)\n",
      "\n",
      "/mnt/object_group/data/raw/apache\n",
      "\n",
      "/mnt/object_group/data/raw/apache/arrow\n",
      "  └── apache_arrow_41716.diff\n",
      "     --- File preview ---\n",
      "     diff --git a/ci/scripts/cpp_build.sh b/ci/scripts/cpp_build.sh\n",
      "     index a1f40fc360e2f..6a3a53f2533cd 100755\n",
      "     ---------------------\n",
      "  └── apache_arrow_41716_comments.jsonl\n",
      "     --- File preview ---\n",
      "     ---------------------\n",
      "  └── apache_arrow_41718.diff\n",
      "     --- File preview ---\n",
      "     diff --git a/java/vector/src/main/java/org/apache/arrow/vector/ipc/message/MessageSerializer.java b/java/vector/src/main/java/org/apache/arrow/vector/ipc/message/MessageSerializer.java\n",
      "     index 9deb42c498cbb..099103cd178f8 100644\n",
      "     ---------------------\n",
      "  └── apache_arrow_41718_comments.jsonl\n",
      "     --- File preview ---\n",
      "     ---------------------\n",
      "  └── apache_arrow_41721.diff\n",
      "     --- File preview ---\n",
      "     diff --git a/.pre-commit-config.yaml b/.pre-commit-config.yaml\n",
      "     index 1e4b91e27ee8a..863fd918e5911 100644\n",
      "     ---------------------\n",
      "  ... (more files hidden)\n",
      "\n",
      "/mnt/object_group/data/raw/apache/kafka\n",
      "  └── apache_kafka_17527.diff\n",
      "     --- File preview ---\n",
      "     diff --git a/clients/src/main/java/org/apache/kafka/clients/NetworkClientUtils.java b/clients/src/main/java/org/apache/kafka/clients/NetworkClientUtils.java\n",
      "     index e044fd48ee3e1..3e161b1e9939b 100644\n",
      "     ---------------------\n",
      "  └── apache_kafka_17527_comments.jsonl\n",
      "     --- File preview ---\n",
      "     {\"id\": 1805583386, \"user_login\": \"coltmcnealy-lh\", \"body\": \"I noticed when developing my test that repeated calls to `poll()` with `timeoutMs == 10` never resulted in `time.milliseconds()` advancing, so the fictitious `Node` in my test never became ready (time never advanced past the throttled time). That's why I made this change; however, it broke all sorts of things.\", \"path\": \"clients/src/test/java/org/apache/kafka/clients/MockClient.java\", \"position\": 28, \"original_position\": 16, \"commit_id\": \"344a5198f249dee5a1b954f0fe9e5087b4630851\", \"original_commit_id\": \"ec9dd116dd0944fefe4ca4d9c484e7fc60a42730\", \"diff_hunk\": \"@@ -336,6 +336,12 @@ public List<ClientResponse> poll(long timeoutMs, long now) {\\n             copy.add(response);\\n         }\\n \\n+        if (copy.isEmpty()) {\\n+            // Simulate time advancing. If no responses are received, then we know that\\n+            // we waited for the whole timeoutMs.\\n+            time.sleep(timeoutMs);\", \"side\": \"RIGHT\", \"created_at\": \"2024-10-17T23:23:31+00:00\", \"updated_at\": \"2024-10-17T23:23:32+00:00\", \"html_url\": \"https://github.com/apache/kafka/pull/17527#discussion_r1805583386\"}\n",
      "     {\"id\": 1805673698, \"user_login\": \"mjsax\", \"body\": \"I guess most test don't expect that time advanced \\\"automatically\\\"... It think we need to remove this.\\r\\n\\r\\nI think there two possibilities: start a background thread in your test before you call `runOnce()` and let the background thread advance mockTime (maybe too complex?), or, make the `MockTime` object more advanced and add some \\\"auto-time-advance\\\" feature, ie, each time `milliseconds()` is called, advance time by 1ms (or some other value passed into MockTime -- only your test would enable this feature.\", \"path\": \"clients/src/test/java/org/apache/kafka/clients/MockClient.java\", \"position\": 28, \"original_position\": 16, \"commit_id\": \"344a5198f249dee5a1b954f0fe9e5087b4630851\", \"original_commit_id\": \"ec9dd116dd0944fefe4ca4d9c484e7fc60a42730\", \"diff_hunk\": \"@@ -336,6 +336,12 @@ public List<ClientResponse> poll(long timeoutMs, long now) {\\n             copy.add(response);\\n         }\\n \\n+        if (copy.isEmpty()) {\\n+            // Simulate time advancing. If no responses are received, then we know that\\n+            // we waited for the whole timeoutMs.\\n+            time.sleep(timeoutMs);\", \"side\": \"RIGHT\", \"created_at\": \"2024-10-18T00:55:50+00:00\", \"updated_at\": \"2024-10-18T00:55:50+00:00\", \"html_url\": \"https://github.com/apache/kafka/pull/17527#discussion_r1805673698\"}\n",
      "     ---------------------\n",
      "  └── apache_kafka_17528.diff\n",
      "     --- File preview ---\n",
      "     diff --git a/clients/src/main/java/org/apache/kafka/common/record/FileRecords.java b/clients/src/main/java/org/apache/kafka/common/record/FileRecords.java\n",
      "     index 84eb6f20ac5f7..64dd73de41212 100644\n",
      "     ---------------------\n",
      "  └── apache_kafka_17528_comments.jsonl\n",
      "     --- File preview ---\n",
      "     {\"id\": 1806619794, \"user_login\": \"jsancio\", \"body\": \"@kevin-wu24 I thought we agreed in our offline discussion to instead fix LogSegment and UnifedLog to return the batch start offset since that is the actual offset that is pointed by the start byte position.\", \"path\": \"raft/src/test/java/org/apache/kafka/raft/MockLog.java\", \"position\": null, \"original_position\": 7, \"commit_id\": \"280b35145c737dd27b730fc45e00a4d9603d0a00\", \"original_commit_id\": \"95544d544b702a00b67109309d98277de0235be8\", \"diff_hunk\": \"@@ -425,7 +425,9 @@ public LogFetchInfo read(long startOffset, Isolation isolation) {\\n                 buffer = batch.writeTo(buffer);\\n \\n                 if (batchStartOffset == null) {\\n-                    batchStartOffset = batch.entries.get(0).logOffsetMetadata();\\n+                    // Return the queried startOffset to match KafkaMetadataLog behavior when\\n+                    // the startOffset is in the middle of a batch\\n+                    batchStartOffset = new LogOffsetMetadata(startOffset, batch.entries.get(0).logOffsetMetadata().metadata());\", \"side\": \"RIGHT\", \"created_at\": \"2024-10-18T14:40:15+00:00\", \"updated_at\": \"2024-10-18T14:40:15+00:00\", \"html_url\": \"https://github.com/apache/kafka/pull/17528#discussion_r1806619794\"}\n",
      "     {\"id\": 1806714453, \"user_login\": \"kevin-wu24\", \"body\": \"Oh. My mistake then; I misremembered. I'm good with fixing LogSegment, since it makes more sense than startOffset not necessarily equaling the offset of the start byte position returned. \\r\\n\\r\\nI tried this approach when working on the other PR and I ran into some test failures (among a lot of other failures) regarding the highWatermark since we use `LogOffsetMetadata` to update it. Is it possible for the highWatermark be in the middle of a batch? I have to do some more investigation to see what exactly is going on.\\r\\n\\r\\nI'll update this PR description and the Jira ticket.\", \"path\": \"raft/src/test/java/org/apache/kafka/raft/MockLog.java\", \"position\": null, \"original_position\": 7, \"commit_id\": \"280b35145c737dd27b730fc45e00a4d9603d0a00\", \"original_commit_id\": \"95544d544b702a00b67109309d98277de0235be8\", \"diff_hunk\": \"@@ -425,7 +425,9 @@ public LogFetchInfo read(long startOffset, Isolation isolation) {\\n                 buffer = batch.writeTo(buffer);\\n \\n                 if (batchStartOffset == null) {\\n-                    batchStartOffset = batch.entries.get(0).logOffsetMetadata();\\n+                    // Return the queried startOffset to match KafkaMetadataLog behavior when\\n+                    // the startOffset is in the middle of a batch\\n+                    batchStartOffset = new LogOffsetMetadata(startOffset, batch.entries.get(0).logOffsetMetadata().metadata());\", \"side\": \"RIGHT\", \"created_at\": \"2024-10-18T15:51:59+00:00\", \"updated_at\": \"2024-10-18T15:52:00+00:00\", \"html_url\": \"https://github.com/apache/kafka/pull/17528#discussion_r1806714453\"}\n",
      "     ---------------------\n",
      "  └── apache_kafka_17534.diff\n",
      "     --- File preview ---\n",
      "     diff --git a/core/src/main/java/kafka/server/share/SharePartitionManager.java b/core/src/main/java/kafka/server/share/SharePartitionManager.java\n",
      "     index 33bd5c376066d..84ccc28565c32 100644\n",
      "     ---------------------\n",
      "  ... (more files hidden)\n",
      "\n",
      "/mnt/object_group/data/raw/electron\n",
      "\n",
      "/mnt/object_group/data/raw/electron/electron\n",
      "  └── electron_electron_39749.diff\n",
      "     --- File preview ---\n",
      "     diff --git a/spec/api-content-tracing-spec.ts b/spec/api-content-tracing-spec.ts\n",
      "     index 2d43de3e90192..62081ff5d67f3 100644\n",
      "     ---------------------\n",
      "  └── electron_electron_39749_comments.jsonl\n",
      "     --- File preview ---\n",
      "     ---------------------\n",
      "  └── electron_electron_39759.diff\n",
      "     --- File preview ---\n",
      "     diff --git a/shell/browser/native_window_mac.mm b/shell/browser/native_window_mac.mm\n",
      "     index d3d0152497d3b..e339366b05a2d 100644\n",
      "     ---------------------\n",
      "  └── electron_electron_39759_comments.jsonl\n",
      "     --- File preview ---\n",
      "     ---------------------\n",
      "  └── electron_electron_39762.diff\n",
      "     --- File preview ---\n",
      "     diff --git a/shell/common/node_bindings.cc b/shell/common/node_bindings.cc\n",
      "     index 910237fdd656f..87c32167d3577 100644\n",
      "     ---------------------\n",
      "  ... (more files hidden)\n",
      "\n",
      "/mnt/object_group/data/raw/etcd-io\n",
      "\n",
      "/mnt/object_group/data/raw/etcd-io/etcd\n",
      "  └── etcd-io_etcd_16698.diff\n",
      "     --- File preview ---\n",
      "     diff --git a/server/etcdserver/server.go b/server/etcdserver/server.go\n",
      "     index 1d48fa6732c..4b40e32bada 100644\n",
      "     ---------------------\n",
      "  └── etcd-io_etcd_16698_comments.jsonl\n",
      "     --- File preview ---\n",
      "     {\"id\": 1348374074, \"user_login\": \"serathius\", \"body\": \"Did you mean 3 seconds? I'm pretty sure gofail accepts duration which would be more readable.\\r\\n```suggestion\\r\\n\\trequire.NoError(t, clus.Procs[0].Failpoints().Setup(ctx, \\\"beforeApplyEntryNormal\\\", `sleep(3s)`))\\r\\n```\", \"path\": \"tests/e2e/http_health_check_test.go\", \"position\": null, \"original_position\": 187, \"commit_id\": \"1324f032542b61db1d052a541d4d78abe7b0645b\", \"original_commit_id\": \"8c55ebf6e247331dc9e43a1880989a35d58b6870\", \"diff_hunk\": \"@@ -0,0 +1,211 @@\\n+// Copyright 2023 The etcd Authors\\n+//\\n+// Licensed under the Apache License, Version 2.0 (the \\\"License\\\");\\n+// you may not use this file except in compliance with the License.\\n+// You may obtain a copy of the License at\\n+//\\n+//     http://www.apache.org/licenses/LICENSE-2.0\\n+//\\n+// Unless required by applicable law or agreed to in writing, software\\n+// distributed under the License is distributed on an \\\"AS IS\\\" BASIS,\\n+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\n+// See the License for the specific language governing permissions and\\n+// limitations under the License.\\n+\\n+//go:build !cluster_proxy\\n+\\n+package e2e\\n+\\n+import (\\n+\\t\\\"context\\\"\\n+\\t\\\"io\\\"\\n+\\t\\\"net/http\\\"\\n+\\t\\\"os\\\"\\n+\\t\\\"strings\\\"\\n+\\t\\\"testing\\\"\\n+\\t\\\"time\\\"\\n+\\n+\\t\\\"github.com/stretchr/testify/require\\\"\\n+\\n+\\t\\\"go.etcd.io/etcd/tests/v3/framework/config\\\"\\n+\\t\\\"go.etcd.io/etcd/tests/v3/framework/e2e\\\"\\n+\\t\\\"go.etcd.io/etcd/tests/v3/framework/testutils\\\"\\n+)\\n+\\n+type healthCheckConfig struct {\\n+\\turl                string\\n+\\texpectStatusCode   int\\n+\\texpectTimeoutError bool\\n+}\\n+\\n+func TestHTTPHealthHandler(t *testing.T) {\\n+\\te2e.BeforeTest(t)\\n+\\tclient := &http.Client{}\\n+\\ttcs := []struct {\\n+\\t\\tname           string\\n+\\t\\tinjectFailure  func(ctx context.Context, t *testing.T, clus *e2e.EtcdProcessCluster)\\n+\\t\\tclusterOptions []e2e.EPClusterOption\\n+\\t\\tauthEnabled    bool\\n+\\t\\thealthChecks   []healthCheckConfig\\n+\\t}{\\n+\\t\\t{\\n+\\t\\t\\tname:           \\\"no failures\\\", // happy case\\n+\\t\\t\\tinjectFailure:  noop,\\n+\\t\\t\\tclusterOptions: []e2e.EPClusterOption{e2e.WithClusterSize(1)},\\n+\\t\\t\\thealthChecks: []healthCheckConfig{\\n+\\t\\t\\t\\t{\\n+\\t\\t\\t\\t\\turl:              \\\"/health\\\",\\n+\\t\\t\\t\\t\\texpectStatusCode: http.StatusOK,\\n+\\t\\t\\t\\t},\\n+\\t\\t\\t},\\n+\\t\\t},\\n+\\t\\t{\\n+\\t\\t\\tname:           \\\"activated no space alarm\\\",\\n+\\t\\t\\tinjectFailure:  triggerNoSpaceAlarm,\\n+\\t\\t\\tclusterOptions: []e2e.EPClusterOption{e2e.WithClusterSize(1), e2e.WithQuotaBackendBytes(int64(13 * os.Getpagesize()))},\\n+\\t\\t\\thealthChecks: []healthCheckConfig{\\n+\\t\\t\\t\\t{\\n+\\t\\t\\t\\t\\turl:              \\\"/health\\\",\\n+\\t\\t\\t\\t\\texpectStatusCode: http.StatusServiceUnavailable,\\n+\\t\\t\\t\\t},\\n+\\t\\t\\t\\t{\\n+\\t\\t\\t\\t\\turl:              \\\"/health?exclude=NOSPACE\\\",\\n+\\t\\t\\t\\t\\texpectStatusCode: http.StatusOK,\\n+\\t\\t\\t\\t},\\n+\\t\\t\\t},\\n+\\t\\t},\\n+\\t\\t{\\n+\\t\\t\\tname:           \\\"overloaded server slow apply\\\",\\n+\\t\\t\\tinjectFailure:  triggerSlowApply,\\n+\\t\\t\\tclusterOptions: []e2e.EPClusterOption{e2e.WithClusterSize(3), e2e.WithGoFailEnabled(true)},\\n+\\t\\t\\thealthChecks: []healthCheckConfig{\\n+\\t\\t\\t\\t{\\n+\\t\\t\\t\\t\\turl:              \\\"/health?serializable=true\\\",\\n+\\t\\t\\t\\t\\texpectStatusCode: http.StatusOK,\\n+\\t\\t\\t\\t},\\n+\\t\\t\\t\\t{\\n+\\t\\t\\t\\t\\turl:                \\\"/health?serializable=false\\\",\\n+\\t\\t\\t\\t\\texpectTimeoutError: true,\\n+\\t\\t\\t\\t},\\n+\\t\\t\\t},\\n+\\t\\t},\\n+\\t\\t{\\n+\\t\\t\\tname:           \\\"network partitioned\\\",\\n+\\t\\t\\tinjectFailure:  blackhole,\\n+\\t\\t\\tclusterOptions: []e2e.EPClusterOption{e2e.WithClusterSize(3), e2e.WithIsPeerTLS(true), e2e.WithPeerProxy(true)},\\n+\\t\\t\\thealthChecks: []healthCheckConfig{\\n+\\t\\t\\t\\t{\\n+\\t\\t\\t\\t\\turl:              \\\"/health?serializable=true\\\",\\n+\\t\\t\\t\\t\\texpectStatusCode: http.StatusOK,\\n+\\t\\t\\t\\t},\\n+\\t\\t\\t\\t{\\n+\\t\\t\\t\\t\\turl:                \\\"/health?serializable=false\\\",\\n+\\t\\t\\t\\t\\texpectTimeoutError: true,\\n+\\t\\t\\t\\t},\\n+\\t\\t\\t},\\n+\\t\\t},\\n+\\t\\t{\\n+\\t\\t\\tname:           \\\"raft loop deadlock auth is enabled\\\",\\n+\\t\\t\\tinjectFailure:  triggerRaftLoopDeadLock,\\n+\\t\\t\\tclusterOptions: []e2e.EPClusterOption{e2e.WithClusterSize(3), e2e.WithGoFailEnabled(true)},\\n+\\t\\t\\tauthEnabled:    true,\\n+\\t\\t\\thealthChecks: []healthCheckConfig{\\n+\\t\\t\\t\\t{\\n+\\t\\t\\t\\t\\t// current kubeadm etcd liveness check failed to detect raft loop deadlock in steady state\\n+\\t\\t\\t\\t\\turl:              \\\"/health?serializable=true\\\",\\n+\\t\\t\\t\\t\\texpectStatusCode: http.StatusOK,\\n+\\t\\t\\t\\t},\\n+\\t\\t\\t\\t{\\n+\\t\\t\\t\\t\\turl:                \\\"/health?serializable=false\\\",\\n+\\t\\t\\t\\t\\texpectTimeoutError: true,\\n+\\t\\t\\t\\t},\\n+\\t\\t\\t},\\n+\\t\\t},\\n+\\t}\\n+\\n+\\tfor _, tc := range tcs {\\n+\\t\\tt.Run(tc.name, func(t *testing.T) {\\n+\\t\\t\\tctx, cancel := context.WithTimeout(context.Background(), 10*time.Second)\\n+\\t\\t\\tdefer cancel()\\n+\\t\\t\\tclus, err := e2e.NewEtcdProcessCluster(ctx, t, tc.clusterOptions...)\\n+\\t\\t\\trequire.NoError(t, err)\\n+\\t\\t\\tdefer clus.Close()\\n+\\t\\t\\ttestutils.ExecuteUntil(ctx, t, func() {\\n+\\t\\t\\t\\tif tc.authEnabled {\\n+\\t\\t\\t\\t\\tsetupAuth(ctx, t, clus.Etcdctl())\\n+\\t\\t\\t\\t}\\n+\\t\\t\\t\\ttc.injectFailure(ctx, t, clus)\\n+\\n+\\t\\t\\t\\tfor _, hc := range tc.healthChecks {\\n+\\t\\t\\t\\t\\trequestURL := clus.Procs[0].EndpointsHTTP()[0] + hc.url\\n+\\t\\t\\t\\t\\tt.Logf(\\\"health check URL is %s\\\", requestURL)\\n+\\t\\t\\t\\t\\tdoHealthCheckAndVerify(t, client, requestURL, hc.expectStatusCode, hc.expectTimeoutError)\\n+\\t\\t\\t\\t}\\n+\\t\\t\\t})\\n+\\t\\t})\\n+\\t}\\n+}\\n+\\n+func doHealthCheckAndVerify(t *testing.T, client *http.Client, url string, expectStatusCode int, expectTimeoutError bool) {\\n+\\tctx, cancel := context.WithTimeout(context.Background(), 2*time.Second)\\n+\\treq, err := http.NewRequestWithContext(ctx, \\\"GET\\\", url, nil)\\n+\\trequire.NoErrorf(t, err, \\\"failed to creat request %+v\\\", err)\\n+\\tresp, herr := client.Do(req)\\n+\\tcancel()\\n+\\tif expectTimeoutError {\\n+\\t\\trequire.Contains(t, herr.Error(), context.DeadlineExceeded.Error())\\n+\\t\\treturn\\n+\\t}\\n+\\trequire.NoErrorf(t, herr, \\\"failed to get response %+v\\\", err)\\n+\\n+\\tbody, err := io.ReadAll(resp.Body)\\n+\\tresp.Body.Close()\\n+\\trequire.NoErrorf(t, err, \\\"failed to read response %+v\\\", err)\\n+\\n+\\tt.Logf(\\\"health check response body is: %s\\\", body)\\n+\\trequire.Equal(t, expectStatusCode, resp.StatusCode)\\n+}\\n+\\n+func noop(_ context.Context, _ *testing.T, _ *e2e.EtcdProcessCluster) {}\\n+\\n+func triggerNoSpaceAlarm(ctx context.Context, t *testing.T, clus *e2e.EtcdProcessCluster) {\\n+\\tbuf := strings.Repeat(\\\"b\\\", os.Getpagesize())\\n+\\tetcdctl := clus.Etcdctl()\\n+\\tfor {\\n+\\t\\tif err := etcdctl.Put(ctx, \\\"foo\\\", buf, config.PutOptions{}); err != nil {\\n+\\t\\t\\tif !strings.Contains(err.Error(), \\\"etcdserver: mvcc: database space exceeded\\\") {\\n+\\t\\t\\t\\tt.Fatal(err)\\n+\\t\\t\\t}\\n+\\t\\t\\tbreak\\n+\\t\\t}\\n+\\t}\\n+}\\n+\\n+func triggerSlowApply(ctx context.Context, t *testing.T, clus *e2e.EtcdProcessCluster) {\\n+\\t// the following proposal will be blocked at applying stage\\n+\\t// because when apply index < committed index, linearizable read would time out.\\n+\\trequire.NoError(t, clus.Procs[0].Failpoints().Setup(ctx, \\\"beforeApplyEntryNormal\\\", `sleep(3000)`))\", \"side\": \"RIGHT\", \"created_at\": \"2023-10-06T07:55:13+00:00\", \"updated_at\": \"2023-10-06T07:55:32+00:00\", \"html_url\": \"https://github.com/etcd-io/etcd/pull/16698#discussion_r1348374074\"}\n",
      "     {\"id\": 1348379362, \"user_login\": \"serathius\", \"body\": \"Love this test!\", \"path\": \"tests/e2e/http_health_check_test.go\", \"position\": 92, \"original_position\": 94, \"commit_id\": \"1324f032542b61db1d052a541d4d78abe7b0645b\", \"original_commit_id\": \"8c55ebf6e247331dc9e43a1880989a35d58b6870\", \"diff_hunk\": \"@@ -0,0 +1,211 @@\\n+// Copyright 2023 The etcd Authors\\n+//\\n+// Licensed under the Apache License, Version 2.0 (the \\\"License\\\");\\n+// you may not use this file except in compliance with the License.\\n+// You may obtain a copy of the License at\\n+//\\n+//     http://www.apache.org/licenses/LICENSE-2.0\\n+//\\n+// Unless required by applicable law or agreed to in writing, software\\n+// distributed under the License is distributed on an \\\"AS IS\\\" BASIS,\\n+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\n+// See the License for the specific language governing permissions and\\n+// limitations under the License.\\n+\\n+//go:build !cluster_proxy\\n+\\n+package e2e\\n+\\n+import (\\n+\\t\\\"context\\\"\\n+\\t\\\"io\\\"\\n+\\t\\\"net/http\\\"\\n+\\t\\\"os\\\"\\n+\\t\\\"strings\\\"\\n+\\t\\\"testing\\\"\\n+\\t\\\"time\\\"\\n+\\n+\\t\\\"github.com/stretchr/testify/require\\\"\\n+\\n+\\t\\\"go.etcd.io/etcd/tests/v3/framework/config\\\"\\n+\\t\\\"go.etcd.io/etcd/tests/v3/framework/e2e\\\"\\n+\\t\\\"go.etcd.io/etcd/tests/v3/framework/testutils\\\"\\n+)\\n+\\n+type healthCheckConfig struct {\\n+\\turl                string\\n+\\texpectStatusCode   int\\n+\\texpectTimeoutError bool\\n+}\\n+\\n+func TestHTTPHealthHandler(t *testing.T) {\\n+\\te2e.BeforeTest(t)\\n+\\tclient := &http.Client{}\\n+\\ttcs := []struct {\\n+\\t\\tname           string\\n+\\t\\tinjectFailure  func(ctx context.Context, t *testing.T, clus *e2e.EtcdProcessCluster)\\n+\\t\\tclusterOptions []e2e.EPClusterOption\\n+\\t\\tauthEnabled    bool\\n+\\t\\thealthChecks   []healthCheckConfig\\n+\\t}{\\n+\\t\\t{\\n+\\t\\t\\tname:           \\\"no failures\\\", // happy case\\n+\\t\\t\\tinjectFailure:  noop,\\n+\\t\\t\\tclusterOptions: []e2e.EPClusterOption{e2e.WithClusterSize(1)},\\n+\\t\\t\\thealthChecks: []healthCheckConfig{\\n+\\t\\t\\t\\t{\\n+\\t\\t\\t\\t\\turl:              \\\"/health\\\",\\n+\\t\\t\\t\\t\\texpectStatusCode: http.StatusOK,\\n+\\t\\t\\t\\t},\\n+\\t\\t\\t},\\n+\\t\\t},\\n+\\t\\t{\\n+\\t\\t\\tname:           \\\"activated no space alarm\\\",\\n+\\t\\t\\tinjectFailure:  triggerNoSpaceAlarm,\\n+\\t\\t\\tclusterOptions: []e2e.EPClusterOption{e2e.WithClusterSize(1), e2e.WithQuotaBackendBytes(int64(13 * os.Getpagesize()))},\\n+\\t\\t\\thealthChecks: []healthCheckConfig{\\n+\\t\\t\\t\\t{\\n+\\t\\t\\t\\t\\turl:              \\\"/health\\\",\\n+\\t\\t\\t\\t\\texpectStatusCode: http.StatusServiceUnavailable,\\n+\\t\\t\\t\\t},\\n+\\t\\t\\t\\t{\\n+\\t\\t\\t\\t\\turl:              \\\"/health?exclude=NOSPACE\\\",\\n+\\t\\t\\t\\t\\texpectStatusCode: http.StatusOK,\\n+\\t\\t\\t\\t},\\n+\\t\\t\\t},\\n+\\t\\t},\\n+\\t\\t{\\n+\\t\\t\\tname:           \\\"overloaded server slow apply\\\",\\n+\\t\\t\\tinjectFailure:  triggerSlowApply,\\n+\\t\\t\\tclusterOptions: []e2e.EPClusterOption{e2e.WithClusterSize(3), e2e.WithGoFailEnabled(true)},\\n+\\t\\t\\thealthChecks: []healthCheckConfig{\\n+\\t\\t\\t\\t{\\n+\\t\\t\\t\\t\\turl:              \\\"/health?serializable=true\\\",\\n+\\t\\t\\t\\t\\texpectStatusCode: http.StatusOK,\\n+\\t\\t\\t\\t},\\n+\\t\\t\\t\\t{\\n+\\t\\t\\t\\t\\turl:                \\\"/health?serializable=false\\\",\\n+\\t\\t\\t\\t\\texpectTimeoutError: true,\\n+\\t\\t\\t\\t},\\n+\\t\\t\\t},\\n+\\t\\t},\\n+\\t\\t{\\n+\\t\\t\\tname:           \\\"network partitioned\\\",\\n+\\t\\t\\tinjectFailure:  blackhole,\", \"side\": \"RIGHT\", \"created_at\": \"2023-10-06T08:00:22+00:00\", \"updated_at\": \"2023-10-06T08:00:31+00:00\", \"html_url\": \"https://github.com/etcd-io/etcd/pull/16698#discussion_r1348379362\"}\n",
      "     ---------------------\n",
      "  └── etcd-io_etcd_16707.diff\n",
      "     --- File preview ---\n",
      "     diff --git a/tests/e2e/cmux_test.go b/tests/e2e/cmux_test.go\n",
      "     index 37d5826d579..24c8a55b5bd 100644\n",
      "     ---------------------\n",
      "  └── etcd-io_etcd_16707_comments.jsonl\n",
      "     --- File preview ---\n",
      "     {\"id\": 1352602475, \"user_login\": \"ahrtr\", \"body\": \"```suggestion\\r\\n\\t\\tvalue := f.Value.String()\\r\\n\\t\\tif value != \\\"\\\" && value != \\\"false\\\" && value != \\\"0\\\" {\\r\\n\\t\\t\\tvalues[f.Name] = value\\r\\n\\t\\t}\\r\\n```\", \"path\": \"tests/framework/e2e/cluster.go\", \"position\": null, \"original_position\": 130, \"commit_id\": \"c34ccfbc061c77b2108f655d74844e741e461cf9\", \"original_commit_id\": \"b4692a0eb78d746bca2480e2a4fd93cfaff67cee\", \"diff_hunk\": \"@@ -654,6 +602,20 @@ func (cfg *EtcdProcessClusterConfig) EtcdServerProcessConfig(tb testing.TB, i in\\n \\t}\\n }\\n \\n+func values(cfg embed.Config) map[string]string {\\n+\\tfs := flag.NewFlagSet(\\\"etcd\\\", flag.ContinueOnError)\\n+\\tcfg.AddFlags(fs)\\n+\\tvalues := map[string]string{}\\n+\\tfs.VisitAll(func(f *flag.Flag) {\\n+\\t\\tvar value string\\n+\\t\\tif value != \\\"false\\\" && value != \\\"0\\\" {\\n+\\t\\t\\tvalue = f.Value.String()\\n+\\t\\t}\\n+\\t\\tvalues[f.Name] = value\", \"side\": \"RIGHT\", \"created_at\": \"2023-10-10T14:17:27+00:00\", \"updated_at\": \"2023-10-10T14:36:58+00:00\", \"html_url\": \"https://github.com/etcd-io/etcd/pull/16707#discussion_r1352602475\"}\n",
      "     {\"id\": 1352675858, \"user_login\": \"serathius\", \"body\": \"I think the result is different, but I will double check if there impact on the list of flags generated.\", \"path\": \"tests/framework/e2e/cluster.go\", \"position\": null, \"original_position\": 130, \"commit_id\": \"c34ccfbc061c77b2108f655d74844e741e461cf9\", \"original_commit_id\": \"b4692a0eb78d746bca2480e2a4fd93cfaff67cee\", \"diff_hunk\": \"@@ -654,6 +602,20 @@ func (cfg *EtcdProcessClusterConfig) EtcdServerProcessConfig(tb testing.TB, i in\\n \\t}\\n }\\n \\n+func values(cfg embed.Config) map[string]string {\\n+\\tfs := flag.NewFlagSet(\\\"etcd\\\", flag.ContinueOnError)\\n+\\tcfg.AddFlags(fs)\\n+\\tvalues := map[string]string{}\\n+\\tfs.VisitAll(func(f *flag.Flag) {\\n+\\t\\tvar value string\\n+\\t\\tif value != \\\"false\\\" && value != \\\"0\\\" {\\n+\\t\\t\\tvalue = f.Value.String()\\n+\\t\\t}\\n+\\t\\tvalues[f.Name] = value\", \"side\": \"RIGHT\", \"created_at\": \"2023-10-10T14:44:22+00:00\", \"updated_at\": \"2023-10-10T14:44:31+00:00\", \"html_url\": \"https://github.com/etcd-io/etcd/pull/16707#discussion_r1352675858\"}\n",
      "     ---------------------\n",
      "  └── etcd-io_etcd_16708.diff\n",
      "     --- File preview ---\n",
      "     diff --git a/.github/workflows/static-analysis.yaml b/.github/workflows/static-analysis.yaml\n",
      "     index a7ee8d6aa03..529cf9747bf 100644\n",
      "     ---------------------\n",
      "  ... (more files hidden)\n",
      "\n",
      "/mnt/object_group/data/raw/facebook\n",
      "\n",
      "/mnt/object_group/data/raw/facebook/react\n",
      "  └── facebook_react_23267.diff\n",
      "     --- File preview ---\n",
      "     diff --git a/packages/react-reconciler/src/ReactFiberThrow.new.js b/packages/react-reconciler/src/ReactFiberThrow.new.js\n",
      "     index 6a0c70f8e6d00..824f89e967707 100644\n",
      "     ---------------------\n",
      "  └── facebook_react_23267_comments.jsonl\n",
      "     --- File preview ---\n",
      "     {\"id\": 803785355, \"user_login\": \"acdlite\", \"body\": \"This is the key change. Because we exit `throwException` without marking a boundary, the root will unwind all the way to the root without completing.\", \"path\": \"packages/react-reconciler/src/ReactFiberThrow.new.js\", \"position\": 120, \"original_position\": 120, \"commit_id\": \"c14ca1f2373b4347260c520ffd5d11185820b8c6\", \"original_commit_id\": \"c14ca1f2373b4347260c520ffd5d11185820b8c6\", \"diff_hunk\": \"@@ -470,24 +472,47 @@ function throwException(\\n         root,\\n         rootRenderLanes,\\n       );\\n-      attachWakeableListeners(\\n-        suspenseBoundary,\\n-        root,\\n-        wakeable,\\n-        rootRenderLanes,\\n-      );\\n+      // We only attach ping listeners in concurrent mode. Legacy Suspense always\\n+      // commits fallbacks synchronously, so there are no pings.\\n+      if (suspenseBoundary.mode & ConcurrentMode) {\\n+        attachPingListener(root, wakeable, rootRenderLanes);\\n+      }\\n+      attachRetryListener(suspenseBoundary, root, wakeable, rootRenderLanes);\\n       return;\\n     } else {\\n-      // No boundary was found. Fallthrough to error mode.\\n+      // No boundary was found. If we're inside startTransition, this is OK.\\n+      // We can suspend and wait for more data to arrive.\\n+\\n+      if (includesOnlyTransitions(rootRenderLanes)) {\\n+        // This is a transition. Suspend. Since we're not activating a Suspense\\n+        // boundary, this will unwind all the way to the root without performing\\n+        // a second pass to render a fallback. (This is arguably how refresh\\n+        // transitions should work, too, since we're not going to commit the\\n+        // fallbacks anyway.)\\n+        attachPingListener(root, wakeable, rootRenderLanes);\\n+        renderDidSuspendDelayIfPossible();\", \"side\": \"RIGHT\", \"created_at\": \"2022-02-10T15:18:17+00:00\", \"updated_at\": \"2022-02-10T15:18:17+00:00\", \"html_url\": \"https://github.com/facebook/react/pull/23267#discussion_r803785355\"}\n",
      "     {\"id\": 803791975, \"user_login\": \"acdlite\", \"body\": \"This is a new root exit status for when the render phase exits without completing. We treat this like a refresh transition: skip the commit phase and suspend the root so we don't continue working on it.\\r\\n\\r\\nIn the future, I expect we could use this mechanism to unwind from an aborted render without doing a second pass to render fallbacks and error boundaries.\", \"path\": \"packages/react-reconciler/src/ReactFiberWorkLoop.new.js\", \"position\": 62, \"original_position\": 62, \"commit_id\": \"c14ca1f2373b4347260c520ffd5d11185820b8c6\", \"original_commit_id\": \"c14ca1f2373b4347260c520ffd5d11185820b8c6\", \"diff_hunk\": \"@@ -838,45 +839,58 @@ function performConcurrentWorkOnRoot(root, didTimeout) {\\n       throw fatalError;\\n     }\\n \\n-    // Check if this render may have yielded to a concurrent event, and if so,\\n-    // confirm that any newly rendered stores are consistent.\\n-    // TODO: It's possible that even a concurrent render may never have yielded\\n-    // to the main thread, if it was fast enough, or if it expired. We could\\n-    // skip the consistency check in that case, too.\\n-    const renderWasConcurrent = !includesBlockingLane(root, lanes);\\n-    const finishedWork: Fiber = (root.current.alternate: any);\\n-    if (\\n-      renderWasConcurrent &&\\n-      !isRenderConsistentWithExternalStores(finishedWork)\\n-    ) {\\n-      // A store was mutated in an interleaved event. Render again,\\n-      // synchronously, to block further mutations.\\n-      exitStatus = renderRootSync(root, lanes);\\n-\\n-      // We need to check again if something threw\\n-      if (exitStatus === RootErrored) {\\n-        const errorRetryLanes = getLanesToRetrySynchronouslyOnError(root);\\n-        if (errorRetryLanes !== NoLanes) {\\n-          lanes = errorRetryLanes;\\n-          exitStatus = recoverFromConcurrentError(root, errorRetryLanes);\\n-          // We assume the tree is now consistent because we didn't yield to any\\n-          // concurrent events.\\n+    if (exitStatus === RootDidNotComplete) {\", \"side\": \"RIGHT\", \"created_at\": \"2022-02-10T15:24:11+00:00\", \"updated_at\": \"2022-02-10T15:53:36+00:00\", \"html_url\": \"https://github.com/facebook/react/pull/23267#discussion_r803791975\"}\n",
      "     ---------------------\n",
      "  └── facebook_react_23280.diff\n",
      "     --- File preview ---\n",
      "     diff --git a/packages/react-reconciler/src/ReactFiberCacheComponent.new.js b/packages/react-reconciler/src/ReactFiberCacheComponent.new.js\n",
      "     index 3ad60546c2b05..cf851af6e3de3 100644\n",
      "     ---------------------\n",
      "  └── facebook_react_23280_comments.jsonl\n",
      "     --- File preview ---\n",
      "     {\"id\": 804971907, \"user_login\": \"acdlite\", \"body\": \"This explanation isn't right, but I believe I know what the issue is. Going offline for a bit. I'll resume this on Monday.\", \"path\": \"packages/react-reconciler/src/__tests__/ReactCache-test.js\", \"position\": 11, \"original_position\": 10, \"commit_id\": \"95495c0a777ecdadb4b3d088230d3ef94540c264\", \"original_commit_id\": \"771740ee3b593a90c77f1e8476178189d1ca7a1e\", \"diff_hunk\": \"@@ -739,14 +739,20 @@ describe('ReactCache', () => {\\n     await act(async () => {\\n       refresh();\\n     });\\n-    expect(Scheduler).toHaveYielded(['Cache miss! [A]', 'Loading...']);\\n+    expect(Scheduler).toHaveYielded([\\n+      'Cache miss! [A]',\\n+      'Loading...',\\n+      // TODO: This happens too early, because we don't retain the refreshed\\n+      // cache until it commits. Will fix in next step.\\n+      'Cache cleanup: A [v1]',\", \"side\": \"RIGHT\", \"created_at\": \"2022-02-11T20:08:23+00:00\", \"updated_at\": \"2022-02-11T20:08:23+00:00\", \"html_url\": \"https://github.com/facebook/react/pull/23280#discussion_r804971907\"}\n",
      "     {\"id\": 805042304, \"user_login\": \"acdlite\", \"body\": \"I looked into it more and I understand what's happening. The new behavior is correct. The v1 cache should be released at this point because there's nothing else in the tree that references it \\u2014 it's all been replaced by a fallback, and when it switches back to visible, it will use v2 cache.\\r\\n\\r\\nThe v1 cache was being kept around longer than necessary because `getSuspendedCachePool` would sometimes return an already-mounted cache.\\r\\n\", \"path\": \"packages/react-reconciler/src/__tests__/ReactCache-test.js\", \"position\": 11, \"original_position\": 10, \"commit_id\": \"95495c0a777ecdadb4b3d088230d3ef94540c264\", \"original_commit_id\": \"771740ee3b593a90c77f1e8476178189d1ca7a1e\", \"diff_hunk\": \"@@ -739,14 +739,20 @@ describe('ReactCache', () => {\\n     await act(async () => {\\n       refresh();\\n     });\\n-    expect(Scheduler).toHaveYielded(['Cache miss! [A]', 'Loading...']);\\n+    expect(Scheduler).toHaveYielded([\\n+      'Cache miss! [A]',\\n+      'Loading...',\\n+      // TODO: This happens too early, because we don't retain the refreshed\\n+      // cache until it commits. Will fix in next step.\\n+      'Cache cleanup: A [v1]',\", \"side\": \"RIGHT\", \"created_at\": \"2022-02-11T22:16:50+00:00\", \"updated_at\": \"2022-02-11T22:16:50+00:00\", \"html_url\": \"https://github.com/facebook/react/pull/23280#discussion_r805042304\"}\n",
      "     ---------------------\n",
      "  └── facebook_react_23304.diff\n",
      "     --- File preview ---\n",
      "     diff --git a/packages/react-dom/src/__tests__/ReactDOMFizzShellHydration-test.js b/packages/react-dom/src/__tests__/ReactDOMFizzShellHydration-test.js\n",
      "     new file mode 100644\n",
      "     ---------------------\n",
      "  ... (more files hidden)\n",
      "\n",
      "/mnt/object_group/data/raw/grafana\n",
      "\n",
      "/mnt/object_group/data/raw/grafana/grafana\n",
      "  └── grafana_grafana_100012.diff\n",
      "     --- File preview ---\n",
      "     diff --git a/packages/grafana-data/src/types/action.ts b/packages/grafana-data/src/types/action.ts\n",
      "     index a3cb099ebe84a..bbdfd91b4f5da 100644\n",
      "     ---------------------\n",
      "  └── grafana_grafana_100012_comments.jsonl\n",
      "     --- File preview ---\n",
      "     ---------------------\n",
      "  └── grafana_grafana_100017.diff\n",
      "     --- File preview ---\n",
      "     diff --git a/pkg/apis/provisioning/v0alpha1/jobs.go b/pkg/apis/provisioning/v0alpha1/jobs.go\n",
      "     index 747907e62e5b9..7e60bafd6bb80 100644\n",
      "     ---------------------\n",
      "  └── grafana_grafana_100017_comments.jsonl\n",
      "     --- File preview ---\n",
      "     {\"id\": 1941883274, \"user_login\": \"ryantxu\", \"body\": \"greatly simplifies logic to avoid renamed (it becomes delete and add)\", \"path\": \"pkg/registry/apis/provisioning/repository/repository.go\", \"position\": 9, \"original_position\": 4, \"commit_id\": \"a2069f423ff8fb467f32dd3dc48d76ac8740909c\", \"original_commit_id\": \"80d5d82a4f030d249bd0a5cdb7ef26e8a84ac5cd\", \"diff_hunk\": \"@@ -52,16 +52,18 @@ type FileAction string\\n const (\\n \\tFileActionCreated FileAction = \\\"created\\\"\\n \\tFileActionUpdated FileAction = \\\"updated\\\"\\n-\\tFileActionRenamed FileAction = \\\"renamed\\\"\", \"side\": \"LEFT\", \"created_at\": \"2025-02-04T20:55:11+00:00\", \"updated_at\": \"2025-02-04T20:55:11+00:00\", \"html_url\": \"https://github.com/grafana/grafana/pull/100017#discussion_r1941883274\"}\n",
      "     {\"id\": 1941885231, \"user_login\": \"ryantxu\", \"body\": \"should the sync button always be `complete`?  Only incremental when from a webhook?\\r\\n\\r\\nThis may be too funky, but it makes things incremental *unless* the sync was called within the last 10s\", \"path\": \"public/app/features/provisioning/SyncRepository.tsx\", \"position\": null, \"original_position\": 8, \"commit_id\": \"a2069f423ff8fb467f32dd3dc48d76ac8740909c\", \"original_commit_id\": \"80d5d82a4f030d249bd0a5cdb7ef26e8a84ac5cd\", \"diff_hunk\": \"@@ -40,7 +40,10 @@ export function SyncRepository({ repository }: Props) {\\n     if (!name) {\\n       return;\\n     }\\n-    syncResource({ name, body: { complete: true } });\\n+    const elapsed = Date.now() - (repository.status?.sync.started ?? 0);\\n+    const complete = elapsed < 10000; // If clicked recently, do complete\\n+    console.log('Queue sync task', { elapsed, complete });\\n+    syncResource({ name, body: { complete } });\", \"side\": \"RIGHT\", \"created_at\": \"2025-02-04T20:56:49+00:00\", \"updated_at\": \"2025-02-04T20:56:49+00:00\", \"html_url\": \"https://github.com/grafana/grafana/pull/100017#discussion_r1941885231\"}\n",
      "     ---------------------\n",
      "  └── grafana_grafana_100020.diff\n",
      "     --- File preview ---\n",
      "     diff --git a/docs/sources/setup-grafana/configure-grafana/_index.md b/docs/sources/setup-grafana/configure-grafana/_index.md\n",
      "     index 5c15ff2c0b88d..6b28b29e34005 100644\n",
      "     ---------------------\n",
      "  ... (more files hidden)\n",
      "\n",
      "/mnt/object_group/data/raw/hashicorp\n",
      "\n",
      "/mnt/object_group/data/raw/hashicorp/terraform\n",
      "  └── hashicorp_terraform_28940.diff\n",
      "     --- File preview ---\n",
      "     diff --git a/website/docs/configuration/functions/type.html.md b/website/docs/language/functions/type.html.md\n",
      "     similarity index 100%\n",
      "     ---------------------\n",
      "  └── hashicorp_terraform_28940_comments.jsonl\n",
      "     --- File preview ---\n",
      "     ---------------------\n",
      "  └── hashicorp_terraform_28973.diff\n",
      "     --- File preview ---\n",
      "     diff --git a/internal/addrs/module.go b/internal/addrs/module.go\n",
      "     index 18f3dbec8ebe..80f394ad7536 100644\n",
      "     ---------------------\n",
      "  └── hashicorp_terraform_28973_comments.jsonl\n",
      "     --- File preview ---\n",
      "     {\"id\": 653805427, \"user_login\": \"jbardin\", \"body\": \"If we're adding the decoder here, I think we should probably add the validation as well, less it slip through the cracks. \", \"path\": \"internal/configs/moved.go\", \"position\": 24, \"original_position\": 24, \"commit_id\": \"0dde821cdb9f8bcfca4220bb81dc1923514ebf61\", \"original_commit_id\": \"ca114d62388434124ff48e37a9311c4449f961b7\", \"diff_hunk\": \"@@ -0,0 +1,70 @@\\n+package configs\\n+\\n+import (\\n+\\t\\\"github.com/hashicorp/hcl/v2\\\"\\n+\\t\\\"github.com/hashicorp/terraform/internal/addrs\\\"\\n+)\\n+\\n+type Moved struct {\\n+\\tFrom *addrs.Target\\n+\\tTo   *addrs.Target\\n+\\n+\\tDeclRange hcl.Range\\n+}\\n+\\n+func decodeMovedBlock(block *hcl.Block) (*Moved, hcl.Diagnostics) {\\n+\\tvar diags hcl.Diagnostics\\n+\\tmoved := &Moved{\\n+\\t\\tDeclRange: block.DefRange,\\n+\\t}\\n+\\n+\\tcontent, moreDiags := block.Body.Content(movedBlockSchema)\\n+\\tdiags = append(diags, moreDiags...)\\n+\\n+\\tif attr, exists := content.Attributes[\\\"from\\\"]; exists {\", \"side\": \"RIGHT\", \"created_at\": \"2021-06-17T17:48:55+00:00\", \"updated_at\": \"2021-06-17T17:48:55+00:00\", \"html_url\": \"https://github.com/hashicorp/terraform/pull/28973#discussion_r653805427\"}\n",
      "     {\"id\": 653806544, \"user_login\": \"jbardin\", \"body\": \"oh, NM, I see it will validate attributes this now\", \"path\": \"internal/configs/moved.go\", \"position\": 24, \"original_position\": 24, \"commit_id\": \"0dde821cdb9f8bcfca4220bb81dc1923514ebf61\", \"original_commit_id\": \"ca114d62388434124ff48e37a9311c4449f961b7\", \"diff_hunk\": \"@@ -0,0 +1,70 @@\\n+package configs\\n+\\n+import (\\n+\\t\\\"github.com/hashicorp/hcl/v2\\\"\\n+\\t\\\"github.com/hashicorp/terraform/internal/addrs\\\"\\n+)\\n+\\n+type Moved struct {\\n+\\tFrom *addrs.Target\\n+\\tTo   *addrs.Target\\n+\\n+\\tDeclRange hcl.Range\\n+}\\n+\\n+func decodeMovedBlock(block *hcl.Block) (*Moved, hcl.Diagnostics) {\\n+\\tvar diags hcl.Diagnostics\\n+\\tmoved := &Moved{\\n+\\t\\tDeclRange: block.DefRange,\\n+\\t}\\n+\\n+\\tcontent, moreDiags := block.Body.Content(movedBlockSchema)\\n+\\tdiags = append(diags, moreDiags...)\\n+\\n+\\tif attr, exists := content.Attributes[\\\"from\\\"]; exists {\", \"side\": \"RIGHT\", \"created_at\": \"2021-06-17T17:50:27+00:00\", \"updated_at\": \"2021-06-17T17:50:27+00:00\", \"html_url\": \"https://github.com/hashicorp/terraform/pull/28973#discussion_r653806544\"}\n",
      "     ---------------------\n",
      "  └── hashicorp_terraform_29000.diff\n",
      "     --- File preview ---\n",
      "     diff --git a/internal/command/plan_test.go b/internal/command/plan_test.go\n",
      "     index d2012bdae176..84bf5f6ae84f 100644\n",
      "     ---------------------\n",
      "  ... (more files hidden)\n",
      "\n",
      "/mnt/object_group/data/raw/huggingface\n",
      "\n",
      "/mnt/object_group/data/raw/huggingface/transformers\n",
      "  └── huggingface_transformers_34253.diff\n",
      "     --- File preview ---\n",
      "     diff --git a/src/transformers/tokenization_utils_base.py b/src/transformers/tokenization_utils_base.py\n",
      "     index 381f3ef497d9..03df02d21ff3 100644\n",
      "     ---------------------\n",
      "  └── huggingface_transformers_34253_comments.jsonl\n",
      "     --- File preview ---\n",
      "     {\"id\": 1816281788, \"user_login\": \"lewtun\", \"body\": \"FYI @Rocketknight1 I kept the `rstrip()` here for consistency with the prior implementation. Let me know if you want it removed.\", \"path\": \"src/transformers/tokenization_utils_base.py\", \"position\": null, \"original_position\": 12, \"commit_id\": \"0f05f9ea90bfeb2afb8da627ab75d7ae83353616\", \"original_commit_id\": \"0fd98e1a05bb987b9ee0cdcb2246de67d3665dd7\", \"diff_hunk\": \"@@ -1877,8 +1877,13 @@ def apply_chat_template(\\n                 final_message = chat[-1][\\\"content\\\"]\\n                 if isinstance(final_message, (list, tuple)):\\n                     final_message = final_message[-1][\\\"text\\\"]\\n-                final_message = final_message.strip()\\n-                rendered_chat = rendered_chat[: rendered_chat.rindex(final_message) + len(final_message)].rstrip()\\n+                try:\\n+                    final_message = final_message\\n+                    rendered_chat = rendered_chat[: rendered_chat.rindex(final_message) + len(final_message)]\\n+                except:  # noqa: E722\\n+                    # Some chat templates like Llama-3.1 trim messages before rendering, so we must do the same here.\\n+                    final_message = final_message.strip()\\n+                    rendered_chat = rendered_chat[: rendered_chat.rindex(final_message) + len(final_message)].rstrip()\", \"side\": \"RIGHT\", \"created_at\": \"2024-10-25T08:49:22+00:00\", \"updated_at\": \"2024-10-25T08:49:22+00:00\", \"html_url\": \"https://github.com/huggingface/transformers/pull/34253#discussion_r1816281788\"}\n",
      "     {\"id\": 1816793391, \"user_login\": \"lewtun\", \"body\": \"Added a regression test here @Rocketknight1 \", \"path\": \"tests/test_tokenization_common.py\", \"position\": 5, \"original_position\": 5, \"commit_id\": \"0f05f9ea90bfeb2afb8da627ab75d7ae83353616\", \"original_commit_id\": \"2cbbc1f6cc25ecab0e76605e006cb3d13f3d94df\", \"diff_hunk\": \"@@ -1357,6 +1357,38 @@ def test_continue_final_message(self):\\n                     \\\"<|im_start|>system\\\\nsystem message<|im_end|>\\\\n<|im_start|>user\\\\nuser message<|im_end|>\\\\n<|im_start|>assistant\\\\nassistant message\\\",\\n                 )\\n \\n+    @require_jinja\\n+    def test_continue_final_message_with_trim(self):\", \"side\": \"RIGHT\", \"created_at\": \"2024-10-25T14:29:39+00:00\", \"updated_at\": \"2024-10-25T14:29:39+00:00\", \"html_url\": \"https://github.com/huggingface/transformers/pull/34253#discussion_r1816793391\"}\n",
      "     ---------------------\n",
      "  └── huggingface_transformers_34266.diff\n",
      "     --- File preview ---\n",
      "     diff --git a/utils/modular_model_converter.py b/utils/modular_model_converter.py\n",
      "     index c107a4831862..bda143c25772 100644\n",
      "     ---------------------\n",
      "  └── huggingface_transformers_34266_comments.jsonl\n",
      "     --- File preview ---\n",
      "     {\"id\": 1810852171, \"user_login\": \"ArthurZucker\", \"body\": \"I think we'd better use `os.path.join` no? This would directly support correct separation? \", \"path\": \"utils/modular_model_converter.py\", \"position\": null, \"original_position\": 25, \"commit_id\": \"5e158af07e5ce3804a32b57708e0c31d4080b2e6\", \"original_commit_id\": \"6b8b5ae868574a0c5ccabc258c21c34eb6aba7be\", \"diff_hunk\": \"@@ -1142,9 +1142,12 @@ def convert_modular_file(modular_file, old_model_name=None, new_model_name=None,\\n         for file, node in cst_transformers.files.items():\\n             if node != {}:\\n                 # Get relative path starting from src/transformers/\\n+                sep = os.sep * 2 if os.sep == \\\"\\\\\\\\\\\" else os.sep\\n                 relative_path = re.search(\\n-                    rf\\\"(src{os.sep}transformers{os.sep}.*|examples{os.sep}.*)\\\", os.path.abspath(modular_file)\\n+                    rf\\\"(src{sep}transformers{sep}.*|examples{sep}.*)\\\", os.path.abspath(modular_file)\", \"side\": \"RIGHT\", \"created_at\": \"2024-10-22T14:33:33+00:00\", \"updated_at\": \"2024-10-22T14:33:50+00:00\", \"html_url\": \"https://github.com/huggingface/transformers/pull/34266#discussion_r1810852171\"}\n",
      "     {\"id\": 1813887348, \"user_login\": \"hlky\", \"body\": \"It's the same separation as `os.sep` with `os.path.join`. This is a python/regex issue due to the combination of raw and formatted.\\r\\n\\r\\nAn alternate solution would be replacing `\\\\\\\\` on `os.path.abspath`, then regex can just use `/`:\\r\\n```python\\r\\nrelative_path = re.search(\\r\\n    r\\\"(src/transformers/.*|examples/.*)\\\", os.path.abspath(modular_file).replace(\\\"\\\\\\\\\\\", \\\"/\\\")\\r\\n).group(1)\\r\\n```\", \"path\": \"utils/modular_model_converter.py\", \"position\": null, \"original_position\": 25, \"commit_id\": \"5e158af07e5ce3804a32b57708e0c31d4080b2e6\", \"original_commit_id\": \"6b8b5ae868574a0c5ccabc258c21c34eb6aba7be\", \"diff_hunk\": \"@@ -1142,9 +1142,12 @@ def convert_modular_file(modular_file, old_model_name=None, new_model_name=None,\\n         for file, node in cst_transformers.files.items():\\n             if node != {}:\\n                 # Get relative path starting from src/transformers/\\n+                sep = os.sep * 2 if os.sep == \\\"\\\\\\\\\\\" else os.sep\\n                 relative_path = re.search(\\n-                    rf\\\"(src{os.sep}transformers{os.sep}.*|examples{os.sep}.*)\\\", os.path.abspath(modular_file)\\n+                    rf\\\"(src{sep}transformers{sep}.*|examples{sep}.*)\\\", os.path.abspath(modular_file)\", \"side\": \"RIGHT\", \"created_at\": \"2024-10-23T22:53:37+00:00\", \"updated_at\": \"2024-10-23T22:53:37+00:00\", \"html_url\": \"https://github.com/huggingface/transformers/pull/34266#discussion_r1813887348\"}\n",
      "     ---------------------\n",
      "  └── huggingface_transformers_34274.diff\n",
      "     --- File preview ---\n",
      "     diff --git a/src/transformers/models/chameleon/modeling_chameleon.py b/src/transformers/models/chameleon/modeling_chameleon.py\n",
      "     index 0661da872799..3255b6f44c05 100644\n",
      "     ---------------------\n",
      "  ... (more files hidden)\n",
      "\n",
      "/mnt/object_group/data/raw/jaegertracing\n",
      "\n",
      "/mnt/object_group/data/raw/jaegertracing/jaeger\n",
      "  └── jaegertracing_jaeger_1005.diff\n",
      "     --- File preview ---\n",
      "     diff --git a/scripts/travis/kafka-integration-test.sh b/scripts/travis/kafka-integration-test.sh\n",
      "     index 71c1810a6aa..47c86976fe7 100644\n",
      "     ---------------------\n",
      "  └── jaegertracing_jaeger_1005_comments.jsonl\n",
      "     --- File preview ---\n",
      "     {\"id\": 210959808, \"user_login\": \"jpkrohling\", \"body\": \"Does it fail immediately if the port is not up, or does it block for a few ms/seconds?\", \"path\": \"scripts/travis/kafka-integration-test.sh\", \"position\": 10, \"original_position\": 10, \"commit_id\": \"7e0d9683288695385ed16cd0207ded432bb9e578\", \"original_commit_id\": \"a0c6b728ffe021940d3778efbb963d3774073612\", \"diff_hunk\": \"@@ -4,6 +4,14 @@ set -e\\n \\n docker pull spotify/kafka\\n CID=$(docker run -d -p 2181:2181 -p 9092:9092 --env ADVERTISED_HOST=localhost --env ADVERTISED_PORT=9092 --rm spotify/kafka)\\n+\\n+# Guarantees no matter what happens, docker will remove the instance at the end.\\n+trap 'docker rm -f $CID 2>/dev/null' EXIT INT TERM\\n+\\n export STORAGE=kafka\\n+while true; do\\n+    if nc -z localhost 9092; then\", \"side\": \"RIGHT\", \"created_at\": \"2018-08-17T16:11:20+00:00\", \"updated_at\": \"2018-08-17T17:25:09+00:00\", \"html_url\": \"https://github.com/jaegertracing/jaeger/pull/1005#discussion_r210959808\"}\n",
      "     {\"id\": 210963657, \"user_login\": \"yurishkuro\", \"body\": \"Looks like it blocks forever until port is available. The build still failed though.\", \"path\": \"scripts/travis/kafka-integration-test.sh\", \"position\": 10, \"original_position\": 10, \"commit_id\": \"7e0d9683288695385ed16cd0207ded432bb9e578\", \"original_commit_id\": \"a0c6b728ffe021940d3778efbb963d3774073612\", \"diff_hunk\": \"@@ -4,6 +4,14 @@ set -e\\n \\n docker pull spotify/kafka\\n CID=$(docker run -d -p 2181:2181 -p 9092:9092 --env ADVERTISED_HOST=localhost --env ADVERTISED_PORT=9092 --rm spotify/kafka)\\n+\\n+# Guarantees no matter what happens, docker will remove the instance at the end.\\n+trap 'docker rm -f $CID 2>/dev/null' EXIT INT TERM\\n+\\n export STORAGE=kafka\\n+while true; do\\n+    if nc -z localhost 9092; then\", \"side\": \"RIGHT\", \"created_at\": \"2018-08-17T16:25:42+00:00\", \"updated_at\": \"2018-08-17T17:25:09+00:00\", \"html_url\": \"https://github.com/jaegertracing/jaeger/pull/1005#discussion_r210963657\"}\n",
      "     ---------------------\n",
      "  └── jaegertracing_jaeger_1009.diff\n",
      "     --- File preview ---\n",
      "     diff --git a/pkg/es/config/config.go b/pkg/es/config/config.go\n",
      "     index 23f813c5df5..7305bc312ca 100644\n",
      "     ---------------------\n",
      "  └── jaegertracing_jaeger_1009_comments.jsonl\n",
      "     --- File preview ---\n",
      "     {\"id\": 211637967, \"user_login\": \"yurishkuro\", \"body\": \"nit: we're doing this for every span write? lots of GC pressure. At least we can pre-concatenate the prefix with the constant string in the constructor. Ideally we would cache the mapping from date (sans time) to string.\", \"path\": \"plugin/storage/es/dependencystore/storage.go\", \"position\": null, \"original_position\": 80, \"commit_id\": \"ddd8ad0db99f4ec3cfc0fd720aa76ec268c41e36\", \"original_commit_id\": \"f08f10ee38247de6d153a403cad48d9835308ff5\", \"diff_hunk\": \"@@ -107,18 +112,18 @@ func buildTSQuery(endTs time.Time, lookback time.Duration) elastic.Query {\\n \\treturn elastic.NewRangeQuery(\\\"timestamp\\\").Gte(endTs.Add(-lookback)).Lte(endTs)\\n }\\n \\n-func getIndices(ts time.Time, lookback time.Duration) []string {\\n+func getIndices(prefix string, ts time.Time, lookback time.Duration) []string {\\n \\tvar indices []string\\n-\\tfirstIndex := indexName(ts.Add(-lookback))\\n-\\tcurrentIndex := indexName(ts)\\n+\\tfirstIndex := indexName(prefix, ts.Add(-lookback))\\n+\\tcurrentIndex := indexName(prefix, ts)\\n \\tfor currentIndex != firstIndex {\\n \\t\\tindices = append(indices, currentIndex)\\n \\t\\tts = ts.Add(-24 * time.Hour)\\n-\\t\\tcurrentIndex = indexName(ts)\\n+\\t\\tcurrentIndex = indexName(prefix, ts)\\n \\t}\\n \\treturn append(indices, firstIndex)\\n }\\n \\n-func indexName(date time.Time) string {\\n-\\treturn dependencyIndexPrefix + date.UTC().Format(\\\"2006-01-02\\\")\\n+func indexName(prefix string, date time.Time) string {\\n+\\treturn prefix + dependencyIndex + date.UTC().Format(\\\"2006-01-02\\\")\", \"side\": \"RIGHT\", \"created_at\": \"2018-08-21T14:54:52+00:00\", \"updated_at\": \"2018-08-22T09:23:46+00:00\", \"html_url\": \"https://github.com/jaegertracing/jaeger/pull/1009#discussion_r211637967\"}\n",
      "     {\"id\": 211638336, \"user_login\": \"yurishkuro\", \"body\": \"could just say `default \\\"\\\"`\", \"path\": \"plugin/storage/es/esCleaner.py\", \"position\": null, \"original_position\": 5, \"commit_id\": \"ddd8ad0db99f4ec3cfc0fd720aa76ec268c41e36\", \"original_commit_id\": \"f08f10ee38247de6d153a403cad48d9835308ff5\", \"diff_hunk\": \"@@ -8,16 +8,23 @@\\n \\n def main():\\n     if len(sys.argv) == 1:\\n-        print('USAGE: [TIMEOUT=(default 120)] %s NUM_OF_DAYS HOSTNAME[:PORT] ...' % sys.argv[0])\\n+        print('USAGE: [TIMEOUT=(default 120)] [INDEX_PREFIX=(default \\\\'\\\\')] %s NUM_OF_DAYS HOSTNAME[:PORT] ...' % sys.argv[0])\", \"side\": \"RIGHT\", \"created_at\": \"2018-08-21T14:55:44+00:00\", \"updated_at\": \"2018-08-22T09:23:46+00:00\", \"html_url\": \"https://github.com/jaegertracing/jaeger/pull/1009#discussion_r211638336\"}\n",
      "     ---------------------\n",
      "  └── jaegertracing_jaeger_1018.diff\n",
      "     --- File preview ---\n",
      "     diff --git a/model/converter/json/domain_span_compare_test.go b/model/converter/json/domain_span_compare_test.go\n",
      "     deleted file mode 100644\n",
      "     ---------------------\n",
      "  ... (more files hidden)\n",
      "\n",
      "/mnt/object_group/data/raw/jenkinsci\n",
      "\n",
      "/mnt/object_group/data/raw/jenkinsci/git-client-plugin\n",
      "  └── jenkinsci_git-client-plugin_1043.diff\n",
      "     --- File preview ---\n",
      "     diff --git a/pom.xml b/pom.xml\n",
      "     index a8b5cc0476..813ed0d394 100644\n",
      "     ---------------------\n",
      "  └── jenkinsci_git-client-plugin_1043_comments.jsonl\n",
      "     --- File preview ---\n",
      "     ---------------------\n",
      "  └── jenkinsci_git-client-plugin_1044.diff\n",
      "     --- File preview ---\n",
      "     diff --git a/pom.xml b/pom.xml\n",
      "     index a8b5cc0476..132c06f3b0 100644\n",
      "     ---------------------\n",
      "  └── jenkinsci_git-client-plugin_1044_comments.jsonl\n",
      "     --- File preview ---\n",
      "     ---------------------\n",
      "  └── jenkinsci_git-client-plugin_1045.diff\n",
      "     --- File preview ---\n",
      "     diff --git a/pom.xml b/pom.xml\n",
      "     index 6a36ff1201..29ccd68a88 100644\n",
      "     ---------------------\n",
      "  ... (more files hidden)\n",
      "\n",
      "/mnt/object_group/data/raw/jenkinsci/git-plugin\n",
      "  └── jenkinsci_git-plugin_1001.diff\n",
      "     --- File preview ---\n",
      "     diff --git a/README.adoc b/README.adoc\n",
      "     index b3e4f56da5..baba232186 100644\n",
      "     ---------------------\n",
      "  └── jenkinsci_git-plugin_1001_comments.jsonl\n",
      "     --- File preview ---\n",
      "     {\"id\": 521897206, \"user_login\": \"timja\", \"body\": \"could this be the default? I can't see people wanting to use this these days\", \"path\": \"README.adoc\", \"position\": null, \"original_position\": 14, \"commit_id\": \"75defe67f9cc9b56b3df80f580b8117e26139bda\", \"original_commit_id\": \"efb3d910a89e806c8f37148d3d77865f4cc7835a\", \"diff_hunk\": \"@@ -193,6 +193,19 @@ Preserve second fetch during initial checkout::\\n   This setting is only needed if there is a bug in the redundant fetch removal logic.\\n   If you enable this setting, please report a git plugin issue that describes why you needed to enable it.\\n \\n+[[do-not-add-git-tag-action-to-jobs]]\\n+Do not add git tag action to jobs::\", \"side\": \"RIGHT\", \"created_at\": \"2020-11-12T07:48:36+00:00\", \"updated_at\": \"2020-11-14T17:15:26+00:00\", \"html_url\": \"https://github.com/jenkinsci/git-plugin/pull/1001#discussion_r521897206\"}\n",
      "     {\"id\": 521897672, \"user_login\": \"timja\", \"body\": \"new screenshot looks way nicer \\ud83d\\ude04 \", \"path\": \"README.adoc\", \"position\": 13, \"original_position\": 13, \"commit_id\": \"75defe67f9cc9b56b3df80f580b8117e26139bda\", \"original_commit_id\": \"efb3d910a89e806c8f37148d3d77865f4cc7835a\", \"diff_hunk\": \"@@ -193,6 +193,19 @@ Preserve second fetch during initial checkout::\\n   This setting is only needed if there is a bug in the redundant fetch removal logic.\\n   If you enable this setting, please report a git plugin issue that describes why you needed to enable it.\\n \\n+[[do-not-add-git-tag-action-to-jobs]]\", \"side\": \"RIGHT\", \"created_at\": \"2020-11-12T07:49:36+00:00\", \"updated_at\": \"2020-11-14T17:15:26+00:00\", \"html_url\": \"https://github.com/jenkinsci/git-plugin/pull/1001#discussion_r521897672\"}\n",
      "     ---------------------\n",
      "  └── jenkinsci_git-plugin_1013.diff\n",
      "     --- File preview ---\n",
      "     diff --git a/src/main/java/hudson/plugins/git/GitSCM.java b/src/main/java/hudson/plugins/git/GitSCM.java\n",
      "     index 39c3f67c9e..982d5de11b 100644\n",
      "     ---------------------\n",
      "  └── jenkinsci_git-plugin_1013_comments.jsonl\n",
      "     --- File preview ---\n",
      "     {\"id\": 540371310, \"user_login\": \"MarkEWaite\", \"body\": \"The branch name \\\"master${GIT_REVISION}\\\" or \\\"master${GIT_COMMIT}\\\" only works because of the happy accident that GIT_REVISION and GIT_COMMIT do not have a value when running in this test.  Using variables with an empty value does not test what we need to test.\\r\\n\\r\\nI believe the definition of GIT_REVISION and GIT_COMMIT does not happen until after the checkout has been performed.  If that's the case, then I'm not sure how a user could use those values in their refspec.\\r\\n\\r\\nI think the test needs to use environment variables that are defined by the Jenkins core, like JOB_BASE_NAME, NODE_NAME, and JENKINS_USER_NAME.\\r\\n\\r\\nThe test probably should also test a parameter value with a parameterized job definition.  The parameter might be USER_SELECTED_BRANCH_NAME or some other name that clearly shows it is provided by the user.\\r\\n\", \"path\": \"src/test/java/hudson/plugins/git/extensions/impl/CloneOptionHonorRefSpecTest.java\", \"position\": null, \"original_position\": 51, \"commit_id\": \"93b85a09b99b95905bca922bc84aa8e104edc5ec\", \"original_commit_id\": \"238ed35505296333590ad2bef950c92180aa7fbf\", \"diff_hunk\": \"@@ -0,0 +1,75 @@\\n+package hudson.plugins.git.extensions.impl;\\n+\\n+import hudson.model.FreeStyleBuild;\\n+import hudson.model.FreeStyleProject;\\n+import hudson.model.Result;\\n+import org.jvnet.hudson.test.Issue;\\n+import hudson.plugins.git.AbstractGitTestCase;\\n+import hudson.plugins.git.BranchSpec;\\n+import hudson.plugins.git.GitSCM;\\n+import hudson.plugins.git.UserRemoteConfig;\\n+import org.eclipse.jgit.transport.RefSpec;\\n+import org.jenkinsci.plugins.gitclient.CloneCommand;\\n+import org.junit.Before;\\n+import org.junit.Test;\\n+import org.junit.runner.RunWith;\\n+import org.junit.runners.Parameterized;\\n+\\n+import java.util.ArrayList;\\n+import java.util.Collection;\\n+import java.util.Collections;\\n+import java.util.List;\\n+\\n+@RunWith(Parameterized.class)\\n+public class CloneOptionHonorRefSpecTest extends AbstractGitTestCase {\\n+\\n+    private String refSpecVar;\\n+\\n+    public CloneOptionHonorRefSpecTest(String useRefSpecVariable) {\\n+        this.refSpecVar = useRefSpecVariable;\\n+    }\\n+\\n+    @Parameterized.Parameters(name = \\\"{0}\\\")\\n+    public static Collection permuteRefSpecVariable() {\\n+        List<Object[]> values = new ArrayList<>();\\n+        String[] allowed = {\\\"${GIT_REVISION}\\\", \\\"${GIT_COMMIT}\\\"};\\n+        for (String refSpecValue : allowed) {\\n+            Object[] combination = {refSpecValue};\\n+            values.add(combination);\\n+        }\\n+        return values;\\n+    }\\n+\\n+    /**\\n+     * This test confirms behavior of refspecs on initial clone with expanded variables.\\n+     * @throws Exception on error\\n+     */\\n+    @Test\\n+    @Issue(\\\"JENKINS-56063\\\")\\n+    public void testRefSpecWithExpandedVariables() throws Exception {\\n+        List<UserRemoteConfig> repos = new ArrayList<>();\\n+        repos.add(new UserRemoteConfig(testRepo.gitDir.getAbsolutePath(), \\\"origin\\\", \\\"+refs/heads/master:refs/remotes/origin/master\\\" + refSpecVar, null));\", \"side\": \"RIGHT\", \"created_at\": \"2020-12-10T17:44:58+00:00\", \"updated_at\": \"2020-12-17T17:57:18+00:00\", \"html_url\": \"https://github.com/jenkinsci/git-plugin/pull/1013#discussion_r540371310\"}\n",
      "     {\"id\": 540373107, \"user_login\": \"MarkEWaite\", \"body\": \"This branch creation is not needed for the test as currently written, but likely will be needed as the test is expanded to check variables that are expanded.  I think a branch will need to be created at this point in the test that is named with the expanded value of refSpecVar.  If refSpecVar is \\\"JENKINS_USER_NAME\\\", then my login user name (\\\"mwaite\\\" in my case) would be the value of the branch name.\", \"path\": \"src/test/java/hudson/plugins/git/extensions/impl/CloneOptionHonorRefSpecTest.java\", \"position\": null, \"original_position\": 70, \"commit_id\": \"93b85a09b99b95905bca922bc84aa8e104edc5ec\", \"original_commit_id\": \"238ed35505296333590ad2bef950c92180aa7fbf\", \"diff_hunk\": \"@@ -0,0 +1,75 @@\\n+package hudson.plugins.git.extensions.impl;\\n+\\n+import hudson.model.FreeStyleBuild;\\n+import hudson.model.FreeStyleProject;\\n+import hudson.model.Result;\\n+import org.jvnet.hudson.test.Issue;\\n+import hudson.plugins.git.AbstractGitTestCase;\\n+import hudson.plugins.git.BranchSpec;\\n+import hudson.plugins.git.GitSCM;\\n+import hudson.plugins.git.UserRemoteConfig;\\n+import org.eclipse.jgit.transport.RefSpec;\\n+import org.jenkinsci.plugins.gitclient.CloneCommand;\\n+import org.junit.Before;\\n+import org.junit.Test;\\n+import org.junit.runner.RunWith;\\n+import org.junit.runners.Parameterized;\\n+\\n+import java.util.ArrayList;\\n+import java.util.Collection;\\n+import java.util.Collections;\\n+import java.util.List;\\n+\\n+@RunWith(Parameterized.class)\\n+public class CloneOptionHonorRefSpecTest extends AbstractGitTestCase {\\n+\\n+    private String refSpecVar;\\n+\\n+    public CloneOptionHonorRefSpecTest(String useRefSpecVariable) {\\n+        this.refSpecVar = useRefSpecVariable;\\n+    }\\n+\\n+    @Parameterized.Parameters(name = \\\"{0}\\\")\\n+    public static Collection permuteRefSpecVariable() {\\n+        List<Object[]> values = new ArrayList<>();\\n+        String[] allowed = {\\\"${GIT_REVISION}\\\", \\\"${GIT_COMMIT}\\\"};\\n+        for (String refSpecValue : allowed) {\\n+            Object[] combination = {refSpecValue};\\n+            values.add(combination);\\n+        }\\n+        return values;\\n+    }\\n+\\n+    /**\\n+     * This test confirms behavior of refspecs on initial clone with expanded variables.\\n+     * @throws Exception on error\\n+     */\\n+    @Test\\n+    @Issue(\\\"JENKINS-56063\\\")\\n+    public void testRefSpecWithExpandedVariables() throws Exception {\\n+        List<UserRemoteConfig> repos = new ArrayList<>();\\n+        repos.add(new UserRemoteConfig(testRepo.gitDir.getAbsolutePath(), \\\"origin\\\", \\\"+refs/heads/master:refs/remotes/origin/master\\\" + refSpecVar, null));\\n+\\n+        /* Set CloneOption to honor refspec on initial clone with expanded var */\\n+        FreeStyleProject projectWithMasterExpanded = setupProject(repos, Collections.singletonList(new BranchSpec(\\\"master\\\" + refSpecVar)), null, false, null);\\n+        CloneOption cloneOptionMasterExpanded = new CloneOption(false, null, null);\\n+        cloneOptionMasterExpanded.setHonorRefspec(true);\\n+        ((GitSCM)projectWithMasterExpanded.getScm()).getExtensions().add(cloneOptionMasterExpanded);\\n+\\n+        /* Set CloneOption to honor refspec on initial clone without expanded var, should fail */\\n+        FreeStyleProject projectWithMaster = setupProject(repos, Collections.singletonList(new BranchSpec(\\\"master\\\")), null, false, null);\\n+        CloneOption cloneOptionMaster = new CloneOption(false, null, null);\\n+        cloneOptionMaster.setHonorRefspec(true);\\n+        ((GitSCM)projectWithMaster.getScm()).getExtensions().add(cloneOptionMaster);\\n+\\n+        // create initial commit\\n+        final String commitFile1 = \\\"commitFile1\\\";\\n+        commit(commitFile1, johnDoe, \\\"Commit in master\\\");\\n+        // create branch and make initial commit\\n+        git.checkout().ref(\\\"master\\\").branch(\\\"foo\\\").execute();\\n+        commit(commitFile1, johnDoe, \\\"Commit in foo\\\");\", \"side\": \"RIGHT\", \"created_at\": \"2020-12-10T17:47:37+00:00\", \"updated_at\": \"2020-12-17T17:57:18+00:00\", \"html_url\": \"https://github.com/jenkinsci/git-plugin/pull/1013#discussion_r540373107\"}\n",
      "     ---------------------\n",
      "  └── jenkinsci_git-plugin_1034.diff\n",
      "     --- File preview ---\n",
      "     diff --git a/src/main/java/hudson/plugins/git/browser/GitRepositoryBrowser.java b/src/main/java/hudson/plugins/git/browser/GitRepositoryBrowser.java\n",
      "     index 33f2f5d2d6..31d253d6d5 100644\n",
      "     ---------------------\n",
      "  ... (more files hidden)\n",
      "\n",
      "/mnt/object_group/data/raw/kubernetes\n",
      "\n",
      "/mnt/object_group/data/raw/kubernetes/kubernetes\n",
      "  └── kubernetes_kubernetes_129070.diff\n",
      "     --- File preview ---\n",
      "     diff --git a/test/e2e/framework/pod/wait.go b/test/e2e/framework/pod/wait.go\n",
      "     index 537d59c32241a..92eec8b2c1417 100644\n",
      "     ---------------------\n",
      "  └── kubernetes_kubernetes_129070_comments.jsonl\n",
      "     --- File preview ---\n",
      "     ---------------------\n",
      "  └── kubernetes_kubernetes_129072.diff\n",
      "     --- File preview ---\n",
      "     diff --git a/pkg/kubelet/apis/config/validation/validation.go b/pkg/kubelet/apis/config/validation/validation.go\n",
      "     index 6633b4e634188..a4195a56a68d8 100644\n",
      "     ---------------------\n",
      "  └── kubernetes_kubernetes_129072_comments.jsonl\n",
      "     --- File preview ---\n",
      "     ---------------------\n",
      "  └── kubernetes_kubernetes_129074.diff\n",
      "     --- File preview ---\n",
      "     diff --git a/cmd/kube-controller-manager/app/options/options_test.go b/cmd/kube-controller-manager/app/options/options_test.go\n",
      "     index c2235a9c0827c..33ba650e60307 100644\n",
      "     ---------------------\n",
      "  ... (more files hidden)\n",
      "\n",
      "/mnt/object_group/data/raw/microsoft\n",
      "\n",
      "/mnt/object_group/data/raw/microsoft/vscode\n",
      "  └── microsoft_vscode_180546.diff\n",
      "     --- File preview ---\n",
      "     diff --git a/src/vs/workbench/contrib/tasks/browser/abstractTaskService.ts b/src/vs/workbench/contrib/tasks/browser/abstractTaskService.ts\n",
      "     index 4549df0584c9e..4392fd3d47330 100644\n",
      "     ---------------------\n",
      "  └── microsoft_vscode_180546_comments.jsonl\n",
      "     --- File preview ---\n",
      "     {\"id\": 1184456339, \"user_login\": \"meganrogge\", \"body\": \"is there a reason we don't want to also check `&& terminalData && terminalData.promise)` here before setting `this._lastTask = this._currentTask`?\", \"path\": \"src/vs/workbench/contrib/tasks/browser/terminalTaskSystem.ts\", \"position\": 16, \"original_position\": 14, \"commit_id\": \"826d3ffb9d4c8c353508fad7723b9757b26e10cc\", \"original_commit_id\": \"d3a1f08be9b0de553a40be2e6723c1d94aefbd24\", \"diff_hunk\": \"@@ -286,17 +286,20 @@ export class TerminalTaskSystem extends Disposable implements ITaskSystem {\\n \\tpublic run(task: Task, resolver: ITaskResolver, trigger: string = Triggers.command): ITaskExecuteResult {\\n \\t\\ttask = task.clone(); // A small amount of task state is stored in the task (instance) and tasks passed in to run may have that set already.\\n \\t\\tconst recentTaskKey = task.getRecentlyUsedKey() ?? '';\\n-\\t\\tconst validInstance = task.runOptions && task.runOptions.instanceLimit && this._instances[recentTaskKey] && this._instances[recentTaskKey].instances < task.runOptions.instanceLimit;\\n+\\t\\tconst validInstance = !this._instances[recentTaskKey] || this._instances[recentTaskKey].instances < ((task.runOptions && task.runOptions.instanceLimit) ?? 0);\\n \\t\\tconst instance = this._instances[recentTaskKey] ? this._instances[recentTaskKey].instances : 0;\\n \\t\\tthis._currentTask = new VerifiedTask(task, resolver, trigger);\\n \\t\\tif (instance > 0) {\\n \\t\\t\\ttask.instance = this._instances[recentTaskKey].counter;\\n \\t\\t}\\n \\t\\tconst lastTaskInstance = this.getLastInstance(task);\\n \\t\\tconst terminalData = lastTaskInstance ? this._activeTasks[lastTaskInstance.getMapKey()] : undefined;\\n-\\t\\tif (terminalData && terminalData.promise && !validInstance) {\\n+\\t\\tif (!validInstance) {\", \"side\": \"RIGHT\", \"created_at\": \"2023-05-04T01:34:26+00:00\", \"updated_at\": \"2023-05-04T01:34:27+00:00\", \"html_url\": \"https://github.com/microsoft/vscode/pull/180546#discussion_r1184456339\"}\n",
      "     {\"id\": 1185175066, \"user_login\": \"markw65\", \"body\": \"For one thing, `promise` isn't nullable, so checking `terminalData.promise` was just redundant.\\r\\n\\r\\nBut this was pretty much the point of this change. If we know that we've exceeded the instance count, it doesn't matter whether there's an entry in _activeTasks or not (especially since prior to #180617 there was a race, and _activeTasks doesn't get updated until some time after the task has been launched).\\r\\n\\r\\nSo the changes here are no longer strictly needed (but we do need to undo the newly added `&& !task.configurationProperties.dependsOn?.length`), but I think it's worth keeping anyway. The logic here (maintained in `_instances`) is self contained; we shouldn't ignore it because of some external conditions.\", \"path\": \"src/vs/workbench/contrib/tasks/browser/terminalTaskSystem.ts\", \"position\": 16, \"original_position\": 14, \"commit_id\": \"826d3ffb9d4c8c353508fad7723b9757b26e10cc\", \"original_commit_id\": \"d3a1f08be9b0de553a40be2e6723c1d94aefbd24\", \"diff_hunk\": \"@@ -286,17 +286,20 @@ export class TerminalTaskSystem extends Disposable implements ITaskSystem {\\n \\tpublic run(task: Task, resolver: ITaskResolver, trigger: string = Triggers.command): ITaskExecuteResult {\\n \\t\\ttask = task.clone(); // A small amount of task state is stored in the task (instance) and tasks passed in to run may have that set already.\\n \\t\\tconst recentTaskKey = task.getRecentlyUsedKey() ?? '';\\n-\\t\\tconst validInstance = task.runOptions && task.runOptions.instanceLimit && this._instances[recentTaskKey] && this._instances[recentTaskKey].instances < task.runOptions.instanceLimit;\\n+\\t\\tconst validInstance = !this._instances[recentTaskKey] || this._instances[recentTaskKey].instances < ((task.runOptions && task.runOptions.instanceLimit) ?? 0);\\n \\t\\tconst instance = this._instances[recentTaskKey] ? this._instances[recentTaskKey].instances : 0;\\n \\t\\tthis._currentTask = new VerifiedTask(task, resolver, trigger);\\n \\t\\tif (instance > 0) {\\n \\t\\t\\ttask.instance = this._instances[recentTaskKey].counter;\\n \\t\\t}\\n \\t\\tconst lastTaskInstance = this.getLastInstance(task);\\n \\t\\tconst terminalData = lastTaskInstance ? this._activeTasks[lastTaskInstance.getMapKey()] : undefined;\\n-\\t\\tif (terminalData && terminalData.promise && !validInstance) {\\n+\\t\\tif (!validInstance) {\", \"side\": \"RIGHT\", \"created_at\": \"2023-05-04T15:13:23+00:00\", \"updated_at\": \"2023-05-04T15:13:23+00:00\", \"html_url\": \"https://github.com/microsoft/vscode/pull/180546#discussion_r1185175066\"}\n",
      "     ---------------------\n",
      "  └── microsoft_vscode_180569.diff\n",
      "     --- File preview ---\n",
      "     diff --git a/extensions/notebook-renderers/src/index.ts b/extensions/notebook-renderers/src/index.ts\n",
      "     index 826784b89052f..0809ac9542fa5 100644\n",
      "     ---------------------\n",
      "  └── microsoft_vscode_180569_comments.jsonl\n",
      "     --- File preview ---\n",
      "     {\"id\": 1175581537, \"user_login\": \"rebornix\", \"body\": \"we also want to support non-scrollable output container I think.\", \"path\": \"extensions/notebook-renderers/src/index.ts\", \"position\": 4, \"original_position\": 4, \"commit_id\": \"3897a9cafb06dc36c9d869b9188cd9057c01a573\", \"original_commit_id\": \"33c124ca7ebd0777344d0df236c6556902cf32c4\", \"diff_hunk\": \"@@ -205,6 +205,7 @@ function initializeScroll(scrollableElement: HTMLElement, disposables: Disposabl\\n \\t\\tscrollableElement.scrollTop = scrollTop !== undefined ? scrollTop : scrollableElement.scrollHeight;\\n \\t\\tscrollableElement.addEventListener('scroll', onScrollHandler);\\n \\t\\tdisposables.push({ dispose: () => scrollableElement.removeEventListener('scroll', onScrollHandler) });\\n+\\t\\tscrollableElement.tabIndex = 0;\", \"side\": \"RIGHT\", \"created_at\": \"2023-04-24T17:18:16+00:00\", \"updated_at\": \"2023-04-24T17:20:59+00:00\", \"html_url\": \"https://github.com/microsoft/vscode/pull/180569#discussion_r1175581537\"}\n",
      "     {\"id\": 1175583787, \"user_login\": \"rebornix\", \"body\": \"Tested this and it doesn't seem to work as expected:\\n\\n* When using Voice Over, it says \\\"Escape\\\" to focus on cell container but it doesn't seem to work.\\n* We might only want to have this keybinding in accessibility mode, otherwise it's very easy to steal normal Escape handling in outputs (e.g., Escape to quit Input's Auto Correction or IME).\", \"path\": \"src/vs/workbench/contrib/notebook/browser/controller/editActions.ts\", \"position\": null, \"original_position\": 14, \"commit_id\": \"3897a9cafb06dc36c9d869b9188cd9057c01a573\", \"original_commit_id\": \"33c124ca7ebd0777344d0df236c6556902cf32c4\", \"diff_hunk\": \"@@ -103,6 +103,11 @@ registerAction2(class QuitEditCellAction extends NotebookCellAction {\\n \\t\\t\\t\\t\\t\\tprimary: KeyCode.Escape,\\n \\t\\t\\t\\t\\t\\tweight: NOTEBOOK_EDITOR_WIDGET_ACTION_WEIGHT - 5\\n \\t\\t\\t\\t\\t},\\n+\\t\\t\\t\\t\\t{\\n+\\t\\t\\t\\t\\t\\twhen: ContextKeyExpr.and(NOTEBOOK_OUTPUT_FOCUSED, NOTEBOOK_EDITOR_FOCUSED),\", \"side\": \"RIGHT\", \"created_at\": \"2023-04-24T17:20:41+00:00\", \"updated_at\": \"2023-04-24T17:20:59+00:00\", \"html_url\": \"https://github.com/microsoft/vscode/pull/180569#discussion_r1175583787\"}\n",
      "     ---------------------\n",
      "  └── microsoft_vscode_180617.diff\n",
      "     --- File preview ---\n",
      "     diff --git a/src/vs/workbench/contrib/tasks/browser/terminalTaskSystem.ts b/src/vs/workbench/contrib/tasks/browser/terminalTaskSystem.ts\n",
      "     index b0be6807914b7..7c3974cfb62ab 100644\n",
      "     ---------------------\n",
      "  ... (more files hidden)\n",
      "\n",
      "/mnt/object_group/data/raw/prometheus\n",
      "\n",
      "/mnt/object_group/data/raw/prometheus/prometheus\n",
      "  └── prometheus_prometheus_10347.diff\n",
      "     --- File preview ---\n",
      "     diff --git a/CHANGELOG.md b/CHANGELOG.md\n",
      "     index 7d6edc3664a..406abe48a52 100644\n",
      "     ---------------------\n",
      "  └── prometheus_prometheus_10347_comments.jsonl\n",
      "     --- File preview ---\n",
      "     {\"id\": 813463529, \"user_login\": \"LeviHarrison\", \"body\": \"I think this might fall under the `[CHANGE]` category because it removes the old Jaeger tracing support. Also, maybe that migration aspect of it should be made more clear in the description.\", \"path\": \"CHANGELOG.md\", \"position\": null, \"original_position\": 4, \"commit_id\": \"db1737f91995814b75803d2a5b954d921750b9fa\", \"original_commit_id\": \"de2129b9ab50f59a96af87de5b477d161d51c924\", \"diff_hunk\": \"@@ -1,3 +1,18 @@\\n+## 2.34.0-rc.0 / 2022-02-24\\n+\\n+* [CHANGE] UI: Classic UI removed. #10208\\n+* [FEATURE] Tracing: OpenTelemetry based tracing. #9724, #10203, #10276\", \"side\": \"RIGHT\", \"created_at\": \"2022-02-24T01:08:33+00:00\", \"updated_at\": \"2022-02-24T01:10:21+00:00\", \"html_url\": \"https://github.com/prometheus/prometheus/pull/10347#discussion_r813463529\"}\n",
      "     {\"id\": 813463727, \"user_login\": \"LeviHarrison\", \"body\": \"Does #10338 need to be included in this list?\", \"path\": \"CHANGELOG.md\", \"position\": null, \"original_position\": 4, \"commit_id\": \"db1737f91995814b75803d2a5b954d921750b9fa\", \"original_commit_id\": \"de2129b9ab50f59a96af87de5b477d161d51c924\", \"diff_hunk\": \"@@ -1,3 +1,18 @@\\n+## 2.34.0-rc.0 / 2022-02-24\\n+\\n+* [CHANGE] UI: Classic UI removed. #10208\\n+* [FEATURE] Tracing: OpenTelemetry based tracing. #9724, #10203, #10276\", \"side\": \"RIGHT\", \"created_at\": \"2022-02-24T01:09:04+00:00\", \"updated_at\": \"2022-02-24T01:10:21+00:00\", \"html_url\": \"https://github.com/prometheus/prometheus/pull/10347#discussion_r813463727\"}\n",
      "     ---------------------\n",
      "  └── prometheus_prometheus_10365.diff\n",
      "     --- File preview ---\n",
      "     diff --git a/config/config_test.go b/config/config_test.go\n",
      "     index c82601fcc24..7a84f5c7749 100644\n",
      "     ---------------------\n",
      "  └── prometheus_prometheus_10365_comments.jsonl\n",
      "     --- File preview ---\n",
      "     {\"id\": 834136528, \"user_login\": \"roidelapluie\", \"body\": \"We should check the error here to avoid deferring a nil pointer\", \"path\": \"discovery/azure/azure.go\", \"position\": 78, \"original_position\": 76, \"commit_id\": \"54867fedafa5e6bfd5cedeca7f7e6eea658988d4\", \"original_commit_id\": \"10b6d005291b51c092a1eac428f50e00290c484b\", \"diff_hunk\": \"@@ -429,9 +431,25 @@ func (client *azureClient) getVMs(ctx context.Context) ([]virtualMachine, error)\\n \\treturn vms, nil\\n }\\n \\n-func (client *azureClient) getScaleSets(ctx context.Context) ([]compute.VirtualMachineScaleSet, error) {\\n+type VmssListResultPage interface {\\n+\\tNextWithContext(ctx context.Context) (err error)\\n+\\tNotDone() bool\\n+\\tValues() []compute.VirtualMachineScaleSet\\n+}\\n+\\n+func (client *azureClient) getScaleSets(ctx context.Context, resourceGroup string) ([]compute.VirtualMachineScaleSet, error) {\\n \\tvar scaleSets []compute.VirtualMachineScaleSet\\n-\\tresult, err := client.vmss.ListAll(ctx)\\n+\\tvar result VmssListResultPage\\n+\\tvar err error\\n+\\tif len(resourceGroup) == 0 {\\n+\\t\\tvar rtn compute.VirtualMachineScaleSetListWithLinkResultPage\\n+\\t\\trtn, err = client.vmss.ListAll(ctx)\", \"side\": \"RIGHT\", \"created_at\": \"2022-03-24T10:12:34+00:00\", \"updated_at\": \"2022-03-24T10:12:34+00:00\", \"html_url\": \"https://github.com/prometheus/prometheus/pull/10365#discussion_r834136528\"}\n",
      "     {\"id\": 834136621, \"user_login\": \"roidelapluie\", \"body\": \"Same\", \"path\": \"discovery/azure/azure.go\", \"position\": 85, \"original_position\": 80, \"commit_id\": \"54867fedafa5e6bfd5cedeca7f7e6eea658988d4\", \"original_commit_id\": \"10b6d005291b51c092a1eac428f50e00290c484b\", \"diff_hunk\": \"@@ -429,9 +431,25 @@ func (client *azureClient) getVMs(ctx context.Context) ([]virtualMachine, error)\\n \\treturn vms, nil\\n }\\n \\n-func (client *azureClient) getScaleSets(ctx context.Context) ([]compute.VirtualMachineScaleSet, error) {\\n+type VmssListResultPage interface {\\n+\\tNextWithContext(ctx context.Context) (err error)\\n+\\tNotDone() bool\\n+\\tValues() []compute.VirtualMachineScaleSet\\n+}\\n+\\n+func (client *azureClient) getScaleSets(ctx context.Context, resourceGroup string) ([]compute.VirtualMachineScaleSet, error) {\\n \\tvar scaleSets []compute.VirtualMachineScaleSet\\n-\\tresult, err := client.vmss.ListAll(ctx)\\n+\\tvar result VmssListResultPage\\n+\\tvar err error\\n+\\tif len(resourceGroup) == 0 {\\n+\\t\\tvar rtn compute.VirtualMachineScaleSetListWithLinkResultPage\\n+\\t\\trtn, err = client.vmss.ListAll(ctx)\\n+\\t\\tresult = &rtn\\n+\\t} else {\\n+\\t\\tvar rtn compute.VirtualMachineScaleSetListResultPage\\n+\\t\\trtn, err = client.vmss.List(ctx, resourceGroup)\", \"side\": \"RIGHT\", \"created_at\": \"2022-03-24T10:12:41+00:00\", \"updated_at\": \"2022-03-24T10:12:41+00:00\", \"html_url\": \"https://github.com/prometheus/prometheus/pull/10365#discussion_r834136621\"}\n",
      "     ---------------------\n",
      "  └── prometheus_prometheus_10367.diff\n",
      "     --- File preview ---\n",
      "     diff --git a/promql/engine_test.go b/promql/engine_test.go\n",
      "     index b64e32ba467..237ff7239ea 100644\n",
      "     ---------------------\n",
      "  ... (more files hidden)\n",
      "\n",
      "/mnt/object_group/data/raw/tensorflow\n",
      "\n",
      "/mnt/object_group/data/raw/tensorflow/tensorflow\n",
      "  └── tensorflow_tensorflow_50020.diff\n",
      "     --- File preview ---\n",
      "     diff --git a/tensorflow/compiler/mlir/hlo/BUILD b/tensorflow/compiler/mlir/hlo/BUILD\n",
      "     index abae4e58ce15c9..345cf2e961cdad 100644\n",
      "     ---------------------\n",
      "  └── tensorflow_tensorflow_50020_comments.jsonl\n",
      "     --- File preview ---\n",
      "     {\"id\": 649365973, \"user_login\": \"joker-eph\", \"body\": \"consumered > consumed\", \"path\": \"tensorflow/compiler/mlir/hlo/include/mlir-hlo/Dialect/mhlo/transforms/fusion_utils.h\", \"position\": null, \"original_position\": 40, \"commit_id\": \"923c8d61f7fe00a2a0df22d5be396508f0667964\", \"original_commit_id\": \"4e73f3833449e174b06b3a62b09704b706b8a810\", \"diff_hunk\": \"@@ -0,0 +1,239 @@\\n+/* Copyright 2021 The TensorFlow Authors. All Rights Reserved.\\n+\\n+Licensed under the Apache License, Version 2.0 (the \\\"License\\\");\\n+you may not use this file except in compliance with the License.\\n+You may obtain a copy of the License at\\n+\\n+    http://www.apache.org/licenses/LICENSE-2.0\\n+\\n+Unless required by applicable law or agreed to in writing, software\\n+distributed under the License is distributed on an \\\"AS IS\\\" BASIS,\\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\n+See the License for the specific language governing permissions and\\n+limitations under the License.\\n+==============================================================================*/\\n+\\n+#ifndef TENSORFLOW_COMPILER_MLIR_HLO_INCLUDE_MLIR_HLO_DIALECT_MHLO_TRANSFORMS_FUSION_UTILS_H_\\n+#define TENSORFLOW_COMPILER_MLIR_HLO_INCLUDE_MLIR_HLO_DIALECT_MHLO_TRANSFORMS_FUSION_UTILS_H_\\n+\\n+#include <memory>\\n+#include <vector>\\n+\\n+#include \\\"llvm/ADT/EquivalenceClasses.h\\\"\\n+#include \\\"llvm/Support/Debug.h\\\"\\n+#include \\\"mlir-hlo/Dialect/mhlo/IR/lhlo_ops.h\\\"\\n+#include \\\"mlir/Dialect/StandardOps/IR/Ops.h\\\"  // TF:llvm-project\\n+\\n+// This file implements some helper functions and classes used to do fusion\\n+// & code generation.\\n+\\n+namespace mlir {\\n+namespace lmhlo {\\n+\\n+// kLoop fusion template satisfies:\\n+//   - all ops in the fusion pattern are element-wise.\\n+//   - all the shapes of outputs of fusion pattern are same or have same number\\n+//   of elements, and thus can fit into a same parallel loop.\\n+//\\n+// kInput fusion template satisfies:\\n+//   - any op in the fusion pattern is either element-wise or a reduction.\\n+//   - if a op is a reduction, its output cannot be consumered by other\", \"side\": \"RIGHT\", \"created_at\": \"2021-06-10T16:58:13+00:00\", \"updated_at\": \"2021-06-10T19:22:55+00:00\", \"html_url\": \"https://github.com/tensorflow/tensorflow/pull/50020#discussion_r649365973\"}\n",
      "     {\"id\": 649366770, \"user_login\": \"joker-eph\", \"body\": \"Y\", \"path\": \"tensorflow/compiler/mlir/hlo/include/mlir-hlo/Dialect/mhlo/transforms/fusion_utils.h\", \"position\": 41, \"original_position\": 41, \"commit_id\": \"923c8d61f7fe00a2a0df22d5be396508f0667964\", \"original_commit_id\": \"4e73f3833449e174b06b3a62b09704b706b8a810\", \"diff_hunk\": \"@@ -0,0 +1,239 @@\\n+/* Copyright 2021 The TensorFlow Authors. All Rights Reserved.\\n+\\n+Licensed under the Apache License, Version 2.0 (the \\\"License\\\");\\n+you may not use this file except in compliance with the License.\\n+You may obtain a copy of the License at\\n+\\n+    http://www.apache.org/licenses/LICENSE-2.0\\n+\\n+Unless required by applicable law or agreed to in writing, software\\n+distributed under the License is distributed on an \\\"AS IS\\\" BASIS,\\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\n+See the License for the specific language governing permissions and\\n+limitations under the License.\\n+==============================================================================*/\\n+\\n+#ifndef TENSORFLOW_COMPILER_MLIR_HLO_INCLUDE_MLIR_HLO_DIALECT_MHLO_TRANSFORMS_FUSION_UTILS_H_\\n+#define TENSORFLOW_COMPILER_MLIR_HLO_INCLUDE_MLIR_HLO_DIALECT_MHLO_TRANSFORMS_FUSION_UTILS_H_\\n+\\n+#include <memory>\\n+#include <vector>\\n+\\n+#include \\\"llvm/ADT/EquivalenceClasses.h\\\"\\n+#include \\\"llvm/Support/Debug.h\\\"\\n+#include \\\"mlir-hlo/Dialect/mhlo/IR/lhlo_ops.h\\\"\\n+#include \\\"mlir/Dialect/StandardOps/IR/Ops.h\\\"  // TF:llvm-project\\n+\\n+// This file implements some helper functions and classes used to do fusion\\n+// & code generation.\\n+\\n+namespace mlir {\\n+namespace lmhlo {\\n+\\n+// kLoop fusion template satisfies:\\n+//   - all ops in the fusion pattern are element-wise.\\n+//   - all the shapes of outputs of fusion pattern are same or have same number\\n+//   of elements, and thus can fit into a same parallel loop.\\n+//\\n+// kInput fusion template satisfies:\\n+//   - any op in the fusion pattern is either element-wise or a reduction.\\n+//   - if a op is a reduction, its output cannot be consumered by other\\n+//     ops in the same fusion pattern.\", \"side\": \"RIGHT\", \"created_at\": \"2021-06-10T16:59:17+00:00\", \"updated_at\": \"2021-06-10T19:22:56+00:00\", \"html_url\": \"https://github.com/tensorflow/tensorflow/pull/50020#discussion_r649366770\"}\n",
      "     ---------------------\n",
      "  └── tensorflow_tensorflow_50056.diff\n",
      "     --- File preview ---\n",
      "     diff --git a/RELEASE.md b/RELEASE.md\n",
      "     index 977d18ea507d08..70221cefdb9fc3 100644\n",
      "     ---------------------\n",
      "  └── tensorflow_tensorflow_50056_comments.jsonl\n",
      "     --- File preview ---\n",
      "     {\"id\": 645174719, \"user_login\": \"jsimsa\", \"body\": \"replace \\\"is only applicable on\\\" with \\\"only supports\\\"\", \"path\": \"tensorflow/python/data/ops/dataset_ops.py\", \"position\": null, \"original_position\": 15, \"commit_id\": \"893261bc5ae9b1954c3f3c623ac7b0027eb78bfc\", \"original_commit_id\": \"e96f4cbd9e49b18ee389cb39602ca9508084dcbb\", \"diff_hunk\": \"@@ -2723,6 +2723,26 @@ def take_while(self, predicate):\\n \\n     return _TakeWhileDataset(self, predicate)\\n \\n+  def unique(self):\\n+    \\\"\\\"\\\"A transformation that discards duplicate elements of a `Dataset`.\\n+\\n+    Use this transformation to produce a dataset that contains one instance of\\n+    each unique element in the input. For example:\\n+\\n+    >>> dataset = tf.data.Dataset.from_tensor_slices([1, 37, 2, 37, 2, 1])\\n+    >>> dataset = dataset.unique()\\n+    >>> sorted(list(dataset.as_numpy_iterator()))\\n+    [1, 2, 37]\\n+\\n+    NOTE: This transformation is only applicable on datasets with either\", \"side\": \"RIGHT\", \"created_at\": \"2021-06-03T22:36:24+00:00\", \"updated_at\": \"2021-06-03T22:36:24+00:00\", \"html_url\": \"https://github.com/tensorflow/tensorflow/pull/50056#discussion_r645174719\"}\n",
      "     {\"id\": 645175178, \"user_login\": \"jsimsa\", \"body\": \"The documentation should also mention that the transformation does not support datasets that do not fit into memory.\", \"path\": \"tensorflow/python/data/ops/dataset_ops.py\", \"position\": null, \"original_position\": 16, \"commit_id\": \"893261bc5ae9b1954c3f3c623ac7b0027eb78bfc\", \"original_commit_id\": \"e96f4cbd9e49b18ee389cb39602ca9508084dcbb\", \"diff_hunk\": \"@@ -2723,6 +2723,26 @@ def take_while(self, predicate):\\n \\n     return _TakeWhileDataset(self, predicate)\\n \\n+  def unique(self):\\n+    \\\"\\\"\\\"A transformation that discards duplicate elements of a `Dataset`.\\n+\\n+    Use this transformation to produce a dataset that contains one instance of\\n+    each unique element in the input. For example:\\n+\\n+    >>> dataset = tf.data.Dataset.from_tensor_slices([1, 37, 2, 37, 2, 1])\\n+    >>> dataset = dataset.unique()\\n+    >>> sorted(list(dataset.as_numpy_iterator()))\\n+    [1, 2, 37]\\n+\\n+    NOTE: This transformation is only applicable on datasets with either\\n+      `tf.int32`, `tf.int64` or `tf.string` type elements.\", \"side\": \"RIGHT\", \"created_at\": \"2021-06-03T22:37:43+00:00\", \"updated_at\": \"2021-06-03T22:37:43+00:00\", \"html_url\": \"https://github.com/tensorflow/tensorflow/pull/50056#discussion_r645175178\"}\n",
      "     ---------------------\n",
      "  └── tensorflow_tensorflow_50073.diff\n",
      "     --- File preview ---\n",
      "     diff --git a/tensorflow/compiler/mlir/hlo/lib/Dialect/mhlo/transforms/lhlo_legalize_to_affine.cc b/tensorflow/compiler/mlir/hlo/lib/Dialect/mhlo/transforms/lhlo_legalize_to_affine.cc\n",
      "     index 2e7d2a49c66192..359e2fa97a8385 100644\n",
      "     ---------------------\n",
      "  ... (more files hidden)\n",
      "\n",
      "/mnt/object_group/data/raw/vercel\n",
      "\n",
      "/mnt/object_group/data/raw/vercel/next.js\n",
      "  └── vercel_next.js_73057.diff\n",
      "     --- File preview ---\n",
      "     diff --git a/bench/heavy-npm-deps/next.config.mjs b/bench/heavy-npm-deps/next.config.mjs\n",
      "     index 02ab1381cc78b..ca1dc40a4b212 100644\n",
      "     ---------------------\n",
      "  └── vercel_next.js_73057_comments.jsonl\n",
      "     --- File preview ---\n",
      "     ---------------------\n",
      "  └── vercel_next.js_73058.diff\n",
      "     --- File preview ---\n",
      "     diff --git a/crates/next-custom-transforms/src/transforms/server_actions.rs b/crates/next-custom-transforms/src/transforms/server_actions.rs\n",
      "     index 454707c957d39..3568bac6adc70 100644\n",
      "     ---------------------\n",
      "  └── vercel_next.js_73058_comments.jsonl\n",
      "     --- File preview ---\n",
      "     {\"id\": 1855085026, \"user_login\": \"huozhi\", \"body\": \"this could be optimized later when there's no usage then we don't insert?\", \"path\": \"crates/next-custom-transforms/tests/fixture/server-actions/server/56/output.js\", \"position\": 3, \"original_position\": 3, \"commit_id\": \"e66a4ceda1e8a8306944c155849414d6ab419b2f\", \"original_commit_id\": \"e66a4ceda1e8a8306944c155849414d6ab419b2f\", \"diff_hunk\": \"@@ -0,0 +1,14 @@\\n+/* __next_internal_action_entry_do_not_use__ {} */ import { registerServerReference } from \\\"private-next-rsc-server-reference\\\";\\n+import { encryptActionBoundArgs, decryptActionBoundArgs } from \\\"private-next-rsc-action-encryption\\\";\\n+import { cache as $$cache__ } from \\\"private-next-rsc-cache-wrapper\\\";\", \"side\": \"RIGHT\", \"created_at\": \"2024-11-23T05:09:48+00:00\", \"updated_at\": \"2024-11-23T05:09:51+00:00\", \"html_url\": \"https://github.com/vercel/next.js/pull/73058#discussion_r1855085026\"}\n",
      "     {\"id\": 1856386789, \"user_login\": \"unstubbable\", \"body\": \"This is handled in #73160.\", \"path\": \"crates/next-custom-transforms/tests/fixture/server-actions/server/56/output.js\", \"position\": 3, \"original_position\": 3, \"commit_id\": \"e66a4ceda1e8a8306944c155849414d6ab419b2f\", \"original_commit_id\": \"e66a4ceda1e8a8306944c155849414d6ab419b2f\", \"diff_hunk\": \"@@ -0,0 +1,14 @@\\n+/* __next_internal_action_entry_do_not_use__ {} */ import { registerServerReference } from \\\"private-next-rsc-server-reference\\\";\\n+import { encryptActionBoundArgs, decryptActionBoundArgs } from \\\"private-next-rsc-action-encryption\\\";\\n+import { cache as $$cache__ } from \\\"private-next-rsc-cache-wrapper\\\";\", \"side\": \"RIGHT\", \"created_at\": \"2024-11-25T10:54:57+00:00\", \"updated_at\": \"2024-11-25T10:54:57+00:00\", \"html_url\": \"https://github.com/vercel/next.js/pull/73058#discussion_r1856386789\"}\n",
      "     ---------------------\n",
      "  └── vercel_next.js_73059.diff\n",
      "     --- File preview ---\n",
      "     diff --git a/crates/next-custom-transforms/src/transforms/server_actions.rs b/crates/next-custom-transforms/src/transforms/server_actions.rs\n",
      "     index 3568bac6adc70..a134ac8369f83 100644\n",
      "     ---------------------\n",
      "  ... (more files hidden)\n"
     ]
    }
   ],
   "source": [
    "root_dir = \"/mnt/object_group/data\"\n",
    "preview_lines = 2  # number of lines/content lines to print\n",
    "\n",
    "for dirpath, dirnames, filenames in os.walk(root_dir):\n",
    "    print(f\"\\n{dirpath}\")\n",
    "    for fname in filenames[:5]:  # preview up to 5 files\n",
    "        full_path = os.path.join(dirpath, fname)\n",
    "        print(f\"  └── {fname}\")\n",
    "        \n",
    "        try:\n",
    "            with open(full_path, 'r') as f:\n",
    "                print(\"     --- File preview ---\")\n",
    "                for i, line in enumerate(f):\n",
    "                    print(\"     \" + line.strip())\n",
    "                    if i + 1 >= preview_lines:\n",
    "                        break\n",
    "                print(\"     ---------------------\")\n",
    "        except UnicodeDecodeError:\n",
    "            print(\"     [Binary or non-text file — skipped]\")\n",
    "        except Exception as e:\n",
    "            print(f\"     [Error reading file: {e}]\")\n",
    "\n",
    "    if len(filenames) > 5:\n",
    "        print(\"  ... (more files hidden)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a501fd69-1265-49c4-a033-f649666fa637",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    {\"comment_id\": 74291026, \"comment_user_login\": \"vikerman\", \"comment_body\": \"Is there a better way to avoid having this in different places?\\n\", \"comment_created_at\": \"2016-08-10T17:30:42+00:00\", \"comment_html_url\": \"https://github.com/angular/angular/pull/10620#discussion_r74291026\", \"comment_path\": \"modules/@angular/common/src/forms-deprecated/directives/abstract_control_directive.ts\", \"comment_position\": 10, \"comment_original_position\": 10, \"comment_commit_id\": \"ee8e802b7bc25eafbe54109b3924e9dbc36dce11\", \"comment_original_commit_id\": \"ee8e802b7bc25eafbe54109b3924e9dbc36dce11\", \"diff_line_content\": \"}\\n\", \"diff_line_type\": \"added\", \"diff_line_source_no\": null, \"diff_line_target_no\": 15, \"diff_hunk_header\": \"\", \"diff\": \"@@ -6,10 +6,13 @@\\n  * found in the LICENSE file at https://angular.io/license\\n  */\\n \\n-import {unimplemented} from '../../facade/exceptions';\\n+import {BaseException} from '@angular/core';\\n import {isPresent} from '../../facade/lang';\\n import {AbstractControl} from '../model';\\n \\n+function unimplemented(): any {\\n+  throw new BaseException('unimplemented');\", \"side\": \"RIGHT\", \"line_offset\": 9, \"diff_file_source\": \"a/modules/@angular/common/src/forms-deprecated/directives/abstract_control_directive.ts\", \"diff_file_target\": \"b/modules/@angular/common/src/forms-deprecated/directives/abstract_control_directive.ts\", \"repo\": \"angular/angular\"}\n",
      "    {\"comment_id\": 74305223, \"comment_user_login\": \"alexeagle\", \"comment_body\": \"we could export it in the public API but that's not nice.\\nOr we could inline the method and delete it.\\n\", \"comment_created_at\": \"2016-08-10T18:42:59+00:00\", \"comment_html_url\": \"https://github.com/angular/angular/pull/10620#discussion_r74305223\", \"comment_path\": \"modules/@angular/common/src/forms-deprecated/directives/abstract_control_directive.ts\", \"comment_position\": 10, \"comment_original_position\": 10, \"comment_commit_id\": \"ee8e802b7bc25eafbe54109b3924e9dbc36dce11\", \"comment_original_commit_id\": \"ee8e802b7bc25eafbe54109b3924e9dbc36dce11\", \"diff_line_content\": \"}\\n\", \"diff_line_type\": \"added\", \"diff_line_source_no\": null, \"diff_line_target_no\": 15, \"diff_hunk_header\": \"\", \"diff\": \"@@ -6,10 +6,13 @@\\n  * found in the LICENSE file at https://angular.io/license\\n  */\\n \\n-import {unimplemented} from '../../facade/exceptions';\\n+import {BaseException} from '@angular/core';\\n import {isPresent} from '../../facade/lang';\\n import {AbstractControl} from '../model';\\n \\n+function unimplemented(): any {\\n+  throw new BaseException('unimplemented');\", \"side\": \"RIGHT\", \"line_offset\": 9, \"diff_file_source\": \"a/modules/@angular/common/src/forms-deprecated/directives/abstract_control_directive.ts\", \"diff_file_target\": \"b/modules/@angular/common/src/forms-deprecated/directives/abstract_control_directive.ts\", \"repo\": \"angular/angular\"}\n",
      "    {\"comment_id\": 74523490, \"comment_user_login\": \"vicb\", \"comment_body\": \"const\\n\", \"comment_created_at\": \"2016-08-11T23:43:20+00:00\", \"comment_html_url\": \"https://github.com/angular/angular/pull/10622#discussion_r74523490\", \"comment_path\": \"modules/@angular/compiler-cli/integrationtest/test/basic_spec.ts\", \"comment_position\": 36, \"comment_original_position\": 36, \"comment_commit_id\": \"616c5c56924b77bc0ef539d01d1cf3596c0da87f\", \"comment_original_commit_id\": \"91f50b2f513f99cf56b7406b3a74066e128b9c7d\", \"diff_line_content\": \"    it('should support i18n for content tags', () => {\\n\", \"diff_line_type\": \"added\", \"diff_line_source_no\": null, \"diff_line_target_no\": 79, \"diff_hunk_header\": \"describe('template codegen output', () => {\", \"diff\": \"@@ -44,24 +44,40 @@ describe('template codegen output', () => {\\n   it('should support ngIf', () => {\\n     var compFixture = createComponent(BasicComp);\\n     var debugElement = compFixture.debugElement;\\n-    expect(debugElement.children.length).toBe(2);\\n+    expect(debugElement.children.length).toBe(3);\\n \\n     compFixture.componentInstance.ctxBool = true;\\n     compFixture.detectChanges();\\n-    expect(debugElement.children.length).toBe(3);\\n+    expect(debugElement.children.length).toBe(4);\\n     expect(debugElement.children[2].injector.get(MultipleComponentsMyComp)).toBeTruthy();\\n   });\\n \\n   it('should support ngFor', () => {\\n     var compFixture = createComponent(BasicComp);\\n     var debugElement = compFixture.debugElement;\\n-    expect(debugElement.children.length).toBe(2);\\n+    expect(debugElement.children.length).toBe(3);\\n \\n     // test NgFor\\n     compFixture.componentInstance.ctxArr = [1, 2];\\n     compFixture.detectChanges();\\n-    expect(debugElement.children.length).toBe(4);\\n+    expect(debugElement.children.length).toBe(5);\\n     expect(debugElement.children[2].attributes['value']).toBe('1');\\n     expect(debugElement.children[3].attributes['value']).toBe('2');\\n   });\\n+\\n+  describe('i18n', () => {\\n+    it('should support i18n for content tags', () => {\\n+      var compFixture = createComponent(BasicComp);\\n+      expect(compFixture.componentInstance.ctxLocale).toEqual('FI');\\n+    });\\n+\\n+    it('should support i18n for content tags', () => {\\n+      var compFixture = createComponent(BasicComp);\", \"side\": \"RIGHT\", \"line_offset\": 35, \"diff_file_source\": \"a/modules/@angular/compiler-cli/integrationtest/test/basic_spec.ts\", \"diff_file_target\": \"b/modules/@angular/compiler-cli/integrationtest/test/basic_spec.ts\", \"repo\": \"angular/angular\"}\n",
      "    {\"comment_id\": 74523504, \"comment_user_login\": \"vicb\", \"comment_body\": \"const\\n\", \"comment_created_at\": \"2016-08-11T23:43:27+00:00\", \"comment_html_url\": \"https://github.com/angular/angular/pull/10622#discussion_r74523504\", \"comment_path\": \"modules/@angular/compiler-cli/integrationtest/test/basic_spec.ts\", \"comment_position\": 31, \"comment_original_position\": 31, \"comment_commit_id\": \"616c5c56924b77bc0ef539d01d1cf3596c0da87f\", \"comment_original_commit_id\": \"91f50b2f513f99cf56b7406b3a74066e128b9c7d\", \"diff_line_content\": \"    it('should inject the translations format into the component', () => {\\n\", \"diff_line_type\": \"added\", \"diff_line_source_no\": null, \"diff_line_target_no\": 74, \"diff_hunk_header\": \"describe('template codegen output', () => {\", \"diff\": \"@@ -44,24 +44,40 @@ describe('template codegen output', () => {\\n   it('should support ngIf', () => {\\n     var compFixture = createComponent(BasicComp);\\n     var debugElement = compFixture.debugElement;\\n-    expect(debugElement.children.length).toBe(2);\\n+    expect(debugElement.children.length).toBe(3);\\n \\n     compFixture.componentInstance.ctxBool = true;\\n     compFixture.detectChanges();\\n-    expect(debugElement.children.length).toBe(3);\\n+    expect(debugElement.children.length).toBe(4);\\n     expect(debugElement.children[2].injector.get(MultipleComponentsMyComp)).toBeTruthy();\\n   });\\n \\n   it('should support ngFor', () => {\\n     var compFixture = createComponent(BasicComp);\\n     var debugElement = compFixture.debugElement;\\n-    expect(debugElement.children.length).toBe(2);\\n+    expect(debugElement.children.length).toBe(3);\\n \\n     // test NgFor\\n     compFixture.componentInstance.ctxArr = [1, 2];\\n     compFixture.detectChanges();\\n-    expect(debugElement.children.length).toBe(4);\\n+    expect(debugElement.children.length).toBe(5);\\n     expect(debugElement.children[2].attributes['value']).toBe('1');\\n     expect(debugElement.children[3].attributes['value']).toBe('2');\\n   });\\n+\\n+  describe('i18n', () => {\\n+    it('should support i18n for content tags', () => {\\n+      var compFixture = createComponent(BasicComp);\", \"side\": \"RIGHT\", \"line_offset\": 30, \"diff_file_source\": \"a/modules/@angular/compiler-cli/integrationtest/test/basic_spec.ts\", \"diff_file_target\": \"b/modules/@angular/compiler-cli/integrationtest/test/basic_spec.ts\", \"repo\": \"angular/angular\"}\n",
      "    {\"comment_id\": 74526501, \"comment_user_login\": \"vicb\", \"comment_body\": \"fi\\n\", \"comment_created_at\": \"2016-08-12T00:18:35+00:00\", \"comment_html_url\": \"https://github.com/angular/angular/pull/10622#discussion_r74526501\", \"comment_path\": \"modules/@angular/compiler-cli/integrationtest/test/basic_spec.ts\", \"comment_position\": 32, \"comment_original_position\": 32, \"comment_commit_id\": \"616c5c56924b77bc0ef539d01d1cf3596c0da87f\", \"comment_original_commit_id\": \"7333b4c6d9527a1a6b812ab6eea2797f228d03bd\", \"diff_line_content\": \"      const compFixture = createComponent(BasicComp);\\n\", \"diff_line_type\": \"added\", \"diff_line_source_no\": null, \"diff_line_target_no\": 75, \"diff_hunk_header\": \"describe('template codegen output', () => {\", \"diff\": \"@@ -44,24 +44,45 @@ describe('template codegen output', () => {\\n   it('should support ngIf', () => {\\n     var compFixture = createComponent(BasicComp);\\n     var debugElement = compFixture.debugElement;\\n-    expect(debugElement.children.length).toBe(2);\\n+    expect(debugElement.children.length).toBe(3);\\n \\n     compFixture.componentInstance.ctxBool = true;\\n     compFixture.detectChanges();\\n-    expect(debugElement.children.length).toBe(3);\\n+    expect(debugElement.children.length).toBe(4);\\n     expect(debugElement.children[2].injector.get(MultipleComponentsMyComp)).toBeTruthy();\\n   });\\n \\n   it('should support ngFor', () => {\\n     var compFixture = createComponent(BasicComp);\\n     var debugElement = compFixture.debugElement;\\n-    expect(debugElement.children.length).toBe(2);\\n+    expect(debugElement.children.length).toBe(3);\\n \\n     // test NgFor\\n     compFixture.componentInstance.ctxArr = [1, 2];\\n     compFixture.detectChanges();\\n-    expect(debugElement.children.length).toBe(4);\\n+    expect(debugElement.children.length).toBe(5);\\n     expect(debugElement.children[2].attributes['value']).toBe('1');\\n     expect(debugElement.children[3].attributes['value']).toBe('2');\\n   });\\n+\\n+  describe('i18n', () => {\\n+    it('should inject the locale into the component', () => {\\n+      const compFixture = createComponent(BasicComp);\\n+      expect(compFixture.componentInstance.ctxLocale).toEqual('FI');\", \"side\": \"RIGHT\", \"line_offset\": 31, \"diff_file_source\": \"a/modules/@angular/compiler-cli/integrationtest/test/basic_spec.ts\", \"diff_file_target\": \"b/modules/@angular/compiler-cli/integrationtest/test/basic_spec.ts\", \"repo\": \"angular/angular\"}\n",
      "    {\"comment_id\": 74526531, \"comment_user_login\": \"vicb\", \"comment_body\": \"remove !\\n\", \"comment_created_at\": \"2016-08-12T00:19:06+00:00\", \"comment_html_url\": \"https://github.com/angular/angular/pull/10622#discussion_r74526531\", \"comment_path\": \"modules/@angular/compiler-cli/src/codegen.ts\", \"comment_position\": 36, \"comment_original_position\": 36, \"comment_commit_id\": \"616c5c56924b77bc0ef539d01d1cf3596c0da87f\", \"comment_original_commit_id\": \"7333b4c6d9527a1a6b812ab6eea2797f228d03bd\", \"diff_line_content\": \"    const reflectorHost = new ReflectorHost(program, compilerHost, options, reflectorHostContext);\\n\", \"diff_line_type\": \"context\", \"diff_line_source_no\": 132, \"diff_line_target_no\": 142, \"diff_hunk_header\": \"export class CodeGenerator {\", \"diff\": \"@@ -88,6 +93,35 @@ export class CodeGenerator {\\n     return path.join(this.options.genDir, relativePath);\\n   }\\n \\n+  private _constructTranslationBundle(fileMetas: any[], analyzedNgModules: any): Promise<any> {\", \"side\": \"RIGHT\", \"line_offset\": 11, \"diff_file_source\": \"a/modules/@angular/compiler-cli/src/codegen.ts\", \"diff_file_target\": \"b/modules/@angular/compiler-cli/src/codegen.ts\", \"repo\": \"angular/angular\"}\n",
      "    {\"comment_id\": 74526731, \"comment_user_login\": \"vicb\", \"comment_body\": \"append at the end to make diff readable\\n\", \"comment_created_at\": \"2016-08-12T00:21:49+00:00\", \"comment_html_url\": \"https://github.com/angular/angular/pull/10622#discussion_r74526731\", \"comment_path\": \"modules/@angular/compiler-cli/src/codegen.ts\", \"comment_position\": 26, \"comment_original_position\": 26, \"comment_commit_id\": \"616c5c56924b77bc0ef539d01d1cf3596c0da87f\", \"comment_original_commit_id\": \"7333b4c6d9527a1a6b812ab6eea2797f228d03bd\", \"diff_line_content\": \"      }\\n\", \"diff_line_type\": \"context\", \"diff_line_source_no\": 129, \"diff_line_target_no\": 132, \"diff_hunk_header\": \"export class CodeGenerator {\", \"diff\": \"@@ -32,7 +35,9 @@ const PREAMBLE = `/**\\n \\n export class CodeGenerator {\\n   constructor(\\n-      private options: AngularCompilerOptions, private program: ts.Program,\\n+      private options: AngularCompilerOptions, private transContent: string,\", \"side\": \"RIGHT\", \"line_offset\": 1, \"diff_file_source\": \"a/modules/@angular/compiler-cli/src/codegen.ts\", \"diff_file_target\": \"b/modules/@angular/compiler-cli/src/codegen.ts\", \"repo\": \"angular/angular\"}\n",
      "    {\"comment_id\": 74527376, \"comment_user_login\": \"vicb\", \"comment_body\": \"This is the only line that you should keep beside loading the file\\n\", \"comment_created_at\": \"2016-08-12T00:30:39+00:00\", \"comment_html_url\": \"https://github.com/angular/angular/pull/10622#discussion_r74527376\", \"comment_path\": \"modules/@angular/compiler-cli/src/codegen.ts\", \"comment_position\": 46, \"comment_original_position\": 130, \"comment_commit_id\": \"616c5c56924b77bc0ef539d01d1cf3596c0da87f\", \"comment_original_commit_id\": \"7333b4c6d9527a1a6b812ab6eea2797f228d03bd\", \"diff_line_content\": \"        new NgModuleCompiler(), new TypeScriptEmitter(reflectorHost), cliOptions.locale,\\n\", \"diff_line_type\": \"added\", \"diff_line_source_no\": null, \"diff_line_target_no\": 164, \"diff_hunk_header\": \"export class CodeGenerator {\", \"diff\": \"@@ -132,7 +175,7 @@ export class CodeGenerator {\\n     const reflectorHost = new ReflectorHost(program, compilerHost, options, reflectorHostContext);\\n     const staticReflector = new StaticReflector(reflectorHost);\\n     StaticAndDynamicReflectionCapabilities.install(staticReflector);\\n-    const htmlParser = new compiler.i18n.HtmlParser(new HtmlParser());\\n+    const htmlParser = new compiler.i18n.HtmlParser(new HtmlParser(), transContent);\", \"side\": \"RIGHT\", \"line_offset\": 3, \"diff_file_source\": \"a/modules/@angular/compiler-cli/src/codegen.ts\", \"diff_file_target\": \"b/modules/@angular/compiler-cli/src/codegen.ts\", \"repo\": \"angular/angular\"}\n",
      "    {\"comment_id\": 74528827, \"comment_user_login\": \"vicb\", \"comment_body\": \"Id\\n\", \"comment_created_at\": \"2016-08-12T00:51:47+00:00\", \"comment_html_url\": \"https://github.com/angular/angular/pull/10622#discussion_r74528827\", \"comment_path\": \"modules/@angular/compiler-cli/integrationtest/src/basic.ts\", \"comment_position\": 15, \"comment_original_position\": 15, \"comment_commit_id\": \"616c5c56924b77bc0ef539d01d1cf3596c0da87f\", \"comment_original_commit_id\": \"e7940fc0e83c9d0a549891774db0a609ec591c19\", \"diff_line_content\": \"  }\\n\", \"diff_line_type\": \"added\", \"diff_line_source_no\": null, \"diff_line_target_no\": 30, \"diff_hunk_header\": \"export class BasicComp {\", \"diff\": \"@@ -23,5 +23,9 @@ export class BasicComp {\\n   ctxProp: string;\\n   ctxBool: boolean;\\n   ctxArr: any[] = [];\\n-  constructor() { this.ctxProp = 'initialValue'; }\\n+  constructor(\\n+      @Inject(LOCALE_ID) public localeID: string,\", \"side\": \"RIGHT\", \"line_offset\": 7, \"diff_file_source\": \"a/modules/@angular/compiler-cli/integrationtest/src/basic.ts\", \"diff_file_target\": \"b/modules/@angular/compiler-cli/integrationtest/src/basic.ts\", \"repo\": \"angular/angular\"}\n",
      "    {\"comment_id\": 74528853, \"comment_user_login\": \"vicb\", \"comment_body\": \"only add the format\\n\", \"comment_created_at\": \"2016-08-12T00:52:10+00:00\", \"comment_html_url\": \"https://github.com/angular/angular/pull/10622#discussion_r74528853\", \"comment_path\": \"modules/@angular/compiler-cli/src/extract_i18n.ts\", \"comment_position\": 6, \"comment_original_position\": 6, \"comment_commit_id\": \"616c5c56924b77bc0ef539d01d1cf3596c0da87f\", \"comment_original_commit_id\": \"e7940fc0e83c9d0a549891774db0a609ec591c19\", \"diff_line_content\": \"  const extractor = Extractor.create(ngOptions, cliOptions.i18nFormat, program, host);\\n\", \"diff_line_type\": \"added\", \"diff_line_source_no\": null, \"diff_line_target_no\": 33, \"diff_hunk_header\": \"import {StaticAndDynamicReflectionCapabilities} from './static_reflection_capabi\", \"diff\": \"@@ -28,8 +28,9 @@ import {StaticAndDynamicReflectionCapabilities} from './static_reflection_capabi\\n import {StaticReflector, StaticSymbol} from './static_reflector';\\n \\n function extract(\\n-    ngOptions: tsc.AngularCompilerOptions, program: ts.Program, host: ts.CompilerHost) {\\n-  const extractor = Extractor.create(ngOptions, program, host);\\n+    ngOptions: tsc.AngularCompilerOptions, cliOptions: tsc.CliOptions, program: ts.Program,\", \"side\": \"RIGHT\", \"line_offset\": 5, \"diff_file_source\": \"a/modules/@angular/compiler-cli/src/extract_i18n.ts\", \"diff_file_target\": \"b/modules/@angular/compiler-cli/src/extract_i18n.ts\", \"repo\": \"angular/angular\"}\n",
      "    {\"comment_id\": 74530724, \"comment_user_login\": \"vicb\", \"comment_body\": \"`, null, null` would be more semantically correct\\n\", \"comment_created_at\": \"2016-08-12T01:22:30+00:00\", \"comment_html_url\": \"https://github.com/angular/angular/pull/10622#discussion_r74530724\", \"comment_path\": \"modules/@angular/compiler-cli/src/extract_i18n.ts\", \"comment_position\": 37, \"comment_original_position\": 37, \"comment_commit_id\": \"616c5c56924b77bc0ef539d01d1cf3596c0da87f\", \"comment_original_commit_id\": \"6781377043d6eae20d6504b83a121ebde353f043\", \"diff_line_content\": \"}\\n\", \"diff_line_type\": \"added\", \"diff_line_source_no\": null, \"diff_line_target_no\": 196, \"diff_hunk_header\": \"interface FileMetadata {\", \"diff\": \"@@ -163,7 +164,7 @@ export class Extractor {\\n         config, console, elementSchemaRegistry, staticReflector);\\n     const offlineCompiler = new compiler.OfflineCompiler(\\n         resolver, normalizer, tmplParser, new StyleCompiler(urlResolver), new ViewCompiler(config),\\n-        new NgModuleCompiler(), new TypeScriptEmitter(reflectorHost));\\n+        new NgModuleCompiler(), new TypeScriptEmitter(reflectorHost), '', translationsFormat);\", \"side\": \"RIGHT\", \"line_offset\": 12, \"diff_file_source\": \"a/modules/@angular/compiler-cli/src/extract_i18n.ts\", \"diff_file_target\": \"b/modules/@angular/compiler-cli/src/extract_i18n.ts\", \"repo\": \"angular/angular\"}\n"
     ]
    }
   ],
   "source": [
    "import gzip\n",
    "train = \"/mnt/object_group/data/processed/train.jsonl.gz\"\n",
    "with gzip.open(train, 'rt', encoding='utf-8') as f:\n",
    "    for i, line in enumerate(f):\n",
    "        print(\"    \" + line.strip())\n",
    "        if i >= 10:\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "483d6154-9d34-468f-ae56-fd0301437a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_ROOT = Path(\"/mnt/object_group/data/processed\")  # Adjust as needed\n",
    "\n",
    "def load_samples(limit_per_split=10):\n",
    "    diff_samples = []\n",
    "    for repo_dir in DATA_ROOT.iterdir():\n",
    "        if not repo_dir.is_dir():\n",
    "            continue\n",
    "        for pr_dir in (repo_dir / \"diff\").iterdir():\n",
    "            pr_id = pr_dir.name\n",
    "            diff_file = pr_dir\n",
    "            comments_file = DATA_ROOT / repo_dir.name / \"comments\" / f\"{repo_dir.name}_{pr_id}_comments.jsonl\"\n",
    "\n",
    "            if not diff_file.exists() or not comments_file.exists():\n",
    "                continue\n",
    "\n",
    "            with open(diff_file, \"r\", encoding=\"utf-8\") as df:\n",
    "                diff_content = df.read()\n",
    "\n",
    "            with open(comments_file, \"r\", encoding=\"utf-8\") as cf:\n",
    "                for line in cf:\n",
    "                    try:\n",
    "                        comment = json.loads(line)\n",
    "                        offset = comment.get(\"original_position\")\n",
    "                        side = comment.get(\"side\", \"RIGHT\")\n",
    "                        body = comment.get(\"body\", \"\").strip()\n",
    "                        if offset is not None and body:\n",
    "                            sample = {\n",
    "                                \"input\": f\"<DIFF>\\n{diff_content}\\n</DIFF>\\n<COMMENT side=\\\"{side}\\\" offset=\\\"{offset}\\\">\",\n",
    "                                \"output\": body\n",
    "                            }\n",
    "                            diff_samples.append(sample)\n",
    "                    except json.JSONDecodeError:\n",
    "                        continue\n",
    "\n",
    "    random.shuffle(diff_samples)\n",
    "    return {\n",
    "        \"train\": diff_samples[:limit_per_split],\n",
    "        \"test\": diff_samples[limit_per_split:2*limit_per_split],\n",
    "        \"eval\": diff_samples[2*limit_per_split:3*limit_per_split],\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e0fad3e5-a39c-42b2-af16-f18d85946d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_jsonl_gz(file_path, limit):\n",
    "    samples = []\n",
    "    with gzip.open(file_path, 'rt', encoding='utf-8') as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if i >= limit:\n",
    "                break\n",
    "            entry = json.loads(line)\n",
    "            prompt = f\"<DIFF>\\n{entry['diff']}\\n</DIFF>\\n\"\n",
    "            comment = f\"<COMMENT side=\\\"{entry['side']}\\\" offset=\\\"{entry['line_offset']}\\\">{entry['comment_body']}\"\n",
    "            samples.append({'input': prompt, 'label': comment})\n",
    "    return samples\n",
    "\n",
    "train_data = load_jsonl_gz(\"/mnt/object_group/data/processed/train.jsonl.gz\", 10)\n",
    "test_data  = load_jsonl_gz(\"/mnt/object_group/data/processed/test.jsonl.gz\", 8)\n",
    "eval_data  = load_jsonl_gz(\"/mnt/object_group/data/processed/val.jsonl.gz\", 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f2ddda06-1c1e-4d44-a915-d6253ea8bb84",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "aac47a0d-db7f-40bc-84ce-d02d66eb7617",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd5f12c430b64937874740ccba8791f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b01aff039c54f4ebcff5df8a8742b88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d898f78dcf04091ba13758e359f6074",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/DeepSeek-Coder-V2-Lite-Base\")\n",
    "def tokenize(example):\n",
    "    input = tokenizer(example['input'], truncation=True, padding='max_length', max_length=512)\n",
    "    label = tokenizer(example['label'], truncation=True, padding='max_length', max_length=128)\n",
    "    input['labels'] = label['input_ids']\n",
    "    return input\n",
    "\n",
    "train_ds = Dataset.from_list(train_data).map(tokenize)\n",
    "test_ds  = Dataset.from_list(test_data).map(tokenize)\n",
    "eval_ds  = Dataset.from_list(eval_data).map(tokenize)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b2791263-9c1b-4351-b9bb-5b41ac39ccd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5c794df7-b26e-468e-a00c-fabdb955cce3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e027145eaddb4b3e9e44f4275a13f8f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 800.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 4.75 MiB is free. Process 11661 has 79.24 GiB memory in use. Of the allocated memory 78.78 GiB is allocated by PyTorch, and 55.91 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[54], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m base_model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdeepseek-ai/DeepSeek-Coder-V2-Lite-Base\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m lora_cfg \u001b[38;5;241m=\u001b[39m LoraConfig(\n\u001b[1;32m      3\u001b[0m     task_type\u001b[38;5;241m=\u001b[39mTaskType\u001b[38;5;241m.\u001b[39mCAUSAL_LM,\n\u001b[1;32m      4\u001b[0m     r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      8\u001b[0m     target_modules\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mq_proj\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mv_proj\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mk_proj\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mo_proj\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgate_proj\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mup_proj\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdown_proj\u001b[39m\u001b[38;5;124m\"\u001b[39m]  \u001b[38;5;66;03m# Add this\u001b[39;00m\n\u001b[1;32m      9\u001b[0m )\n\u001b[1;32m     10\u001b[0m model \u001b[38;5;241m=\u001b[39m get_peft_model(base_model, lora_cfg)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/transformers/modeling_utils.py:3698\u001b[0m, in \u001b[0;36mPreTrainedModel.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3693\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_present_in_args:\n\u001b[1;32m   3694\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   3695\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou cannot cast a GPTQ model in a new `dtype`. Make sure to load the model using `from_pretrained` using the desired\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3696\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m `dtype` by passing the correct `torch_dtype` argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3697\u001b[0m         )\n\u001b[0;32m-> 3698\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/torch/nn/modules/module.py:1340\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1337\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1338\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m-> 1340\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/torch/nn/modules/module.py:900\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    898\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    899\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 900\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    902\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    903\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    904\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    905\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    910\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    911\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/torch/nn/modules/module.py:900\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    898\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    899\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 900\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    902\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    903\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    904\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    905\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    910\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    911\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/torch/nn/modules/module.py:927\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    923\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    924\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    925\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    926\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 927\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    928\u001b[0m p_should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    930\u001b[0m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/torch/nn/modules/module.py:1326\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1320\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(\n\u001b[1;32m   1321\u001b[0m             device,\n\u001b[1;32m   1322\u001b[0m             dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1323\u001b[0m             non_blocking,\n\u001b[1;32m   1324\u001b[0m             memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format,\n\u001b[1;32m   1325\u001b[0m         )\n\u001b[0;32m-> 1326\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1327\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1328\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1329\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1330\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1331\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1332\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot copy out of meta tensor; no data!\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 800.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 4.75 MiB is free. Process 11661 has 79.24 GiB memory in use. Of the allocated memory 78.78 GiB is allocated by PyTorch, and 55.91 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "base_model = AutoModelForCausalLM.from_pretrained(\"deepseek-ai/DeepSeek-Coder-V2-Lite-Base\", trust_remote_code = True).to(\"cuda\")\n",
    "lora_cfg = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]  # Add this\n",
    ")\n",
    "model = get_peft_model(base_model, lora_cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ba1eb9-8e70-4c05-a707-4ca3b993c5e9",
   "metadata": {},
   "source": [
    "We get CUDA Out of Memory error even to get te pef model, hence do optimizations to reduce the size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce1eaec6-225f-4327-925f-39cb46e7f3b9",
   "metadata": {},
   "source": [
    "### ONNX Optimizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "771d7f8a-bad8-49b9-aa51-a3fe5ad1e6d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "457f3e12-bc39-466d-b729-f12af218c470",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc715ae1ba604980ab535e736c3c10bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_arch = \"deepseek-ai/DeepSeek-Coder-V2-Lite-Base\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_arch, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "a8f20285-cbc8-4cc3-95c3-fd4bd00bb73a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeepseekV2ForCausalLM(\n",
       "  (model): DeepseekV2Model(\n",
       "    (embed_tokens): Embedding(102400, 2048)\n",
       "    (layers): ModuleList(\n",
       "      (0): DeepseekV2DecoderLayer(\n",
       "        (self_attn): DeepseekV2Attention(\n",
       "          (q_proj): Linear(in_features=2048, out_features=3072, bias=False)\n",
       "          (kv_a_proj_with_mqa): Linear(in_features=2048, out_features=576, bias=False)\n",
       "          (kv_a_layernorm): DeepseekV2RMSNorm()\n",
       "          (kv_b_proj): Linear(in_features=512, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (rotary_emb): DeepseekV2YarnRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): DeepseekV2MLP(\n",
       "          (gate_proj): Linear(in_features=2048, out_features=10944, bias=False)\n",
       "          (up_proj): Linear(in_features=2048, out_features=10944, bias=False)\n",
       "          (down_proj): Linear(in_features=10944, out_features=2048, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): DeepseekV2RMSNorm()\n",
       "        (post_attention_layernorm): DeepseekV2RMSNorm()\n",
       "      )\n",
       "      (1-26): 26 x DeepseekV2DecoderLayer(\n",
       "        (self_attn): DeepseekV2Attention(\n",
       "          (q_proj): Linear(in_features=2048, out_features=3072, bias=False)\n",
       "          (kv_a_proj_with_mqa): Linear(in_features=2048, out_features=576, bias=False)\n",
       "          (kv_a_layernorm): DeepseekV2RMSNorm()\n",
       "          (kv_b_proj): Linear(in_features=512, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (rotary_emb): DeepseekV2YarnRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): DeepseekV2MoE(\n",
       "          (experts): ModuleList(\n",
       "            (0-63): 64 x DeepseekV2MLP(\n",
       "              (gate_proj): Linear(in_features=2048, out_features=1408, bias=False)\n",
       "              (up_proj): Linear(in_features=2048, out_features=1408, bias=False)\n",
       "              (down_proj): Linear(in_features=1408, out_features=2048, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "          )\n",
       "          (gate): MoEGate()\n",
       "          (shared_experts): DeepseekV2MLP(\n",
       "            (gate_proj): Linear(in_features=2048, out_features=2816, bias=False)\n",
       "            (up_proj): Linear(in_features=2048, out_features=2816, bias=False)\n",
       "            (down_proj): Linear(in_features=2816, out_features=2048, bias=False)\n",
       "            (act_fn): SiLU()\n",
       "          )\n",
       "        )\n",
       "        (input_layernorm): DeepseekV2RMSNorm()\n",
       "        (post_attention_layernorm): DeepseekV2RMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): DeepseekV2RMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=102400, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_dict = torch.load(model_path, map_location=device, weights_only=False)\n",
    "model.load_state_dict(state_dict)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1afbca1-17e4-4346-8740-b411687589f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "f3026405-513b-4126-838c-9fa4dd82ce4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_model_path = \"models/finetunedDeepseekcoV2.onnx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "ebbfe0a8-2882-49ba-a4f1-2dae1712792b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token count: 136\n",
      "<DIFF>\n",
      "@@ -28,8 +28,9 @@ import {StaticAndDynamicReflectionCapabilities} from './static_reflection_capabi\n",
      " import {StaticReflector, StaticSymbol} from './static_reflector';\n",
      " \n",
      " function extract(\n",
      "-    ngOptions: tsc.AngularCompilerOptions, program: ts.Program, host: ts.CompilerHost) {\n",
      "-  const extractor = Extractor.create(ngOptions, program, host);\n",
      "+    ngOptions: tsc.AngularCompilerOptions, cliOptions: tsc.CliOptions, program: ts.Program,\n",
      "</DIFF>\n",
      "\n",
      "<COMMENT side=\"RIGHT\" offset=\"5\">only add the format\n",
      "\n",
      "ho\n",
      "{'input': \"<DIFF>\\n@@ -28,8 +28,9 @@ import {StaticAndDynamicReflectionCapabilities} from './static_reflection_capabi\\n import {StaticReflector, StaticSymbol} from './static_reflector';\\n \\n function extract(\\n-    ngOptions: tsc.AngularCompilerOptions, program: ts.Program, host: ts.CompilerHost) {\\n-  const extractor = Extractor.create(ngOptions, program, host);\\n+    ngOptions: tsc.AngularCompilerOptions, cliOptions: tsc.CliOptions, program: ts.Program,\\n</DIFF>\\n\", 'label': '<COMMENT side=\"RIGHT\" offset=\"5\">only add the format\\n'}\n"
     ]
    }
   ],
   "source": [
    "# Sample input token count\n",
    "idx = 9\n",
    "sample_input = train_data[idx]['input']\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/DeepSeek-Coder-V2-Lite-Base\", trust_remote_code=True)\n",
    "tokens = tokenizer(sample_input, return_tensors=\"pt\")\n",
    "print(\"Token count:\", tokens['input_ids'].shape[1])\n",
    "print(train_data[idx]['input'] + \"\\n\" + train_data[idx]['label'])\n",
    "# print(train_data[idx].keys())\n",
    "print(\"ho\")\n",
    "print(train_data[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745723f6-c7ca-4107-8f3a-0cd8f14627f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477a0257-d95b-45e8-94f7-9cd63a9ed4e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.12/site-packages/transformers/modeling_attn_mask_utils.py:88: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if input_shape[-1] > 1 or self.sliding_window is not None:\n",
      "/opt/conda/lib/python3.12/site-packages/transformers/modeling_attn_mask_utils.py:164: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if past_key_values_length > 0:\n",
      "/home/jovyan/.cache/huggingface/modules/transformers_modules/deepseek-ai/DeepSeek-Coder-V2-Lite-Base/ea9b066cee82f82906fdd58898cb3788b1c5d770/modeling_deepseek.py:875: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if attn_weights.size() != (bsz, self.num_heads, q_len, kv_seq_len):\n",
      "/home/jovyan/.cache/huggingface/modules/transformers_modules/deepseek-ai/DeepSeek-Coder-V2-Lite-Base/ea9b066cee82f82906fdd58898cb3788b1c5d770/modeling_deepseek.py:882: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if attention_mask.size() != (bsz, 1, q_len, kv_seq_len):\n",
      "/home/jovyan/.cache/huggingface/modules/transformers_modules/deepseek-ai/DeepSeek-Coder-V2-Lite-Base/ea9b066cee82f82906fdd58898cb3788b1c5d770/modeling_deepseek.py:897: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if attn_output.size() != (bsz, self.num_heads, q_len, self.v_head_dim):\n",
      "/home/jovyan/.cache/huggingface/modules/transformers_modules/deepseek-ai/DeepSeek-Coder-V2-Lite-Base/ea9b066cee82f82906fdd58898cb3788b1c5d770/modeling_deepseek.py:630: TracerWarning: Converting a tensor to a NumPy array might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  tokens_per_expert = tokens_per_expert.cpu().numpy()\n",
      "In-place op on output of tensor.shape. See https://pytorch.org/docs/main/onnx.html#avoid-inplace-operations-when-using-tensor-shape-in-tracing-mode\n",
      "In-place op on output of tensor.shape. See https://pytorch.org/docs/main/onnx.html#avoid-inplace-operations-when-using-tensor-shape-in-tracing-mode\n",
      "In-place op on output of tensor.shape. See https://pytorch.org/docs/main/onnx.html#avoid-inplace-operations-when-using-tensor-shape-in-tracing-mode\n",
      "In-place op on output of tensor.shape. See https://pytorch.org/docs/main/onnx.html#avoid-inplace-operations-when-using-tensor-shape-in-tracing-mode\n",
      "In-place op on output of tensor.shape. See https://pytorch.org/docs/main/onnx.html#avoid-inplace-operations-when-using-tensor-shape-in-tracing-mode\n",
      "In-place op on output of tensor.shape. See https://pytorch.org/docs/main/onnx.html#avoid-inplace-operations-when-using-tensor-shape-in-tracing-mode\n",
      "In-place op on output of tensor.shape. See https://pytorch.org/docs/main/onnx.html#avoid-inplace-operations-when-using-tensor-shape-in-tracing-mode\n",
      "In-place op on output of tensor.shape. See https://pytorch.org/docs/main/onnx.html#avoid-inplace-operations-when-using-tensor-shape-in-tracing-mode\n",
      "In-place op on output of tensor.shape. See https://pytorch.org/docs/main/onnx.html#avoid-inplace-operations-when-using-tensor-shape-in-tracing-mode\n",
      "In-place op on output of tensor.shape. See https://pytorch.org/docs/main/onnx.html#avoid-inplace-operations-when-using-tensor-shape-in-tracing-mode\n",
      "In-place op on output of tensor.shape. See https://pytorch.org/docs/main/onnx.html#avoid-inplace-operations-when-using-tensor-shape-in-tracing-mode\n",
      "In-place op on output of tensor.shape. See https://pytorch.org/docs/main/onnx.html#avoid-inplace-operations-when-using-tensor-shape-in-tracing-mode\n",
      "In-place op on output of tensor.shape. See https://pytorch.org/docs/main/onnx.html#avoid-inplace-operations-when-using-tensor-shape-in-tracing-mode\n",
      "In-place op on output of tensor.shape. See https://pytorch.org/docs/main/onnx.html#avoid-inplace-operations-when-using-tensor-shape-in-tracing-mode\n",
      "In-place op on output of tensor.shape. See https://pytorch.org/docs/main/onnx.html#avoid-inplace-operations-when-using-tensor-shape-in-tracing-mode\n",
      "In-place op on output of tensor.shape. See https://pytorch.org/docs/main/onnx.html#avoid-inplace-operations-when-using-tensor-shape-in-tracing-mode\n",
      "In-place op on output of tensor.shape. See https://pytorch.org/docs/main/onnx.html#avoid-inplace-operations-when-using-tensor-shape-in-tracing-mode\n",
      "In-place op on output of tensor.shape. See https://pytorch.org/docs/main/onnx.html#avoid-inplace-operations-when-using-tensor-shape-in-tracing-mode\n",
      "In-place op on output of tensor.shape. See https://pytorch.org/docs/main/onnx.html#avoid-inplace-operations-when-using-tensor-shape-in-tracing-mode\n",
      "In-place op on output of tensor.shape. See https://pytorch.org/docs/main/onnx.html#avoid-inplace-operations-when-using-tensor-shape-in-tracing-mode\n",
      "In-place op on output of tensor.shape. See https://pytorch.org/docs/main/onnx.html#avoid-inplace-operations-when-using-tensor-shape-in-tracing-mode\n",
      "In-place op on output of tensor.shape. See https://pytorch.org/docs/main/onnx.html#avoid-inplace-operations-when-using-tensor-shape-in-tracing-mode\n",
      "In-place op on output of tensor.shape. See https://pytorch.org/docs/main/onnx.html#avoid-inplace-operations-when-using-tensor-shape-in-tracing-mode\n",
      "In-place op on output of tensor.shape. See https://pytorch.org/docs/main/onnx.html#avoid-inplace-operations-when-using-tensor-shape-in-tracing-mode\n",
      "In-place op on output of tensor.shape. See https://pytorch.org/docs/main/onnx.html#avoid-inplace-operations-when-using-tensor-shape-in-tracing-mode\n",
      "In-place op on output of tensor.shape. See https://pytorch.org/docs/main/onnx.html#avoid-inplace-operations-when-using-tensor-shape-in-tracing-mode\n",
      "In-place op on output of tensor.shape. See https://pytorch.org/docs/main/onnx.html#avoid-inplace-operations-when-using-tensor-shape-in-tracing-mode\n"
     ]
    }
   ],
   "source": [
    "torch.onnx.export(model, tokens[\"input_ids\"], onnx_model_path,\n",
    "                  export_params=True, opset_version=20,\n",
    "                  do_constant_folding=True, input_names=['input'],\n",
    "                  output_names=['output'], dynamic_axes={\"input\": {0: \"batch_size\"}, \"output\": {0: \"batch_size\"}})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d14ef0-4058-4702-b475-8f45f817443d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"ONNX model saved to {onnx_model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6cc4a64-68ca-49ad-be7f-79fce49ed0dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_model = onnx.load(onnx_model_path)\n",
    "onnx.checker.check_model(onnx_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27616e08-6b73-41af-a7eb-b26d1f43dc3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2876cc-418b-4ddc-ab2b-f54156397bc4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "9b3c8dba-b84b-4c22-9d75-2dd9951ae21c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "614fbe51-a4da-4c6f-8b4e-a1ab19dc877f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import onnx\n",
    "import onnxruntime as ort\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "3aaaf8b8-98e1-404e-8e6c-9eb93455ce9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "device= torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710aa532-5e36-4e52-a499-640b89c5b5fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./deepseek-finetune\",\n",
    "    per_device_train_batch_size=2,\n",
    "    num_train_epochs=1,\n",
    "    logging_steps=5,\n",
    "    save_strategy=\"no\",\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "768711ad-dd1f-41fd-be5c-31794be473e2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'onnx_model_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[68], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m onnx_model_size \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mgetsize(\u001b[43monnx_model_path\u001b[49m) \n",
      "\u001b[0;31mNameError\u001b[0m: name 'onnx_model_path' is not defined"
     ]
    }
   ],
   "source": [
    "onnx_model_size = os.path.getsize(onnx_model_path) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a98398e-a83e-47ae-b086-5590b413dff8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
