{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fcc6e203-7aa8-4dee-82e6-28be0a766a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gzip, json, time\n",
    "import torch \n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from datasets import Dataset, load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForCausalLM, \n",
    "    TrainingArguments, \n",
    "    Trainer, \n",
    "    DataCollatorForLanguageModeling,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "from peft import get_peft_model, LoraConfig, TaskType, PeftModel, PeftConfig\n",
    "from bert_score import score\n",
    "import onnxruntime as ort\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import onnx\n",
    "import onnxruntime as ort\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "from bert_score import score as bert_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c2b65e45-c89f-4ec1-b7b7-30278aabe4db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1332"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81ba2b28-2e9a-4820-97f3-da7cc81706c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"codellama/CodeLlama-7b-Instruct-hf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99102de2-dcbf-4073-acfd-7bfc824d4508",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10455bebaba242959c8c3417a63c4583",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.59k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4db78fd723ec42c5bfa12b55c20e3763",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da49331a54b8402c80b4a7a8eb5bd599",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "751e4a826cc84036a3399aa6a89b3152",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/411 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21e23972ed9e46bfbb177a62653ed130",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/646 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e3435aa4ddf49409c982b75acc2f631",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/25.1k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4f56b548e924ba3b0e692602311a21e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fa4ca0a6af045e390dbcc83f50768eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/3.50G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d34196680904ec9888821c1388a475c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/9.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29009406619e495d910bd35fcb22667a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9540fa7073824d3c8bea9441d5a2de88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"codellama/CodeLlama-7b-Instruct-hf\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model = AutoModelForCausalLM.from_pretrained(\"codellama/CodeLlama-7b-Instruct-hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7259a2ba-f3b9-4713-b0f0-fac43d8e294b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(32016, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32016, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d01799a-d78c-44f4-8d25-484671ed776d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_size(model):\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    dtype_size = torch.finfo(model.dtype).bits // 8\n",
    "    \n",
    "    model_size_gb = total_params * dtype_size / (1024 ** 3)\n",
    "    # print(f\"Model size: {model_size_gb:2f} GB\")\n",
    "    return model_size_gb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a8390be8-f2f0-44d2-a514-31c9f5646a43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25.103042602539062"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_model_size(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c6ba147-ff75-4ae3-a934-0fed0478d870",
   "metadata": {},
   "source": [
    "### Floating Point 16 params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "121c0545-e9ff-4873-a42e-3bcabf34231f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1877c7977e9a49e9b19b9c2669d09520",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"codellama/CodeLlama-7b-Instruct-hf\",\n",
    "    torch_dtype=torch.float16,\n",
    "    # device_map={\"\": 0},\n",
    "    # low_cpu_mem_usage=True\n",
    "    device_map=\"cuda:0\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c5b9097e-bc5f-482c-b5f6-8ec29b6977e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12.551521301269531"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_model_size(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "77cc5210-f4f3-4842-a142-1a9ddb7f00ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun May 11 23:17:45 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 560.35.05              Driver Version: 560.35.05      CUDA Version: 12.6     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA A30                     Off |   00000000:99:00.0 Off |                    0 |\n",
      "| N/A   40C    P0             34W /  165W |   23321MiB /  24576MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d2f3cac4-bf69-4588-aca5-a97eb7d4a42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"/mnt/object_group/data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "90160599-5139-4efe-af50-0e539dfcd398",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    {\"comment_id\": 74291026, \"comment_user_login\": \"vikerman\", \"comment_body\": \"Is there a better way to avoid having this in different places?\\n\", \"comment_created_at\": \"2016-08-10T17:30:42+00:00\", \"comment_html_url\": \"https://github.com/angular/angular/pull/10620#discussion_r74291026\", \"comment_path\": \"modules/@angular/common/src/forms-deprecated/directives/abstract_control_directive.ts\", \"comment_position\": 10, \"comment_original_position\": 10, \"comment_commit_id\": \"ee8e802b7bc25eafbe54109b3924e9dbc36dce11\", \"comment_original_commit_id\": \"ee8e802b7bc25eafbe54109b3924e9dbc36dce11\", \"diff_line_content\": \"}\\n\", \"diff_line_type\": \"added\", \"diff_line_source_no\": null, \"diff_line_target_no\": 15, \"diff_hunk_header\": \"\", \"diff\": \"@@ -6,10 +6,13 @@\\n  * found in the LICENSE file at https://angular.io/license\\n  */\\n \\n-import {unimplemented} from '../../facade/exceptions';\\n+import {BaseException} from '@angular/core';\\n import {isPresent} from '../../facade/lang';\\n import {AbstractControl} from '../model';\\n \\n+function unimplemented(): any {\\n+  throw new BaseException('unimplemented');\", \"side\": \"RIGHT\", \"line_offset\": 9, \"diff_file_source\": \"a/modules/@angular/common/src/forms-deprecated/directives/abstract_control_directive.ts\", \"diff_file_target\": \"b/modules/@angular/common/src/forms-deprecated/directives/abstract_control_directive.ts\", \"repo\": \"angular/angular\"}\n",
      "    {\"comment_id\": 74305223, \"comment_user_login\": \"alexeagle\", \"comment_body\": \"we could export it in the public API but that's not nice.\\nOr we could inline the method and delete it.\\n\", \"comment_created_at\": \"2016-08-10T18:42:59+00:00\", \"comment_html_url\": \"https://github.com/angular/angular/pull/10620#discussion_r74305223\", \"comment_path\": \"modules/@angular/common/src/forms-deprecated/directives/abstract_control_directive.ts\", \"comment_position\": 10, \"comment_original_position\": 10, \"comment_commit_id\": \"ee8e802b7bc25eafbe54109b3924e9dbc36dce11\", \"comment_original_commit_id\": \"ee8e802b7bc25eafbe54109b3924e9dbc36dce11\", \"diff_line_content\": \"}\\n\", \"diff_line_type\": \"added\", \"diff_line_source_no\": null, \"diff_line_target_no\": 15, \"diff_hunk_header\": \"\", \"diff\": \"@@ -6,10 +6,13 @@\\n  * found in the LICENSE file at https://angular.io/license\\n  */\\n \\n-import {unimplemented} from '../../facade/exceptions';\\n+import {BaseException} from '@angular/core';\\n import {isPresent} from '../../facade/lang';\\n import {AbstractControl} from '../model';\\n \\n+function unimplemented(): any {\\n+  throw new BaseException('unimplemented');\", \"side\": \"RIGHT\", \"line_offset\": 9, \"diff_file_source\": \"a/modules/@angular/common/src/forms-deprecated/directives/abstract_control_directive.ts\", \"diff_file_target\": \"b/modules/@angular/common/src/forms-deprecated/directives/abstract_control_directive.ts\", \"repo\": \"angular/angular\"}\n",
      "    {\"comment_id\": 74523490, \"comment_user_login\": \"vicb\", \"comment_body\": \"const\\n\", \"comment_created_at\": \"2016-08-11T23:43:20+00:00\", \"comment_html_url\": \"https://github.com/angular/angular/pull/10622#discussion_r74523490\", \"comment_path\": \"modules/@angular/compiler-cli/integrationtest/test/basic_spec.ts\", \"comment_position\": 36, \"comment_original_position\": 36, \"comment_commit_id\": \"616c5c56924b77bc0ef539d01d1cf3596c0da87f\", \"comment_original_commit_id\": \"91f50b2f513f99cf56b7406b3a74066e128b9c7d\", \"diff_line_content\": \"    it('should support i18n for content tags', () => {\\n\", \"diff_line_type\": \"added\", \"diff_line_source_no\": null, \"diff_line_target_no\": 79, \"diff_hunk_header\": \"describe('template codegen output', () => {\", \"diff\": \"@@ -44,24 +44,40 @@ describe('template codegen output', () => {\\n   it('should support ngIf', () => {\\n     var compFixture = createComponent(BasicComp);\\n     var debugElement = compFixture.debugElement;\\n-    expect(debugElement.children.length).toBe(2);\\n+    expect(debugElement.children.length).toBe(3);\\n \\n     compFixture.componentInstance.ctxBool = true;\\n     compFixture.detectChanges();\\n-    expect(debugElement.children.length).toBe(3);\\n+    expect(debugElement.children.length).toBe(4);\\n     expect(debugElement.children[2].injector.get(MultipleComponentsMyComp)).toBeTruthy();\\n   });\\n \\n   it('should support ngFor', () => {\\n     var compFixture = createComponent(BasicComp);\\n     var debugElement = compFixture.debugElement;\\n-    expect(debugElement.children.length).toBe(2);\\n+    expect(debugElement.children.length).toBe(3);\\n \\n     // test NgFor\\n     compFixture.componentInstance.ctxArr = [1, 2];\\n     compFixture.detectChanges();\\n-    expect(debugElement.children.length).toBe(4);\\n+    expect(debugElement.children.length).toBe(5);\\n     expect(debugElement.children[2].attributes['value']).toBe('1');\\n     expect(debugElement.children[3].attributes['value']).toBe('2');\\n   });\\n+\\n+  describe('i18n', () => {\\n+    it('should support i18n for content tags', () => {\\n+      var compFixture = createComponent(BasicComp);\\n+      expect(compFixture.componentInstance.ctxLocale).toEqual('FI');\\n+    });\\n+\\n+    it('should support i18n for content tags', () => {\\n+      var compFixture = createComponent(BasicComp);\", \"side\": \"RIGHT\", \"line_offset\": 35, \"diff_file_source\": \"a/modules/@angular/compiler-cli/integrationtest/test/basic_spec.ts\", \"diff_file_target\": \"b/modules/@angular/compiler-cli/integrationtest/test/basic_spec.ts\", \"repo\": \"angular/angular\"}\n",
      "    {\"comment_id\": 74523504, \"comment_user_login\": \"vicb\", \"comment_body\": \"const\\n\", \"comment_created_at\": \"2016-08-11T23:43:27+00:00\", \"comment_html_url\": \"https://github.com/angular/angular/pull/10622#discussion_r74523504\", \"comment_path\": \"modules/@angular/compiler-cli/integrationtest/test/basic_spec.ts\", \"comment_position\": 31, \"comment_original_position\": 31, \"comment_commit_id\": \"616c5c56924b77bc0ef539d01d1cf3596c0da87f\", \"comment_original_commit_id\": \"91f50b2f513f99cf56b7406b3a74066e128b9c7d\", \"diff_line_content\": \"    it('should inject the translations format into the component', () => {\\n\", \"diff_line_type\": \"added\", \"diff_line_source_no\": null, \"diff_line_target_no\": 74, \"diff_hunk_header\": \"describe('template codegen output', () => {\", \"diff\": \"@@ -44,24 +44,40 @@ describe('template codegen output', () => {\\n   it('should support ngIf', () => {\\n     var compFixture = createComponent(BasicComp);\\n     var debugElement = compFixture.debugElement;\\n-    expect(debugElement.children.length).toBe(2);\\n+    expect(debugElement.children.length).toBe(3);\\n \\n     compFixture.componentInstance.ctxBool = true;\\n     compFixture.detectChanges();\\n-    expect(debugElement.children.length).toBe(3);\\n+    expect(debugElement.children.length).toBe(4);\\n     expect(debugElement.children[2].injector.get(MultipleComponentsMyComp)).toBeTruthy();\\n   });\\n \\n   it('should support ngFor', () => {\\n     var compFixture = createComponent(BasicComp);\\n     var debugElement = compFixture.debugElement;\\n-    expect(debugElement.children.length).toBe(2);\\n+    expect(debugElement.children.length).toBe(3);\\n \\n     // test NgFor\\n     compFixture.componentInstance.ctxArr = [1, 2];\\n     compFixture.detectChanges();\\n-    expect(debugElement.children.length).toBe(4);\\n+    expect(debugElement.children.length).toBe(5);\\n     expect(debugElement.children[2].attributes['value']).toBe('1');\\n     expect(debugElement.children[3].attributes['value']).toBe('2');\\n   });\\n+\\n+  describe('i18n', () => {\\n+    it('should support i18n for content tags', () => {\\n+      var compFixture = createComponent(BasicComp);\", \"side\": \"RIGHT\", \"line_offset\": 30, \"diff_file_source\": \"a/modules/@angular/compiler-cli/integrationtest/test/basic_spec.ts\", \"diff_file_target\": \"b/modules/@angular/compiler-cli/integrationtest/test/basic_spec.ts\", \"repo\": \"angular/angular\"}\n",
      "    {\"comment_id\": 74526501, \"comment_user_login\": \"vicb\", \"comment_body\": \"fi\\n\", \"comment_created_at\": \"2016-08-12T00:18:35+00:00\", \"comment_html_url\": \"https://github.com/angular/angular/pull/10622#discussion_r74526501\", \"comment_path\": \"modules/@angular/compiler-cli/integrationtest/test/basic_spec.ts\", \"comment_position\": 32, \"comment_original_position\": 32, \"comment_commit_id\": \"616c5c56924b77bc0ef539d01d1cf3596c0da87f\", \"comment_original_commit_id\": \"7333b4c6d9527a1a6b812ab6eea2797f228d03bd\", \"diff_line_content\": \"      const compFixture = createComponent(BasicComp);\\n\", \"diff_line_type\": \"added\", \"diff_line_source_no\": null, \"diff_line_target_no\": 75, \"diff_hunk_header\": \"describe('template codegen output', () => {\", \"diff\": \"@@ -44,24 +44,45 @@ describe('template codegen output', () => {\\n   it('should support ngIf', () => {\\n     var compFixture = createComponent(BasicComp);\\n     var debugElement = compFixture.debugElement;\\n-    expect(debugElement.children.length).toBe(2);\\n+    expect(debugElement.children.length).toBe(3);\\n \\n     compFixture.componentInstance.ctxBool = true;\\n     compFixture.detectChanges();\\n-    expect(debugElement.children.length).toBe(3);\\n+    expect(debugElement.children.length).toBe(4);\\n     expect(debugElement.children[2].injector.get(MultipleComponentsMyComp)).toBeTruthy();\\n   });\\n \\n   it('should support ngFor', () => {\\n     var compFixture = createComponent(BasicComp);\\n     var debugElement = compFixture.debugElement;\\n-    expect(debugElement.children.length).toBe(2);\\n+    expect(debugElement.children.length).toBe(3);\\n \\n     // test NgFor\\n     compFixture.componentInstance.ctxArr = [1, 2];\\n     compFixture.detectChanges();\\n-    expect(debugElement.children.length).toBe(4);\\n+    expect(debugElement.children.length).toBe(5);\\n     expect(debugElement.children[2].attributes['value']).toBe('1');\\n     expect(debugElement.children[3].attributes['value']).toBe('2');\\n   });\\n+\\n+  describe('i18n', () => {\\n+    it('should inject the locale into the component', () => {\\n+      const compFixture = createComponent(BasicComp);\\n+      expect(compFixture.componentInstance.ctxLocale).toEqual('FI');\", \"side\": \"RIGHT\", \"line_offset\": 31, \"diff_file_source\": \"a/modules/@angular/compiler-cli/integrationtest/test/basic_spec.ts\", \"diff_file_target\": \"b/modules/@angular/compiler-cli/integrationtest/test/basic_spec.ts\", \"repo\": \"angular/angular\"}\n",
      "    {\"comment_id\": 74526531, \"comment_user_login\": \"vicb\", \"comment_body\": \"remove !\\n\", \"comment_created_at\": \"2016-08-12T00:19:06+00:00\", \"comment_html_url\": \"https://github.com/angular/angular/pull/10622#discussion_r74526531\", \"comment_path\": \"modules/@angular/compiler-cli/src/codegen.ts\", \"comment_position\": 36, \"comment_original_position\": 36, \"comment_commit_id\": \"616c5c56924b77bc0ef539d01d1cf3596c0da87f\", \"comment_original_commit_id\": \"7333b4c6d9527a1a6b812ab6eea2797f228d03bd\", \"diff_line_content\": \"    const reflectorHost = new ReflectorHost(program, compilerHost, options, reflectorHostContext);\\n\", \"diff_line_type\": \"context\", \"diff_line_source_no\": 132, \"diff_line_target_no\": 142, \"diff_hunk_header\": \"export class CodeGenerator {\", \"diff\": \"@@ -88,6 +93,35 @@ export class CodeGenerator {\\n     return path.join(this.options.genDir, relativePath);\\n   }\\n \\n+  private _constructTranslationBundle(fileMetas: any[], analyzedNgModules: any): Promise<any> {\", \"side\": \"RIGHT\", \"line_offset\": 11, \"diff_file_source\": \"a/modules/@angular/compiler-cli/src/codegen.ts\", \"diff_file_target\": \"b/modules/@angular/compiler-cli/src/codegen.ts\", \"repo\": \"angular/angular\"}\n",
      "    {\"comment_id\": 74526731, \"comment_user_login\": \"vicb\", \"comment_body\": \"append at the end to make diff readable\\n\", \"comment_created_at\": \"2016-08-12T00:21:49+00:00\", \"comment_html_url\": \"https://github.com/angular/angular/pull/10622#discussion_r74526731\", \"comment_path\": \"modules/@angular/compiler-cli/src/codegen.ts\", \"comment_position\": 26, \"comment_original_position\": 26, \"comment_commit_id\": \"616c5c56924b77bc0ef539d01d1cf3596c0da87f\", \"comment_original_commit_id\": \"7333b4c6d9527a1a6b812ab6eea2797f228d03bd\", \"diff_line_content\": \"      }\\n\", \"diff_line_type\": \"context\", \"diff_line_source_no\": 129, \"diff_line_target_no\": 132, \"diff_hunk_header\": \"export class CodeGenerator {\", \"diff\": \"@@ -32,7 +35,9 @@ const PREAMBLE = `/**\\n \\n export class CodeGenerator {\\n   constructor(\\n-      private options: AngularCompilerOptions, private program: ts.Program,\\n+      private options: AngularCompilerOptions, private transContent: string,\", \"side\": \"RIGHT\", \"line_offset\": 1, \"diff_file_source\": \"a/modules/@angular/compiler-cli/src/codegen.ts\", \"diff_file_target\": \"b/modules/@angular/compiler-cli/src/codegen.ts\", \"repo\": \"angular/angular\"}\n",
      "    {\"comment_id\": 74527376, \"comment_user_login\": \"vicb\", \"comment_body\": \"This is the only line that you should keep beside loading the file\\n\", \"comment_created_at\": \"2016-08-12T00:30:39+00:00\", \"comment_html_url\": \"https://github.com/angular/angular/pull/10622#discussion_r74527376\", \"comment_path\": \"modules/@angular/compiler-cli/src/codegen.ts\", \"comment_position\": 46, \"comment_original_position\": 130, \"comment_commit_id\": \"616c5c56924b77bc0ef539d01d1cf3596c0da87f\", \"comment_original_commit_id\": \"7333b4c6d9527a1a6b812ab6eea2797f228d03bd\", \"diff_line_content\": \"        new NgModuleCompiler(), new TypeScriptEmitter(reflectorHost), cliOptions.locale,\\n\", \"diff_line_type\": \"added\", \"diff_line_source_no\": null, \"diff_line_target_no\": 164, \"diff_hunk_header\": \"export class CodeGenerator {\", \"diff\": \"@@ -132,7 +175,7 @@ export class CodeGenerator {\\n     const reflectorHost = new ReflectorHost(program, compilerHost, options, reflectorHostContext);\\n     const staticReflector = new StaticReflector(reflectorHost);\\n     StaticAndDynamicReflectionCapabilities.install(staticReflector);\\n-    const htmlParser = new compiler.i18n.HtmlParser(new HtmlParser());\\n+    const htmlParser = new compiler.i18n.HtmlParser(new HtmlParser(), transContent);\", \"side\": \"RIGHT\", \"line_offset\": 3, \"diff_file_source\": \"a/modules/@angular/compiler-cli/src/codegen.ts\", \"diff_file_target\": \"b/modules/@angular/compiler-cli/src/codegen.ts\", \"repo\": \"angular/angular\"}\n",
      "    {\"comment_id\": 74528827, \"comment_user_login\": \"vicb\", \"comment_body\": \"Id\\n\", \"comment_created_at\": \"2016-08-12T00:51:47+00:00\", \"comment_html_url\": \"https://github.com/angular/angular/pull/10622#discussion_r74528827\", \"comment_path\": \"modules/@angular/compiler-cli/integrationtest/src/basic.ts\", \"comment_position\": 15, \"comment_original_position\": 15, \"comment_commit_id\": \"616c5c56924b77bc0ef539d01d1cf3596c0da87f\", \"comment_original_commit_id\": \"e7940fc0e83c9d0a549891774db0a609ec591c19\", \"diff_line_content\": \"  }\\n\", \"diff_line_type\": \"added\", \"diff_line_source_no\": null, \"diff_line_target_no\": 30, \"diff_hunk_header\": \"export class BasicComp {\", \"diff\": \"@@ -23,5 +23,9 @@ export class BasicComp {\\n   ctxProp: string;\\n   ctxBool: boolean;\\n   ctxArr: any[] = [];\\n-  constructor() { this.ctxProp = 'initialValue'; }\\n+  constructor(\\n+      @Inject(LOCALE_ID) public localeID: string,\", \"side\": \"RIGHT\", \"line_offset\": 7, \"diff_file_source\": \"a/modules/@angular/compiler-cli/integrationtest/src/basic.ts\", \"diff_file_target\": \"b/modules/@angular/compiler-cli/integrationtest/src/basic.ts\", \"repo\": \"angular/angular\"}\n",
      "    {\"comment_id\": 74528853, \"comment_user_login\": \"vicb\", \"comment_body\": \"only add the format\\n\", \"comment_created_at\": \"2016-08-12T00:52:10+00:00\", \"comment_html_url\": \"https://github.com/angular/angular/pull/10622#discussion_r74528853\", \"comment_path\": \"modules/@angular/compiler-cli/src/extract_i18n.ts\", \"comment_position\": 6, \"comment_original_position\": 6, \"comment_commit_id\": \"616c5c56924b77bc0ef539d01d1cf3596c0da87f\", \"comment_original_commit_id\": \"e7940fc0e83c9d0a549891774db0a609ec591c19\", \"diff_line_content\": \"  const extractor = Extractor.create(ngOptions, cliOptions.i18nFormat, program, host);\\n\", \"diff_line_type\": \"added\", \"diff_line_source_no\": null, \"diff_line_target_no\": 33, \"diff_hunk_header\": \"import {StaticAndDynamicReflectionCapabilities} from './static_reflection_capabi\", \"diff\": \"@@ -28,8 +28,9 @@ import {StaticAndDynamicReflectionCapabilities} from './static_reflection_capabi\\n import {StaticReflector, StaticSymbol} from './static_reflector';\\n \\n function extract(\\n-    ngOptions: tsc.AngularCompilerOptions, program: ts.Program, host: ts.CompilerHost) {\\n-  const extractor = Extractor.create(ngOptions, program, host);\\n+    ngOptions: tsc.AngularCompilerOptions, cliOptions: tsc.CliOptions, program: ts.Program,\", \"side\": \"RIGHT\", \"line_offset\": 5, \"diff_file_source\": \"a/modules/@angular/compiler-cli/src/extract_i18n.ts\", \"diff_file_target\": \"b/modules/@angular/compiler-cli/src/extract_i18n.ts\", \"repo\": \"angular/angular\"}\n",
      "    {\"comment_id\": 74530724, \"comment_user_login\": \"vicb\", \"comment_body\": \"`, null, null` would be more semantically correct\\n\", \"comment_created_at\": \"2016-08-12T01:22:30+00:00\", \"comment_html_url\": \"https://github.com/angular/angular/pull/10622#discussion_r74530724\", \"comment_path\": \"modules/@angular/compiler-cli/src/extract_i18n.ts\", \"comment_position\": 37, \"comment_original_position\": 37, \"comment_commit_id\": \"616c5c56924b77bc0ef539d01d1cf3596c0da87f\", \"comment_original_commit_id\": \"6781377043d6eae20d6504b83a121ebde353f043\", \"diff_line_content\": \"}\\n\", \"diff_line_type\": \"added\", \"diff_line_source_no\": null, \"diff_line_target_no\": 196, \"diff_hunk_header\": \"interface FileMetadata {\", \"diff\": \"@@ -163,7 +164,7 @@ export class Extractor {\\n         config, console, elementSchemaRegistry, staticReflector);\\n     const offlineCompiler = new compiler.OfflineCompiler(\\n         resolver, normalizer, tmplParser, new StyleCompiler(urlResolver), new ViewCompiler(config),\\n-        new NgModuleCompiler(), new TypeScriptEmitter(reflectorHost));\\n+        new NgModuleCompiler(), new TypeScriptEmitter(reflectorHost), '', translationsFormat);\", \"side\": \"RIGHT\", \"line_offset\": 12, \"diff_file_source\": \"a/modules/@angular/compiler-cli/src/extract_i18n.ts\", \"diff_file_target\": \"b/modules/@angular/compiler-cli/src/extract_i18n.ts\", \"repo\": \"angular/angular\"}\n"
     ]
    }
   ],
   "source": [
    "train = \"/mnt/object_group/data/processed/train.jsonl.gz\"\n",
    "with gzip.open(train, 'rt', encoding='utf-8') as f:\n",
    "    for i, line in enumerate(f):\n",
    "        print(\"    \" + line.strip())\n",
    "        if i >= 5:\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "86df7928-62f7-4ac6-8f93-e08ce6a4b75a",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_ROOT = Path(\"/mnt/object_group/data/processed\")  # Adjust as needed\n",
    "\n",
    "def load_samples(limit_per_split=10):\n",
    "    diff_samples = []\n",
    "    for repo_dir in DATDATA_ROOT = Path(\"/mnt/object_group/data/processed\")  # Adjust as needed\n",
    "\n",
    "def load_samples(limit_per_split=10):\n",
    "    diff_samples = []\n",
    "    for repo_dir in DATA_ROOT.iterdir():\n",
    "        if not repo_dir.is_dir():\n",
    "            continue\n",
    "        for pr_dir in (repo_dir / \"diff\").iterdir():\n",
    "            pr_id = pr_dir.name\n",
    "            diff_file = pr_dir\n",
    "            comments_file = DATA_ROOT / repo_dir.name / \"comments\" / f\"{repo_dir.name}_{pr_id}_comments.jsonl\"\n",
    "\n",
    "            if not diff_file.exists() or not comments_file.exists():\n",
    "                continue\n",
    "\n",
    "            with open(diff_file, \"r\", encoding=\"utf-8\") as df:\n",
    "                diff_content = df.read()\n",
    "\n",
    "            with open(comments_file, \"r\", encoding=\"utf-8\") as cf:\n",
    "                for line in cf:\n",
    "                    try:\n",
    "                        comment = json.loads(line)\n",
    "                        offset = comment.get(\"original_position\")\n",
    "                        side = comment.get(\"side\", \"RIGHT\")\n",
    "                        body = comment.get(\"body\", \"\").strip()\n",
    "                        if offset is not None and body:\n",
    "                            sample = {\n",
    "                                \"input\": f\"<DIFF>\\n{diff_content}\\n</DIFF>\\n<COMMENT side=\\\"{side}\\\" offset=\\\"{offset}\\\">\",\n",
    "                                \"output\": body\n",
    "                            }\n",
    "                            diff_samples.append(sample)\n",
    "                    except json.JSONDecodeError:\n",
    "                        continue\n",
    "\n",
    "    random.shuffle(diff_samples)\n",
    "    return {\n",
    "        \"train\": diff_samples[:limit_per_split],\n",
    "        \"test\": diff_samples[limit_per_split:2*limit_per_split],\n",
    "        \"eval\": diff_samples[2*limit_per_split:3*limit_per_split],\n",
    "    }\n",
    "A_ROOT.iterdir():\n",
    "        if not repo_dir.is_dir():\n",
    "            continue\n",
    "        for pr_dir in (repo_dir / \"diff\").iterdir():\n",
    "            pr_id = pr_dir.name\n",
    "            diff_file = pr_dir\n",
    "            comments_file = DATA_ROOT / repo_dir.name / \"comments\" / f\"{repo_dir.name}_{pr_id}_comments.jsonl\"\n",
    "\n",
    "            if not diff_file.exists() or not comments_file.exists():\n",
    "                continue\n",
    "\n",
    "            with open(diff_file, \"r\", encoding=\"utf-8\") as df:\n",
    "                diff_content = df.read()\n",
    "\n",
    "            with open(comments_file, \"r\", encoding=\"utf-8\") as cf:\n",
    "                for line in cf:\n",
    "                    try:\n",
    "                        comment = json.loads(line)\n",
    "                        offset = comment.get(\"original_position\")\n",
    "                        side = comment.get(\"side\", \"RIGHT\")\n",
    "                        body = comment.get(\"body\", \"\").strip()\n",
    "                        if offset is not None and body:\n",
    "                            sample = {\n",
    "                                \"input\": f\"<DIFF>\\n{diff_content}\\n</DIFF>\\n<COMMENT side=\\\"{side}\\\" offset=\\\"{offset}\\\">\",\n",
    "                                \"output\": body\n",
    "                            }\n",
    "                            diff_samples.append(sample)\n",
    "                    except json.JSONDecodeError:\n",
    "                        continue\n",
    "\n",
    "    random.shuffle(diff_samples)\n",
    "    return {\n",
    "        \"train\": diff_samples[:limit_per_split],\n",
    "        \"test\": diff_samples[limit_per_split:2*limit_per_split],\n",
    "        \"eval\": diff_samples[2*limit_per_split:3*limit_per_split],\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c70b5a55-a831-4432-ae15-5b910735780f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_jsonl_gz(file_path, limit):\n",
    "    samples = []\n",
    "    with gzip.open(file_path, 'rt', encoding='utf-8') as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if i >= limit:\n",
    "                break\n",
    "            entry = json.loads(line)\n",
    "            prompt = f\"<DIFF>\\n{entry['diff']}\\n</DIFF>\\n\"\n",
    "            comment = f\"<COMMENT side=\\\"{entry['side']}\\\" offset=\\\"{entry['line_offset']}\\\">{entry['comment_body']}\"\n",
    "            samples.append({'input': prompt, 'label': comment})\n",
    "    return samples\n",
    "\n",
    "train_data = load_jsonl_gz(\"/mnt/object_group/data/processed/train.jsonl.gz\", 10)\n",
    "test_data  = load_jsonl_gz(\"/mnt/object_group/data/processed/test.jsonl.gz\", 8)\n",
    "eval_data  = load_jsonl_gz(\"/mnt/object_group/data/processed/val.jsonl.gz\", 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "84412e24-a531-49f4-ba95-4610c862f433",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./models/llama_onnx/tokenizer_config.json',\n",
       " './models/llama_onnx/special_tokens_map.json',\n",
       " './models/llama_onnx/tokenizer.json')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained(\"./models/llama_onnx\")\n",
    "# tokenizer.save_pretrained(\"mnt/object/llama_onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a0fb7e8b-f409-4557-b4e6-61fafc226c9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input', 'label'])\n",
      "<DIFF>\n",
      "@@ -6,10 +6,13 @@\n",
      "  * found in the LICENSE file at https://angular.io/license\n",
      "  */\n",
      " \n",
      "-import {unimplemented} from '../../facade/exceptions';\n",
      "+import {BaseException} from '@angular/core';\n",
      " import {isPresent} from '../../facade/lang';\n",
      " import {AbstractControl} from '../model';\n",
      " \n",
      "+function unimplemented(): any {\n",
      "+  throw new BaseException('unimplemented');\n",
      "</DIFF>\n",
      "\n",
      "<COMMENT side=\"RIGHT\" offset=\"9\">Is there a better way to avoid having this in different places?\n",
      "\n",
      "<DIFF>\n",
      "@@ -6,10 +6,13 @@\n",
      "  * found in the LICENSE file at https://angular.io/license\n",
      "  */\n",
      " \n",
      "-import {unimplemented} from '../../facade/exceptions';\n",
      "+import {BaseException} from '@angular/core';\n",
      " import {isPresent} from '../../facade/lang';\n",
      " import {AbstractControl} from '../model';\n",
      " \n",
      "+function unimplemented(): any {\n",
      "+  throw new BaseException('unimplemented');\n",
      "</DIFF>\n",
      "\n",
      "<COMMENT side=\"RIGHT\" offset=\"9\">we could export it in the public API but that's not nice.\n",
      "Or we could inline the method and delete it.\n",
      "\n",
      "<DIFF>\n",
      "@@ -44,24 +44,40 @@ describe('template codegen output', () => {\n",
      "   it('should support ngIf', () => {\n",
      "     var compFixture = createComponent(BasicComp);\n",
      "     var debugElement = compFixture.debugElement;\n",
      "-    expect(debugElement.children.length).toBe(2);\n",
      "+    expect(debugElement.children.length).toBe(3);\n",
      " \n",
      "     compFixture.componentInstance.ctxBool = true;\n",
      "     compFixture.detectChanges();\n",
      "-    expect(debugElement.children.length).toBe(3);\n",
      "+    expect(debugElement.children.length).toBe(4);\n",
      "     expect(debugElement.children[2].injector.get(MultipleComponentsMyComp)).toBeTruthy();\n",
      "   });\n",
      " \n",
      "   it('should support ngFor', () => {\n",
      "     var compFixture = createComponent(BasicComp);\n",
      "     var debugElement = compFixture.debugElement;\n",
      "-    expect(debugElement.children.length).toBe(2);\n",
      "+    expect(debugElement.children.length).toBe(3);\n",
      " \n",
      "     // test NgFor\n",
      "     compFixture.componentInstance.ctxArr = [1, 2];\n",
      "     compFixture.detectChanges();\n",
      "-    expect(debugElement.children.length).toBe(4);\n",
      "+    expect(debugElement.children.length).toBe(5);\n",
      "     expect(debugElement.children[2].attributes['value']).toBe('1');\n",
      "     expect(debugElement.children[3].attributes['value']).toBe('2');\n",
      "   });\n",
      "+\n",
      "+  describe('i18n', () => {\n",
      "+    it('should support i18n for content tags', () => {\n",
      "+      var compFixture = createComponent(BasicComp);\n",
      "+      expect(compFixture.componentInstance.ctxLocale).toEqual('FI');\n",
      "+    });\n",
      "+\n",
      "+    it('should support i18n for content tags', () => {\n",
      "+      var compFixture = createComponent(BasicComp);\n",
      "</DIFF>\n",
      "\n",
      "<COMMENT side=\"RIGHT\" offset=\"35\">const\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(train_data[0].keys())\n",
    "\n",
    "for i in range(3):\n",
    "    print(train_data[i][\"input\"])\n",
    "    print(train_data[i][\"label\"])\n",
    "# print(train_data[0][\"input\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4009389d-1604-4d45-8b29-356c1eb50ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt template\n",
    "instruction = (\n",
    "    \"You are a helpful code reviewer. \"\n",
    "    \"Given a code diff, generate all relevant review comments. \"\n",
    "    \"Each comment must be in the format: \"\n",
    "    \"<COMMENT side=\\\"RIGHT\\\" offset=\\\"X\\\">Your comment here.\\n\"\n",
    "    \"Only output comments, nothing else.\\n\\n\"\n",
    "    \"### Code Diff:\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5eb6f797-96f7-44ad-b438-ed3c12e850ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_comments(text):\n",
    "    pattern = re.compile(r'<COMMENT side=\"(?P<side>[^\"]+)\" offset=\"(?P<offset>\\d+)\">(?P<comment>.+)')\n",
    "    results = []\n",
    "    for line in text.strip().splitlines():\n",
    "        match = pattern.match(line.strip())\n",
    "        if match:\n",
    "            results.append({\n",
    "                \"side\": match.group(\"side\"),\n",
    "                \"offset\": int(match.group(\"offset\")),\n",
    "                \"comment\": match.group(\"comment\").strip()\n",
    "            })\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "bad0089a-5b53-4620-9ee6-2cb46efef9a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32016, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32016, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ec15c265-2cbe-4477-b5d4-5661257c702d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def offset_match(pred_comments, true_comments, window=3):\n",
    "    correct = 0\n",
    "    total = len(pred_comments)\n",
    "\n",
    "    gt_pairs = {(c[\"side\"], c[\"offset\"]) for c in true_comments}\n",
    "    matched = set()\n",
    "\n",
    "    for pred in pred_comments:\n",
    "        pred_side, pred_offset = pred[\"side\"], pred[\"offset\"]\n",
    "        for gt in gt_pairs:\n",
    "            gt_side, gt_offset = gt\n",
    "            if pred_side == gt_side and abs(pred_offset - gt_offset) <= window:\n",
    "                matched.add(gt)\n",
    "                correct += 1\n",
    "                break  # count each pred only once\n",
    "\n",
    "    return correct, total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e72a6268-8a4e-4601-ae46-37fea7b358fd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Sample 0 ===\n",
      "[INST] You are a helpful code reviewer. Given a code diff, generate all relevant review comments. Each comment must be in the format: <COMMENT side=\"RIGHT\" offset=\"X\">Your comment here.\n",
      "Only output comments, nothing else.\n",
      "\n",
      "### Code Diff:\n",
      "<DIFF>\n",
      "@@ -838,45 +839,58 @@ function performConcurrentWorkOnRoot(root, didTimeout) {\n",
      "       throw fatalError;\n",
      "     }\n",
      " \n",
      "-    // Check if this render may have yielded to a concurrent event, and if so,\n",
      "-    // confirm that any newly rendered stores are consistent.\n",
      "-    // TODO: It's possible that even a concurrent render may never have yielded\n",
      "-    // to the main thread, if it was fast enough, or if it expired. We could\n",
      "-    // skip the consistency check in that case, too.\n",
      "-    const renderWasConcurrent = !includesBlockingLane(root, lanes);\n",
      "-    const finishedWork: Fiber = (root.current.alternate: any);\n",
      "-    if (\n",
      "-      renderWasConcurrent &&\n",
      "-      !isRenderConsistentWithExternalStores(finishedWork)\n",
      "-    ) {\n",
      "-      // A store was mutated in an interleaved event. Render again,\n",
      "-      // synchronously, to block further mutations.\n",
      "-      exitStatus = renderRootSync(root, lanes);\n",
      "-\n",
      "-      // We need to check again if something threw\n",
      "-      if (exitStatus === RootErrored) {\n",
      "-        const errorRetryLanes = getLanesToRetrySynchronouslyOnError(root);\n",
      "-        if (errorRetryLanes !== NoLanes) {\n",
      "-          lanes = errorRetryLanes;\n",
      "-          exitStatus = recoverFromConcurrentError(root, errorRetryLanes);\n",
      "-          // We assume the tree is now consistent because we didn't yield to any\n",
      "-          // concurrent events.\n",
      "+    if (exitStatus === RootDidNotComplete) {\n",
      "</DIFF> [/INST]  <COMMENT side=\"RIGHT\" offset=\"838\">This code looks like it's checking if a render was concurrent and if it was consistent with external stores. It's also checking if the render was interrupted by an error and if so, it's retrying the render synchronously.</COMMENT>\n",
      "\n",
      "<COMMENT side=\"RIGHT\" offset=\"840\">It's possible that even a concurrent render may never have yielded to the main thread, if it was fast enough, or if it expired. We could skip the consistency check in that case, too.</COMMENT>\n",
      "\n",
      "<COMMENT side=\"RIGHT\" offset=\"842\">The code is checking if the render was concurrent and if it was consistent with external stores. If it was not consistent, it's retrying the render synchronously.</COMMENT>\n",
      "\n",
      "<COMMENT side=\"RIGHT\" offset=\"844\">It's assuming that the tree is now consistent because it didn't yield to any concurrent events.</COMMENT>\n",
      "\n",
      "<COMMENT side=\"RIGHT\" offset=\"846\">It's checking if the\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Sample 1 ===\n",
      "[INST] You are a helpful code reviewer. Given a code diff, generate all relevant review comments. Each comment must be in the format: <COMMENT side=\"RIGHT\" offset=\"X\">Your comment here.\n",
      "Only output comments, nothing else.\n",
      "\n",
      "### Code Diff:\n",
      "<DIFF>\n",
      "@@ -838,45 +839,58 @@ function performConcurrentWorkOnRoot(root, didTimeout) {\n",
      "       throw fatalError;\n",
      "     }\n",
      " \n",
      "-    // Check if this render may have yielded to a concurrent event, and if so,\n",
      "-    // confirm that any newly rendered stores are consistent.\n",
      "-    // TODO: It's possible that even a concurrent render may never have yielded\n",
      "-    // to the main thread, if it was fast enough, or if it expired. We could\n",
      "-    // skip the consistency check in that case, too.\n",
      "-    const renderWasConcurrent = !includesBlockingLane(root, lanes);\n",
      "-    const finishedWork: Fiber = (root.current.alternate: any);\n",
      "-    if (\n",
      "-      renderWasConcurrent &&\n",
      "-      !isRenderConsistentWithExternalStores(finishedWork)\n",
      "-    ) {\n",
      "-      // A store was mutated in an interleaved event. Render again,\n",
      "-      // synchronously, to block further mutations.\n",
      "-      exitStatus = renderRootSync(root, lanes);\n",
      "-\n",
      "-      // We need to check again if something threw\n",
      "-      if (exitStatus === RootErrored) {\n",
      "-        const errorRetryLanes = getLanesToRetrySynchronouslyOnError(root);\n",
      "-        if (errorRetryLanes !== NoLanes) {\n",
      "-          lanes = errorRetryLanes;\n",
      "-          exitStatus = recoverFromConcurrentError(root, errorRetryLanes);\n",
      "-          // We assume the tree is now consistent because we didn't yield to any\n",
      "-          // concurrent events.\n",
      "+    if (exitStatus === RootDidNotComplete) {\n",
      "+      // The render unwound without completing the tree. This happens in special\n",
      "+      // cases where need to exit the current render without producing a\n",
      "+      // consistent tree or committing.\n",
      "+      //\n",
      "+      // This should only happen during a concurrent render, not a discrete or\n",
      "+      // synchronous update. We should have already checked for this when we\n",
      "+      // unwound the stack.\n",
      "</DIFF> [/INST]  <COMMENT side=\"RIGHT\" offset=\"838\">This code looks like it's checking if a render was concurrent and if it was consistent with external stores. However, it's not clear why this check is necessary or what the implications of this check are.</COMMENT>\n",
      "\n",
      "<COMMENT side=\"RIGHT\" offset=\"845\">This code is checking if a render was concurrent and if it was consistent with external stores. If the render was concurrent and the stores were not consistent, it will re-render the tree synchronously to block further mutations.</COMMENT>\n",
      "\n",
      "<COMMENT side=\"RIGHT\" offset=\"853\">This code is checking if a render was concurrent and if it was consistent with external stores. If the render was concurrent and the stores were not consistent, it will re-render the tree synchronously to block further mutations. However, it's not clear why this check is necessary or what the implications of this check are.</COMMENT>\n",
      "\n",
      "<COMMENT side=\"RIGHT\" offset=\"861\">This code is checking if a render was concurrent and if it was consistent with external stores. If the render\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Sample 2 ===\n",
      "[INST] You are a helpful code reviewer. Given a code diff, generate all relevant review comments. Each comment must be in the format: <COMMENT side=\"RIGHT\" offset=\"X\">Your comment here.\n",
      "Only output comments, nothing else.\n",
      "\n",
      "### Code Diff:\n",
      "<DIFF>\n",
      "@@ -1149,6 +1163,10 @@ function performSyncWorkOnRoot(root) {\n",
      "     throw fatalError;\n",
      "   }\n",
      " \n",
      "+  if (exitStatus === RootDidNotComplete) {\n",
      "</DIFF> [/INST]  <COMMENT side=\"RIGHT\" offset=\"1163\">This line is not needed, as the `if` statement is already checking for the `exitStatus` to be `RootDidNotComplete`.</COMMENT>\n",
      "<COMMENT side=\"RIGHT\" offset=\"1164\">It would be better to remove this line and simplify the code.</COMMENT>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Sample 3 ===\n",
      "[INST] You are a helpful code reviewer. Given a code diff, generate all relevant review comments. Each comment must be in the format: <COMMENT side=\"RIGHT\" offset=\"X\">Your comment here.\n",
      "Only output comments, nothing else.\n",
      "\n",
      "### Code Diff:\n",
      "<DIFF>\n",
      "@@ -739,14 +739,20 @@ describe('ReactCache', () => {\n",
      "     await act(async () => {\n",
      "       refresh();\n",
      "     });\n",
      "-    expect(Scheduler).toHaveYielded(['Cache miss! [A]', 'Loading...']);\n",
      "+    expect(Scheduler).toHaveYielded([\n",
      "+      'Cache miss! [A]',\n",
      "+      'Loading...',\n",
      "+      // TODO: This happens too early, because we don't retain the refreshed\n",
      "+      // cache until it commits. Will fix in next step.\n",
      "+      'Cache cleanup: A [v1]',\n",
      "</DIFF> [/INST]  <COMMENT side=\"RIGHT\" offset=\"10\">This comment is suggesting that the cache cleanup is happening too early, before the refreshed cache has committed. This could be fixed by waiting for the cache to commit before cleaning up.</COMMENT>\n",
      "\n",
      "<COMMENT side=\"RIGHT\" offset=\"15\">This comment is suggesting that the cache cleanup is happening too early, before the refreshed cache has committed. This could be fixed by waiting for the cache to commit before cleaning up.</COMMENT>\n",
      "\n",
      "<COMMENT side=\"RIGHT\" offset=\"18\">This comment is suggesting that the cache cleanup is happening too early, before the refreshed cache has committed. This could be fixed by waiting for the cache to commit before cleaning up.</COMMENT>\n",
      "\n",
      "<COMMENT side=\"RIGHT\" offset=\"21\">This comment is suggesting that the cache cleanup is happening too early, before the refreshed cache has committed. This could be fixed by waiting for the cache to commit before cleaning up.</COMMENT>\n",
      "\n",
      "<COMMENT side=\"RIGHT\" offset=\"24\">This comment is suggesting that the cache cleanup\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Sample 4 ===\n",
      "[INST] You are a helpful code reviewer. Given a code diff, generate all relevant review comments. Each comment must be in the format: <COMMENT side=\"RIGHT\" offset=\"X\">Your comment here.\n",
      "Only output comments, nothing else.\n",
      "\n",
      "### Code Diff:\n",
      "<DIFF>\n",
      "@@ -739,14 +739,20 @@ describe('ReactCache', () => {\n",
      "     await act(async () => {\n",
      "       refresh();\n",
      "     });\n",
      "-    expect(Scheduler).toHaveYielded(['Cache miss! [A]', 'Loading...']);\n",
      "+    expect(Scheduler).toHaveYielded([\n",
      "+      'Cache miss! [A]',\n",
      "+      'Loading...',\n",
      "+      // TODO: This happens too early, because we don't retain the refreshed\n",
      "+      // cache until it commits. Will fix in next step.\n",
      "+      'Cache cleanup: A [v1]',\n",
      "</DIFF> [/INST]  <COMMENT side=\"RIGHT\" offset=\"10\">This comment is suggesting that the cache cleanup is happening too early, before the refreshed cache has committed. This could be fixed by waiting for the cache to commit before cleaning up.</COMMENT>\n",
      "\n",
      "<COMMENT side=\"RIGHT\" offset=\"15\">This comment is suggesting that the cache cleanup is happening too early, before the refreshed cache has committed. This could be fixed by waiting for the cache to commit before cleaning up.</COMMENT>\n",
      "\n",
      "<COMMENT side=\"RIGHT\" offset=\"18\">This comment is suggesting that the cache cleanup is happening too early, before the refreshed cache has committed. This could be fixed by waiting for the cache to commit before cleaning up.</COMMENT>\n",
      "\n",
      "<COMMENT side=\"RIGHT\" offset=\"21\">This comment is suggesting that the cache cleanup is happening too early, before the refreshed cache has committed. This could be fixed by waiting for the cache to commit before cleaning up.</COMMENT>\n",
      "\n",
      "<COMMENT side=\"RIGHT\" offset=\"24\">This comment is suggesting that the cache cleanup\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Sample 5 ===\n",
      "[INST] You are a helpful code reviewer. Given a code diff, generate all relevant review comments. Each comment must be in the format: <COMMENT side=\"RIGHT\" offset=\"X\">Your comment here.\n",
      "Only output comments, nothing else.\n",
      "\n",
      "### Code Diff:\n",
      "<DIFF>\n",
      "@@ -739,14 +739,20 @@ describe('ReactCache', () => {\n",
      "     await act(async () => {\n",
      "       refresh();\n",
      "     });\n",
      "-    expect(Scheduler).toHaveYielded(['Cache miss! [A]', 'Loading...']);\n",
      "+    expect(Scheduler).toHaveYielded([\n",
      "+      'Cache miss! [A]',\n",
      "+      'Loading...',\n",
      "+      // TODO: This happens too early, because we don't retain the refreshed\n",
      "+      // cache until it commits. Will fix in next step.\n",
      "+      'Cache cleanup: A [v1]',\n",
      "</DIFF> [/INST]  <COMMENT side=\"RIGHT\" offset=\"10\">This comment is suggesting that the cache cleanup is happening too early, before the refreshed cache has committed. This could be fixed by waiting for the cache to commit before cleaning up.</COMMENT>\n",
      "\n",
      "<COMMENT side=\"RIGHT\" offset=\"15\">This comment is suggesting that the cache cleanup is happening too early, before the refreshed cache has committed. This could be fixed by waiting for the cache to commit before cleaning up.</COMMENT>\n",
      "\n",
      "<COMMENT side=\"RIGHT\" offset=\"18\">This comment is suggesting that the cache cleanup is happening too early, before the refreshed cache has committed. This could be fixed by waiting for the cache to commit before cleaning up.</COMMENT>\n",
      "\n",
      "<COMMENT side=\"RIGHT\" offset=\"21\">This comment is suggesting that the cache cleanup is happening too early, before the refreshed cache has committed. This could be fixed by waiting for the cache to commit before cleaning up.</COMMENT>\n",
      "\n",
      "<COMMENT side=\"RIGHT\" offset=\"24\">This comment is suggesting that the cache cleanup\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Sample 6 ===\n",
      "[INST] You are a helpful code reviewer. Given a code diff, generate all relevant review comments. Each comment must be in the format: <COMMENT side=\"RIGHT\" offset=\"X\">Your comment here.\n",
      "Only output comments, nothing else.\n",
      "\n",
      "### Code Diff:\n",
      "<DIFF>\n",
      "@@ -0,0 +1,197 @@\n",
      "+/**\n",
      "+ * Copyright (c) Facebook, Inc. and its affiliates.\n",
      "+ *\n",
      "+ * This source code is licensed under the MIT license found in the\n",
      "+ * LICENSE file in the root directory of this source tree.\n",
      "+ *\n",
      "+ * @emails react-core\n",
      "+ */\n",
      "+\n",
      "+let JSDOM;\n",
      "+let React;\n",
      "+let ReactDOM;\n",
      "+let Scheduler;\n",
      "+let clientAct;\n",
      "+let ReactDOMFizzServer;\n",
      "+let Stream;\n",
      "+let document;\n",
      "+let writable;\n",
      "+let container;\n",
      "+let buffer = '';\n",
      "+let hasErrored = false;\n",
      "+let fatalError = undefined;\n",
      "+let textCache;\n",
      "+\n",
      "+describe('useId', () => {\n",
      "+  beforeEach(() => {\n",
      "+    jest.resetModules();\n",
      "+    JSDOM = require('jsdom').JSDOM;\n",
      "+    React = require('react');\n",
      "+    ReactDOM = require('react-dom');\n",
      "+    Scheduler = require('scheduler');\n",
      "+    clientAct = require('jest-react').act;\n",
      "+    ReactDOMFizzServer = require('react-dom/server');\n",
      "+    Stream = require('stream');\n",
      "+\n",
      "+    textCache = new Map();\n",
      "+\n",
      "+    // Test Environment\n",
      "+    const jsdom = new JSDOM(\n",
      "+      '<!DOCTYPE html><html><head></head><body><div id=\"container\">',\n",
      "+      {\n",
      "+        runScripts: 'dangerously',\n",
      "</DIFF> [/INST]  <COMMENT side=\"RIGHT\" offset=\"1\">This code looks like it's setting up a test environment for a React component. It's using JSDOM to create a virtual DOM, and it's requiring several other libraries that are used in React.</COMMENT>\n",
      "\n",
      "  <COMMENT side=\"RIGHT\" offset=\"10\">The `textCache` variable is being initialized as a new Map, which is a data structure that maps keys to values. It's not clear what this variable is used for, but it's likely that it's being used to cache some data or results.</COMMENT>\n",
      "\n",
      "  <COMMENT side=\"RIGHT\" offset=\"15\">The `describe` function is being used to define a test suite. It's not clear what this test suite is testing, but it's likely that it's testing some functionality related to the `useId` hook.</COMMENT>\n",
      "\n",
      "  <COMMENT side=\"RIGHT\" offset=\"20\">The `beforeEach` function is being used to set up some variables that are used in the test suite. It's not clear what these variables are used for\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [01:28<00:00, 11.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Sample 7 ===\n",
      "[INST] You are a helpful code reviewer. Given a code diff, generate all relevant review comments. Each comment must be in the format: <COMMENT side=\"RIGHT\" offset=\"X\">Your comment here.\n",
      "Only output comments, nothing else.\n",
      "\n",
      "### Code Diff:\n",
      "<DIFF>\n",
      "@@ -389,17 +389,6 @@ describe('ReactSuspense', () => {\n",
      "     expect(root).toMatchRenderedOutput('Hi');\n",
      "   });\n",
      " \n",
      "-  it('throws if tree suspends and none of the Suspense ancestors have a boundary', () => {\n",
      "</DIFF> [/INST]  <COMMENT side=\"RIGHT\" offset=\"389\">This test seems to be testing the behavior of the `ReactSuspense` component when it is used in a situation where it is not expected to be used. The test is checking that an error is thrown when the component suspends and none of the Suspense ancestors have a boundary. However, this test is not actually testing the behavior of the `ReactSuspense` component in this situation. It would be more useful to test the behavior of the component in a real-world scenario where it is actually used.\n",
      "\n",
      "Additionally, the test is not actually testing the error message that is thrown when the component suspends and none of the Suspense ancestors have a boundary. It would be more useful to test that the correct error message is thrown in this situation.\n",
      "\n",
      "Overall, this test seems to be testing the behavior of the `ReactSuspense` component in a way that is not actually relevant to its intended use case. It would be more useful to test the component in a more realistic scenario.\n",
      "</COMMENT>\n",
      "\n",
      "<COMMENT side=\"RIGHT\" offset=\"390\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "references = []\n",
    "candidates = []\n",
    "offset_match_total = 0\n",
    "offset_match_correct = 0\n",
    "latencies = []\n",
    "\n",
    "for i, sample in enumerate(tqdm(test_data[:100])):  # Full test set if needed\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        [{\"role\": \"user\", \"content\": instruction + sample[\"input\"]}],\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "    inputs = tokenizer(\n",
    "        prompt,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=4096\n",
    "    ).to(\"cuda:0\")\n",
    "\n",
    "    true_comments = extract_comments(sample[\"label\"])\n",
    "\n",
    "    # Measure inference latency\n",
    "    with torch.no_grad():\n",
    "        start = time.time()\n",
    "        output_ids = model.generate(**inputs, max_new_tokens=256, do_sample=False)\n",
    "        latencies.append(time.time() - start)\n",
    "\n",
    "    # Decode and evaluate\n",
    "    generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True).strip()\n",
    "    generated_text = generated_text.split(\"Setting `pad_token_id\")[0].strip()\n",
    "    print(f\"\\n=== Sample {i} ===\\n{generated_text}\")\n",
    "\n",
    "    pred_comments = extract_comments(generated_text)\n",
    "\n",
    "    for gt in true_comments:\n",
    "        references.append(gt[\"comment\"])\n",
    "        best_pred = max(pred_comments, key=lambda p: p[\"comment\"], default={\"comment\": \"\"})\n",
    "        candidates.append(best_pred[\"comment\"] if best_pred else \"\")\n",
    "\n",
    "    correct, total = offset_match(pred_comments, true_comments, window=3)\n",
    "    offset_match_correct += correct\n",
    "    offset_match_total += total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "49490094-41c4-4d0a-8e51-cb463e34c143",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Inference Latency Stats ===\n",
      "Median latency:        11944.54 ms\n",
      "95th percentile:       11986.58 ms\n",
      "99th percentile:       11990.94 ms\n",
      "Throughput (1 sample): 0.09 samples/sec\n"
     ]
    }
   ],
   "source": [
    "# Latency stats\n",
    "print(\"\\n=== Inference Latency Stats ===\")\n",
    "print(f\"Median latency:        {np.percentile(latencies, 50) * 1000:.2f} ms\")\n",
    "print(f\"95th percentile:       {np.percentile(latencies, 95) * 1000:.2f} ms\")\n",
    "print(f\"99th percentile:       {np.percentile(latencies, 99) * 1000:.2f} ms\")\n",
    "print(f\"Throughput (1 sample): {len(latencies) / np.sum(latencies):.2f} samples/sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b0323dc1-c576-47ab-bbc0-7c930423c226",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a05a22d1d13542058dc2c6ab2e0a5925",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12b70f2540b94a449e1382c5fb7b7ebf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/482 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ae8c08f0a7c4dc49e366276b568866a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0894afb3e66f41aaa212b478ed44d332",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62ef980027db405aace187f0c84cbd43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f5a741258c4450a821bc77382473c1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.42G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b466ba0f625d421ba6b16ed8e907cc59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acbd2d15c2ed4376918854adf3c38c02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 0.15 seconds, 55.11 sentences/sec\n",
      "\n",
      "Evaluation Results\n",
      "Avg BERTScore F1: 0.7275\n",
      "Offset Precision: 0/23 = 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n"
     ]
    }
   ],
   "source": [
    "# BERTScore\n",
    "P, R, F1 = bert_score(candidates, references, lang=\"en\", verbose=True)\n",
    "print(\"\\nEvaluation Results\")\n",
    "print(f\"Avg BERTScore F1: {F1.mean().item():.4f}\")\n",
    "print(f\"Offset Precision: {offset_match_correct}/{offset_match_total} = {offset_match_correct/offset_match_total:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a49350b7-5cf3-458d-84ae-367a972e5511",
   "metadata": {},
   "source": [
    "Batch throughput test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "30882c1b-72a9-4340-a50e-fc29d763e714",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 344.00 MiB. GPU 0 has a total capacity of 23.60 GiB of which 158.25 MiB is free. Process 24736 has 23.44 GiB memory in use. Of the allocated memory 20.11 GiB is allocated by PyTorch, and 3.05 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 23\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Warm-up\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 23\u001b[0m     _ \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mencoded\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m batch_times \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/transformers/generation/utils.py:2465\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, **kwargs)\u001b[0m\n\u001b[1;32m   2457\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2458\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2459\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   2460\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2461\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2462\u001b[0m     )\n\u001b[1;32m   2464\u001b[0m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[0;32m-> 2465\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2466\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2467\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2468\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2469\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2470\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2471\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2472\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2473\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2475\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[1;32m   2476\u001b[0m     \u001b[38;5;66;03m# 11. interleave input_ids with `num_beams` additional sequences per batch\u001b[39;00m\n\u001b[1;32m   2477\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2478\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2479\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[1;32m   2480\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2481\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2482\u001b[0m     )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/transformers/generation/utils.py:3431\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   3428\u001b[0m model_inputs\u001b[38;5;241m.\u001b[39mupdate({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_hidden_states\u001b[39m\u001b[38;5;124m\"\u001b[39m: output_hidden_states} \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;28;01melse\u001b[39;00m {})\n\u001b[1;32m   3430\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_prefill:\n\u001b[0;32m-> 3431\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   3432\u001b[0m     is_prefill \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   3433\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/transformers/utils/generic.py:965\u001b[0m, in \u001b[0;36mcan_return_tuple.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    962\u001b[0m     set_attribute_for_modules(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_is_top_level_module\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    964\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 965\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    966\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_requested_to_return_tuple \u001b[38;5;129;01mor\u001b[39;00m (is_configured_to_return_tuple \u001b[38;5;129;01mand\u001b[39;00m is_top_level_module):\n\u001b[1;32m    967\u001b[0m         output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mto_tuple()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/transformers/utils/deprecation.py:172\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action\u001b[38;5;241m.\u001b[39mNOTIFY, Action\u001b[38;5;241m.\u001b[39mNOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[1;32m    170\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m--> 172\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/transformers/models/llama/modeling_llama.py:821\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, cache_position, logits_to_keep, **kwargs)\u001b[0m\n\u001b[1;32m    816\u001b[0m output_hidden_states \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    817\u001b[0m     output_hidden_states \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39moutput_hidden_states\n\u001b[1;32m    818\u001b[0m )\n\u001b[1;32m    820\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m--> 821\u001b[0m outputs: BaseModelOutputWithPast \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    822\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    823\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    824\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    825\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    826\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    828\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    829\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    830\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    831\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    832\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    834\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlast_hidden_state\n\u001b[1;32m    835\u001b[0m \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/transformers/utils/generic.py:965\u001b[0m, in \u001b[0;36mcan_return_tuple.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    962\u001b[0m     set_attribute_for_modules(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_is_top_level_module\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    964\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 965\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    966\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_requested_to_return_tuple \u001b[38;5;129;01mor\u001b[39;00m (is_configured_to_return_tuple \u001b[38;5;129;01mand\u001b[39;00m is_top_level_module):\n\u001b[1;32m    967\u001b[0m         output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mto_tuple()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/transformers/models/llama/modeling_llama.py:571\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, cache_position, **flash_attn_kwargs)\u001b[0m\n\u001b[1;32m    559\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    560\u001b[0m         partial(decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mflash_attn_kwargs),\n\u001b[1;32m    561\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    568\u001b[0m         position_embeddings,\n\u001b[1;32m    569\u001b[0m     )\n\u001b[1;32m    570\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 571\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    572\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    573\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    574\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    575\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    576\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    577\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    578\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    579\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    580\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mflash_attn_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    581\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    583\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    585\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/transformers/models/llama/modeling_llama.py:334\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    332\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[1;32m    333\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_attention_layernorm(hidden_states)\n\u001b[0;32m--> 334\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    335\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    337\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (hidden_states,)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/transformers/models/llama/modeling_llama.py:172\u001b[0m, in \u001b[0;36mLlamaMLP.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m--> 172\u001b[0m     down_proj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdown_proj(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mact_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgate_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mup_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m down_proj\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 344.00 MiB. GPU 0 has a total capacity of 23.60 GiB of which 158.25 MiB is free. Process 24736 has 23.44 GiB memory in use. Of the allocated memory 20.11 GiB is allocated by PyTorch, and 3.05 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "batch_size = 4\n",
    "num_batches = 10\n",
    "batch_inputs = [test_data[i][\"input\"] for i in range(batch_size)]\n",
    "batch_prompts = [\n",
    "    tokenizer.apply_chat_template(\n",
    "        [{\"role\": \"user\", \"content\": instruction + inp}],\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    for inp in batch_inputs\n",
    "]\n",
    "\n",
    "encoded = tokenizer(\n",
    "    batch_prompts,\n",
    "    return_tensors=\"pt\",\n",
    "    padding=\"max_length\",\n",
    "    truncation=True,\n",
    "    max_length=4096\n",
    ").to(\"cuda:0\")\n",
    "\n",
    "# Warm-up\n",
    "with torch.no_grad():\n",
    "    _ = model.generate(**encoded, max_new_tokens=256, do_sample=False)\n",
    "\n",
    "batch_times = []\n",
    "with torch.no_grad():\n",
    "    for _ in range(num_batches):\n",
    "        start = time.time()\n",
    "        _ = model.generate(**encoded, max_new_tokens=256, do_sample=False)\n",
    "        batch_times.append(time.time() - start)\n",
    "\n",
    "batch_fps = (batch_size * num_batches) / np.sum(batch_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5cd99a-3643-4300-8087-f23fbf8940ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== Final Evaluation Summary ===\")\n",
    "print(f\"Model Size on Disk:               {get_model_size(model):.2f}\")\n",
    "print(f\"BERTScore F1 (avg):               {F1.mean().item():.4f}\")\n",
    "print(f\"Location Precision:               {offset_match_correct}/{offset_match_total} = {offset_match_correct / offset_match_total:.4f}\")\n",
    "print(f\"Inference Latency (median):       {np.percentile(latencies, 50) * 1000:.2f} ms\")\n",
    "print(f\"Inference Latency (95th pct):     {np.percentile(latencies, 95) * 1000:.2f} ms\")\n",
    "print(f\"Inference Latency (99th pct):     {np.percentile(latencies, 99) * 1000:.2f} ms\")\n",
    "print(f\"Inference Throughput (1 sample):  {len(latencies) / np.sum(latencies):.2f} samples/sec\")\n",
    "print(f\"Batch Throughput ({batch_size}):  {batch_fps:.2f} samples/sec\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "911505f5-d4bd-4f7b-a8f5-e699dcec3743",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a1157d-4706-4d00-93fc-a054dcd5f1ea",
   "metadata": {},
   "source": [
    "### Compiled model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "50d13d22-c3ca-449c-ac9c-4c6c24f441c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a408cd1b-9629-42c0-aea3-0d176bc4c52e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Sample 0 ===\n",
      "[INST] You are a helpful code reviewer. Given a code diff, generate all relevant review comments. Each comment must be in the format: <COMMENT side=\"RIGHT\" offset=\"X\">Your comment here.\n",
      "Only output comments, nothing else.\n",
      "\n",
      "### Code Diff:\n",
      "<DIFF>\n",
      "@@ -838,45 +839,58 @@ function performConcurrentWorkOnRoot(root, didTimeout) {\n",
      "       throw fatalError;\n",
      "     }\n",
      " \n",
      "-    // Check if this render may have yielded to a concurrent event, and if so,\n",
      "-    // confirm that any newly rendered stores are consistent.\n",
      "-    // TODO: It's possible that even a concurrent render may never have yielded\n",
      "-    // to the main thread, if it was fast enough, or if it expired. We could\n",
      "-    // skip the consistency check in that case, too.\n",
      "-    const renderWasConcurrent = !includesBlockingLane(root, lanes);\n",
      "-    const finishedWork: Fiber = (root.current.alternate: any);\n",
      "-    if (\n",
      "-      renderWasConcurrent &&\n",
      "-      !isRenderConsistentWithExternalStores(finishedWork)\n",
      "-    ) {\n",
      "-      // A store was mutated in an interleaved event. Render again,\n",
      "-      // synchronously, to block further mutations.\n",
      "-      exitStatus = renderRootSync(root, lanes);\n",
      "-\n",
      "-      // We need to check again if something threw\n",
      "-      if (exitStatus === RootErrored) {\n",
      "-        const errorRetryLanes = getLanesToRetrySynchronouslyOnError(root);\n",
      "-        if (errorRetryLanes !== NoLanes) {\n",
      "-          lanes = errorRetryLanes;\n",
      "-          exitStatus = recoverFromConcurrentError(root, errorRetryLanes);\n",
      "-          // We assume the tree is now consistent because we didn't yield to any\n",
      "-          // concurrent events.\n",
      "+    if (exitStatus === RootDidNotComplete) {\n",
      "</DIFF> [/INST]  <COMMENT side=\"RIGHT\" offset=\"838\">This code looks like it's checking if a render was concurrent and if it was consistent with external stores. It's also checking if the render was interrupted by an error and if so, it's retrying the render synchronously.</COMMENT>\n",
      "\n",
      "<COMMENT side=\"RIGHT\" offset=\"840\">It's possible that even a concurrent render may never have yielded to the main thread, if it was fast enough, or if it expired. We could skip the consistency check in that case, too.</COMMENT>\n",
      "\n",
      "<COMMENT side=\"RIGHT\" offset=\"842\">The code is checking if the render was concurrent and if it was consistent with external stores. If it was not consistent, it's retrying the render synchronously.</COMMENT>\n",
      "\n",
      "<COMMENT side=\"RIGHT\" offset=\"844\">It's assuming that the tree is now consistent because it didn't yield to any concurrent events.</COMMENT>\n",
      "\n",
      "<COMMENT side=\"RIGHT\" offset=\"846\">It's checking if the\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Sample 1 ===\n",
      "[INST] You are a helpful code reviewer. Given a code diff, generate all relevant review comments. Each comment must be in the format: <COMMENT side=\"RIGHT\" offset=\"X\">Your comment here.\n",
      "Only output comments, nothing else.\n",
      "\n",
      "### Code Diff:\n",
      "<DIFF>\n",
      "@@ -838,45 +839,58 @@ function performConcurrentWorkOnRoot(root, didTimeout) {\n",
      "       throw fatalError;\n",
      "     }\n",
      " \n",
      "-    // Check if this render may have yielded to a concurrent event, and if so,\n",
      "-    // confirm that any newly rendered stores are consistent.\n",
      "-    // TODO: It's possible that even a concurrent render may never have yielded\n",
      "-    // to the main thread, if it was fast enough, or if it expired. We could\n",
      "-    // skip the consistency check in that case, too.\n",
      "-    const renderWasConcurrent = !includesBlockingLane(root, lanes);\n",
      "-    const finishedWork: Fiber = (root.current.alternate: any);\n",
      "-    if (\n",
      "-      renderWasConcurrent &&\n",
      "-      !isRenderConsistentWithExternalStores(finishedWork)\n",
      "-    ) {\n",
      "-      // A store was mutated in an interleaved event. Render again,\n",
      "-      // synchronously, to block further mutations.\n",
      "-      exitStatus = renderRootSync(root, lanes);\n",
      "-\n",
      "-      // We need to check again if something threw\n",
      "-      if (exitStatus === RootErrored) {\n",
      "-        const errorRetryLanes = getLanesToRetrySynchronouslyOnError(root);\n",
      "-        if (errorRetryLanes !== NoLanes) {\n",
      "-          lanes = errorRetryLanes;\n",
      "-          exitStatus = recoverFromConcurrentError(root, errorRetryLanes);\n",
      "-          // We assume the tree is now consistent because we didn't yield to any\n",
      "-          // concurrent events.\n",
      "+    if (exitStatus === RootDidNotComplete) {\n",
      "+      // The render unwound without completing the tree. This happens in special\n",
      "+      // cases where need to exit the current render without producing a\n",
      "+      // consistent tree or committing.\n",
      "+      //\n",
      "+      // This should only happen during a concurrent render, not a discrete or\n",
      "+      // synchronous update. We should have already checked for this when we\n",
      "+      // unwound the stack.\n",
      "</DIFF> [/INST]  <COMMENT side=\"RIGHT\" offset=\"838\">This code looks like it's checking if a render was concurrent and if it was consistent with external stores. However, it's not clear why this check is necessary or what the implications of this check are.</COMMENT>\n",
      "\n",
      "<COMMENT side=\"RIGHT\" offset=\"845\">This code is checking if a render was concurrent and if it was consistent with external stores. If the render was concurrent and the stores were not consistent, it will re-render the tree synchronously to block further mutations.</COMMENT>\n",
      "\n",
      "<COMMENT side=\"RIGHT\" offset=\"853\">This code is checking if a render was concurrent and if it was consistent with external stores. If the render was concurrent and the stores were not consistent, it will re-render the tree synchronously to block further mutations. However, it's not clear why this check is necessary or what the implications of this check are.</COMMENT>\n",
      "\n",
      "<COMMENT side=\"RIGHT\" offset=\"861\">This code is checking if a render was concurrent and if it was consistent with external stores. If the render\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Sample 2 ===\n",
      "[INST] You are a helpful code reviewer. Given a code diff, generate all relevant review comments. Each comment must be in the format: <COMMENT side=\"RIGHT\" offset=\"X\">Your comment here.\n",
      "Only output comments, nothing else.\n",
      "\n",
      "### Code Diff:\n",
      "<DIFF>\n",
      "@@ -1149,6 +1163,10 @@ function performSyncWorkOnRoot(root) {\n",
      "     throw fatalError;\n",
      "   }\n",
      " \n",
      "+  if (exitStatus === RootDidNotComplete) {\n",
      "</DIFF> [/INST]  <COMMENT side=\"RIGHT\" offset=\"1150\">This line is not needed. The `throw` statement will already cause the function to exit with an error. It's not necessary to check the `exitStatus` variable and throw a new error if it's not `RootCompleted`.</COMMENT>\n",
      "\n",
      "<COMMENT side=\"RIGHT\" offset=\"1164\">This line is not needed. The `if` statement is already checking if the `exitStatus` variable is equal to `RootDidNotComplete`. It's not necessary to check the `exitStatus` variable again in the `if` statement's body.</COMMENT>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Sample 3 ===\n",
      "[INST] You are a helpful code reviewer. Given a code diff, generate all relevant review comments. Each comment must be in the format: <COMMENT side=\"RIGHT\" offset=\"X\">Your comment here.\n",
      "Only output comments, nothing else.\n",
      "\n",
      "### Code Diff:\n",
      "<DIFF>\n",
      "@@ -739,14 +739,20 @@ describe('ReactCache', () => {\n",
      "     await act(async () => {\n",
      "       refresh();\n",
      "     });\n",
      "-    expect(Scheduler).toHaveYielded(['Cache miss! [A]', 'Loading...']);\n",
      "+    expect(Scheduler).toHaveYielded([\n",
      "+      'Cache miss! [A]',\n",
      "+      'Loading...',\n",
      "+      // TODO: This happens too early, because we don't retain the refreshed\n",
      "+      // cache until it commits. Will fix in next step.\n",
      "+      'Cache cleanup: A [v1]',\n",
      "</DIFF> [/INST]  <COMMENT side=\"RIGHT\" offset=\"10\">This comment is suggesting that the cache cleanup is happening too early, before the refreshed cache has committed. This could be fixed by waiting for the cache to commit before cleaning up.</COMMENT>\n",
      "\n",
      "<COMMENT side=\"RIGHT\" offset=\"15\">This comment is suggesting that the cache cleanup is happening too early, before the refreshed cache has committed. This could be fixed by waiting for the cache to commit before cleaning up.</COMMENT>\n",
      "\n",
      "<COMMENT side=\"RIGHT\" offset=\"18\">This comment is suggesting that the cache cleanup is happening too early, before the refreshed cache has committed. This could be fixed by waiting for the cache to commit before cleaning up.</COMMENT>\n",
      "\n",
      "<COMMENT side=\"RIGHT\" offset=\"21\">This comment is suggesting that the cache cleanup is happening too early, before the refreshed cache has committed. This could be fixed by waiting for the cache to commit before cleaning up.</COMMENT>\n",
      "\n",
      "<COMMENT side=\"RIGHT\" offset=\"24\">This comment is suggesting that the cache cleanup\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:35<00:00,  7.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Sample 4 ===\n",
      "[INST] You are a helpful code reviewer. Given a code diff, generate all relevant review comments. Each comment must be in the format: <COMMENT side=\"RIGHT\" offset=\"X\">Your comment here.\n",
      "Only output comments, nothing else.\n",
      "\n",
      "### Code Diff:\n",
      "<DIFF>\n",
      "@@ -739,14 +739,20 @@ describe('ReactCache', () => {\n",
      "     await act(async () => {\n",
      "       refresh();\n",
      "     });\n",
      "-    expect(Scheduler).toHaveYielded(['Cache miss! [A]', 'Loading...']);\n",
      "+    expect(Scheduler).toHaveYielded([\n",
      "+      'Cache miss! [A]',\n",
      "+      'Loading...',\n",
      "+      // TODO: This happens too early, because we don't retain the refreshed\n",
      "+      // cache until it commits. Will fix in next step.\n",
      "+      'Cache cleanup: A [v1]',\n",
      "</DIFF> [/INST]  <COMMENT side=\"RIGHT\" offset=\"10\">This comment is suggesting that the cache cleanup is happening too early, before the refreshed cache has committed. This could be fixed by waiting for the cache to commit before cleaning up.</COMMENT>\n",
      "\n",
      "<COMMENT side=\"RIGHT\" offset=\"15\">This comment is suggesting that the cache cleanup is happening too early, before the refreshed cache has committed. This could be fixed by waiting for the cache to commit before cleaning up.</COMMENT>\n",
      "\n",
      "<COMMENT side=\"RIGHT\" offset=\"18\">This comment is suggesting that the cache cleanup is happening too early, before the refreshed cache has committed. This could be fixed by waiting for the cache to commit before cleaning up.</COMMENT>\n",
      "\n",
      "<COMMENT side=\"RIGHT\" offset=\"21\">This comment is suggesting that the cache cleanup is happening too early, before the refreshed cache has committed. This could be fixed by waiting for the cache to commit before cleaning up.</COMMENT>\n",
      "\n",
      "<COMMENT side=\"RIGHT\" offset=\"24\">This comment is suggesting that the cache cleanup\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "references = []\n",
    "candidates = []\n",
    "offset_match_total = 0\n",
    "offset_match_correct = 0\n",
    "\n",
    "for i, sample in enumerate(tqdm(test_data[:5])):  # Full test set if needed\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        [{\"role\": \"user\", \"content\": instruction + sample[\"input\"]}],\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "    inputs = tokenizer(\n",
    "        prompt,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=4096\n",
    "    ).to(\"cuda:0\")\n",
    "\n",
    "    true_comments = extract_comments(sample[\"label\"])\n",
    "\n",
    "    # Measure inference latency\n",
    "    with torch.no_grad():\n",
    "        start = time.time()\n",
    "        output_ids = model.generate(**inputs, max_new_tokens=256, do_sample=False)\n",
    "        latencies.append(time.time() - start)\n",
    "\n",
    "    # Decode and evaluate\n",
    "    generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True).strip()\n",
    "    generated_text = generated_text.split(\"Setting `pad_token_id\")[0].strip()\n",
    "    print(f\"\\n=== Sample {i} ===\\n{generated_text}\")\n",
    "\n",
    "    pred_comments = extract_comments(generated_text)\n",
    "\n",
    "    for gt in true_comments:\n",
    "        references.append(gt[\"comment\"])\n",
    "        best_pred = max(pred_comments, key=lambda p: p[\"comment\"], default={\"comment\": \"\"})\n",
    "        candidates.append(best_pred[\"comment\"] if best_pred else \"\")\n",
    "\n",
    "    correct, total = offset_match(pred_comments, true_comments, window=3)\n",
    "    offset_match_correct += correct\n",
    "    offset_match_total += total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0ef32e3d-1306-4f65-9237-96ad4a7f3dcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "batch_size = 4\n",
    "num_batches = 10\n",
    "batch_inputs = [test_data[i][\"input\"] for i in range(batch_size)]\n",
    "batch_prompts = [\n",
    "    tokenizer.apply_chat_template(\n",
    "        [{\"role\": \"user\", \"content\": instruction + inp}],\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    for inp in batch_inputs\n",
    "]\n",
    "\n",
    "encoded = tokenizer(\n",
    "    batch_prompts,\n",
    "    return_tensors=\"pt\",\n",
    "    padding=\"max_length\",\n",
    "    truncation=True,\n",
    "    max_length=4096\n",
    ").to(\"cuda:0\")\n",
    "\n",
    "# Warm-up\n",
    "with torch.no_grad():\n",
    "    _ = model.generate(**encoded, max_new_tokens=256, do_sample=False)\n",
    "\n",
    "batch_times = []\n",
    "with torch.no_grad():\n",
    "    for _ in range(num_batches):\n",
    "        start = time.time()\n",
    "        _ = model.generate(**encoded, max_new_tokens=256, do_sample=False)\n",
    "        batch_times.append(time.time() - start)\n",
    "\n",
    "batch_fps = (batch_size * num_batches) / np.sum(batch_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "befdf788-8e05-446f-9ecd-1e1fb07bb09b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Final Evaluation Summary ===\n",
      "Model Size on Disk:               12.55\n",
      "BERTScore F1 (avg):               0.7252\n",
      "Location Precision:               0/16 = 0.0000\n",
      "Inference Latency (median):       7761.92 ms\n",
      "Inference Latency (95th pct):     8123.67 ms\n",
      "Inference Latency (99th pct):     159196.10 ms\n",
      "Inference Throughput (1 sample):  0.06 samples/sec\n",
      "Batch Throughput (4):  0.26 samples/sec\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== Final Evaluation Summary ===\")\n",
    "print(f\"Model Size on Disk:               {get_model_size(model):.2f}\")\n",
    "print(f\"BERTScore F1 (avg):               {F1.mean().item():.4f}\")\n",
    "print(f\"Location Precision:               {offset_match_correct}/{offset_match_total} = {offset_match_correct / offset_match_total:.4f}\")\n",
    "print(f\"Inference Latency (median):       {np.percentile(latencies, 50) * 1000:.2f} ms\")\n",
    "print(f\"Inference Latency (95th pct):     {np.percentile(latencies, 95) * 1000:.2f} ms\")\n",
    "print(f\"Inference Latency (99th pct):     {np.percentile(latencies, 99) * 1000:.2f} ms\")\n",
    "print(f\"Inference Throughput (1 sample):  {len(latencies) / np.sum(latencies):.2f} samples/sec\")\n",
    "print(f\"Batch Throughput ({batch_size}):  {batch_fps:.2f} samples/sec\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6280cc2-c9ba-4531-bf1a-8eceb0a8a5cc",
   "metadata": {},
   "source": [
    "### Converting to ONNX"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f30c06-36cf-4efe-ae85-19b92cd7fa60",
   "metadata": {},
   "source": [
    "The newer versions don't support Optimum, and with ORTModelForCausalLM, it exports the onnx model too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8eebd2ae-281f-40d4-9132-389fb4e39507",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cae0131b99f14bc4bd8b9041496589a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.12/site-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.12/site-packages/transformers/modeling_attn_mask_utils.py:114: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if (input_shape[-1] > 1 or self.sliding_window is not None) and self.is_causal:\n",
      "/opt/conda/lib/python3.12/site-packages/optimum/exporters/onnx/model_patcher.py:454: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if past_key_values_length > 0:\n",
      "/opt/conda/lib/python3.12/site-packages/transformers/models/llama/modeling_llama.py:150: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if seq_len > self.max_seq_len_cached:\n",
      "/opt/conda/lib/python3.12/site-packages/transformers/models/llama/modeling_llama.py:716: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if attention_mask.size() != (bsz, 1, q_len, kv_seq_len):\n",
      "In-place op on output of tensor.shape. See https://pytorch.org/docs/main/onnx.html#avoid-inplace-operations-when-using-tensor-shape-in-tracing-mode\n",
      "In-place op on output of tensor.shape. See https://pytorch.org/docs/main/onnx.html#avoid-inplace-operations-when-using-tensor-shape-in-tracing-mode\n",
      "In-place op on output of tensor.shape. See https://pytorch.org/docs/main/onnx.html#avoid-inplace-operations-when-using-tensor-shape-in-tracing-mode\n",
      "In-place op on output of tensor.shape. See https://pytorch.org/docs/main/onnx.html#avoid-inplace-operations-when-using-tensor-shape-in-tracing-mode\n",
      "In-place op on output of tensor.shape. See https://pytorch.org/docs/main/onnx.html#avoid-inplace-operations-when-using-tensor-shape-in-tracing-mode\n",
      "In-place op on output of tensor.shape. See https://pytorch.org/docs/main/onnx.html#avoid-inplace-operations-when-using-tensor-shape-in-tracing-mode\n",
      "In-place op on output of tensor.shape. See https://pytorch.org/docs/main/onnx.html#avoid-inplace-operations-when-using-tensor-shape-in-tracing-mode\n",
      "In-place op on output of tensor.shape. See https://pytorch.org/docs/main/onnx.html#avoid-inplace-operations-when-using-tensor-shape-in-tracing-mode\n",
      "In-place op on output of tensor.shape. See https://pytorch.org/docs/main/onnx.html#avoid-inplace-operations-when-using-tensor-shape-in-tracing-mode\n",
      "In-place op on output of tensor.shape. See https://pytorch.org/docs/main/onnx.html#avoid-inplace-operations-when-using-tensor-shape-in-tracing-mode\n",
      "In-place op on output of tensor.shape. See https://pytorch.org/docs/main/onnx.html#avoid-inplace-operations-when-using-tensor-shape-in-tracing-mode\n",
      "In-place op on output of tensor.shape. See https://pytorch.org/docs/main/onnx.html#avoid-inplace-operations-when-using-tensor-shape-in-tracing-mode\n",
      "In-place op on output of tensor.shape. See https://pytorch.org/docs/main/onnx.html#avoid-inplace-operations-when-using-tensor-shape-in-tracing-mode\n",
      "In-place op on output of tensor.shape. See https://pytorch.org/docs/main/onnx.html#avoid-inplace-operations-when-using-tensor-shape-in-tracing-mode\n",
      "In-place op on output of tensor.shape. See https://pytorch.org/docs/main/onnx.html#avoid-inplace-operations-when-using-tensor-shape-in-tracing-mode\n",
      "In-place op on output of tensor.shape. See https://pytorch.org/docs/main/onnx.html#avoid-inplace-operations-when-using-tensor-shape-in-tracing-mode\n",
      "In-place op on output of tensor.shape. See https://pytorch.org/docs/main/onnx.html#avoid-inplace-operations-when-using-tensor-shape-in-tracing-mode\n",
      "In-place op on output of tensor.shape. See https://pytorch.org/docs/main/onnx.html#avoid-inplace-operations-when-using-tensor-shape-in-tracing-mode\n",
      "In-place op on output of tensor.shape. See https://pytorch.org/docs/main/onnx.html#avoid-inplace-operations-when-using-tensor-shape-in-tracing-mode\n",
      "In-place op on output of tensor.shape. See https://pytorch.org/docs/main/onnx.html#avoid-inplace-operations-when-using-tensor-shape-in-tracing-mode\n",
      "In-place op on output of tensor.shape. See https://pytorch.org/docs/main/onnx.html#avoid-inplace-operations-when-using-tensor-shape-in-tracing-mode\n",
      "In-place op on output of tensor.shape. See https://pytorch.org/docs/main/onnx.html#avoid-inplace-operations-when-using-tensor-shape-in-tracing-mode\n",
      "In-place op on output of tensor.shape. See https://pytorch.org/docs/main/onnx.html#avoid-inplace-operations-when-using-tensor-shape-in-tracing-mode\n",
      "In-place op on output of tensor.shape. See https://pytorch.org/docs/main/onnx.html#avoid-inplace-operations-when-using-tensor-shape-in-tracing-mode\n",
      "In-place op on output of tensor.shape. See https://pytorch.org/docs/main/onnx.html#avoid-inplace-operations-when-using-tensor-shape-in-tracing-mode\n",
      "In-place op on output of tensor.shape. See https://pytorch.org/docs/main/onnx.html#avoid-inplace-operations-when-using-tensor-shape-in-tracing-mode\n",
      "In-place op on output of tensor.shape. See https://pytorch.org/docs/main/onnx.html#avoid-inplace-operations-when-using-tensor-shape-in-tracing-mode\n",
      "In-place op on output of tensor.shape. See https://pytorch.org/docs/main/onnx.html#avoid-inplace-operations-when-using-tensor-shape-in-tracing-mode\n",
      "In-place op on output of tensor.shape. See https://pytorch.org/docs/main/onnx.html#avoid-inplace-operations-when-using-tensor-shape-in-tracing-mode\n",
      "In-place op on output of tensor.shape. See https://pytorch.org/docs/main/onnx.html#avoid-inplace-operations-when-using-tensor-shape-in-tracing-mode\n",
      "In-place op on output of tensor.shape. See https://pytorch.org/docs/main/onnx.html#avoid-inplace-operations-when-using-tensor-shape-in-tracing-mode\n",
      "In-place op on output of tensor.shape. See https://pytorch.org/docs/main/onnx.html#avoid-inplace-operations-when-using-tensor-shape-in-tracing-mode\n",
      "/opt/conda/lib/python3.12/site-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONNX model saved to: models/llama_onnx//model.onnx\n"
     ]
    }
   ],
   "source": [
    "model_id = \"codellama/CodeLLaMA-7b-Instruct-hf\"  \n",
    "onnx_dir = \"models/llama_onnx/\"\n",
    "\n",
    "model = ORTModelForCausalLM.from_pretrained(\n",
    "    model_id, \n",
    "    export=True, \n",
    "    cache_dir=onnx_dir\n",
    ")\n",
    "\n",
    "model.save_pretrained(onnx_dir)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.save_pretrained(onnx_dir)\n",
    "\n",
    "print(f\"ONNX model saved to: {onnx_dir}/model.onnx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "73ea5c19-2dec-46ca-955e-dda896876d59",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'onnxruntime' has no attribute 'get_available_providers'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# !pip uninstall onnxruntime -y\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# !pip install onnxruntime-gpu\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mort\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_available_providers\u001b[49m())\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'onnxruntime' has no attribute 'get_available_providers'"
     ]
    }
   ],
   "source": [
    "# !pip uninstall onnxruntime -y\n",
    "# !pip install onnxruntime-gpu\n",
    "print(ort.get_available_providers())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4496440a-b983-4149-818d-800558015034",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'onnxruntime' has no attribute 'InferenceSession'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m ort_session \u001b[38;5;241m=\u001b[39m \u001b[43mort\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mInferenceSession\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodels/llama_onnx/model.onnx\u001b[39m\u001b[38;5;124m\"\u001b[39m, providers\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCUDAExecutionProvider\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'onnxruntime' has no attribute 'InferenceSession'"
     ]
    }
   ],
   "source": [
    "ort_session = ort.InferenceSession(\"models/llama_onnx/model.onnx\", providers=[\"CUDAExecutionProvider\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1ec6690c-1daf-401c-906e-71013696e2ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AzureExecutionProvider', 'CPUExecutionProvider']\n"
     ]
    }
   ],
   "source": [
    "# ort_session.get_providers()\n",
    "import onnxruntime as ort\n",
    "print(ort.get_available_providers())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1cdd910a-db5e-4214-8101-7ba87822fbc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun May 11 13:39:52 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 560.35.05              Driver Version: 560.35.05      CUDA Version: 12.6     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA A100 80GB PCIe          Off |   00000000:27:00.0 Off |                    0 |\n",
      "| N/A   45C    P0             48W /  300W |       1MiB /  81920MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5e945b84-8a3b-437d-a875-6a8e11e9e14c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "AttributeError: module 'onnxruntime' has no attribute 'get_available_providers'\n"
     ]
    }
   ],
   "source": [
    "!python -c \"import onnxruntime as ort; print(ort.get_available_providers())\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "98b5147f-0cb4-473f-b8a8-90f00259a1ae",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Asked to use CUDAExecutionProvider as an ONNX Runtime execution provider, but the available execution providers are ['AzureExecutionProvider', 'CPUExecutionProvider'].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m onnx_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodels/llama_onnx/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Load the ONNX model\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mORTModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43monnx_dir\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Load tokenizer\u001b[39;00m\n\u001b[1;32m      7\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(onnx_dir)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/optimum/onnxruntime/modeling_ort.py:335\u001b[0m, in \u001b[0;36mORTModel.to\u001b[0;34m(self, device)\u001b[0m\n\u001b[1;32m    332\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n\u001b[1;32m    334\u001b[0m provider \u001b[38;5;241m=\u001b[39m get_provider_for_device(device)\n\u001b[0;32m--> 335\u001b[0m \u001b[43mvalidate_provider_availability\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprovider\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# raise error if the provider is not available\u001b[39;00m\n\u001b[1;32m    337\u001b[0m \u001b[38;5;66;03m# IOBinding is only supported for CPU and CUDA Execution Providers.\u001b[39;00m\n\u001b[1;32m    338\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m device\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_use_io_binding \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m provider \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCUDAExecutionProvider\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/optimum/onnxruntime/utils.py:287\u001b[0m, in \u001b[0;36mvalidate_provider_availability\u001b[0;34m(provider)\u001b[0m\n\u001b[1;32m    285\u001b[0m available_providers \u001b[38;5;241m=\u001b[39m ort\u001b[38;5;241m.\u001b[39mget_available_providers()\n\u001b[1;32m    286\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m provider \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m available_providers:\n\u001b[0;32m--> 287\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    288\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAsked to use \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprovider\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m as an ONNX Runtime execution provider, but the available execution providers are \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mavailable_providers\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    289\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Asked to use CUDAExecutionProvider as an ONNX Runtime execution provider, but the available execution providers are ['AzureExecutionProvider', 'CPUExecutionProvider']."
     ]
    }
   ],
   "source": [
    "onnx_dir = \"models/llama_onnx/\"\n",
    "\n",
    "# Load the ONNX model\n",
    "model = ORTModelForCausalLM.from_pretrained(onnx_dir).to(\"cuda\")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(onnx_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b828ecf-f7d0-49c8-9b54-23b0330ccaa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "code_prompt = \"\"\"\n",
    "def fibonacci(n):\n",
    "    if n <= 0:\n",
    "        return []\n",
    "    elif n == 1:\n",
    "        return [0]\n",
    "    elif n == 2:\n",
    "        return [0, 1]\n",
    "    else:\n",
    "        seq = [0, 1]\n",
    "        for i in range(2, n):\n",
    "            seq.append(seq[-1] + seq[-2])\n",
    "        return seq\n",
    "\"\"\"\n",
    "\n",
    "prompt = f\"Given the code diff below, generate relevant code review comments in the format:\\n<COMMENT side=\\\"RIGHT\\\" offset=\\\"X\\\">...\\n\\n{code_prompt}\"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=4096).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f1438a8-d30e-42b6-ba35-c60ed7a25bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    output_ids = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=256,\n",
    "        do_sample=False,\n",
    "        pad_token_id=tokenizer.pad_token_id\n",
    "    )\n",
    "\n",
    "generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "print(\"Generated Review Comments:\\n\", generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f233aab9-549b-4f55-8e32-796434b855b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa7d283d-dfc9-483c-9fbc-3b4366f2b6c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_session(ort_session, input_array, batch_input):\n",
    "    print(f\"\\nExecution provider: {ort_session.get_providers()}\")\n",
    "    input_name = ort_session.get_inputs()[0].name\n",
    "\n",
    "    # Inference latency (single input)\n",
    "    num_trials = 10\n",
    "    ort_session.run(None, {input_name: input_array})  # Warm-up\n",
    "    latencies = []\n",
    "    for _ in range(num_trials):\n",
    "        start = time.time()\n",
    "        ort_session.run(None, {input_name: input_array})\n",
    "        latencies.append(time.time() - start)\n",
    "\n",
    "    print(f\"Inference Latency (median): {np.percentile(latencies, 50) * 1000:.2f} ms\")\n",
    "    print(f\"Inference Latency (95th pct): {np.percentile(latencies, 95) * 1000:.2f} ms\")\n",
    "    print(f\"Inference Latency (99th pct): {np.percentile(latencies, 99) * 1000:.2f} ms\")\n",
    "    print(f\"Inference Throughput (1 sample): {num_trials / np.sum(latencies):.2f} samples/sec\")\n",
    "\n",
    "    # Batch throughput\n",
    "    num_batches = 5\n",
    "    ort_session.run(None, {input_name: batch_input})  # Warm-up\n",
    "    batch_times = []\n",
    "    for _ in range(num_batches):\n",
    "        start = time.time()\n",
    "        ort_session.run(None, {input_name: batch_input})\n",
    "        batch_times.append(time.time() - start)\n",
    "    batch_fps = (batch_input.shape[0] * num_batches) / np.sum(batch_times)\n",
    "    print(f\"Batch Throughput ({batch_input.shape[0]}): {batch_fps:.2f} samples/sec\")\n",
    "\n",
    "    # Model size\n",
    "    model_size = os.path.getsize(ort_session._model_path)\n",
    "    print(f\"Model Size on Disk: {model_size / 1e6:.2f} MB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053ff48b-148a-46d4-91ec-4abb232173c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"Given the code diff below, generate review comments:{train_data[0]['input']}\"\n",
    "\n",
    "# Tokenize it\n",
    "inputs = tokenizer(prompt, return_tensors=\"np\", padding=\"max_length\", truncation=True, max_length=128)\n",
    "\n",
    "# Extract NumPy input\n",
    "input_array = inputs[\"input_ids\"]\n",
    "batch_input = np.repeat(input_array, repeats=4, axis=0)  # batch of 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "074062fd-87af-435d-9e7b-563b2b73d92a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e16b971-73df-48ff-9712-49c210202ec0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c1cab21e-d1c0-4336-8fa0-388de2da71d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: onnxruntime-gpu\n",
      "Version: 1.22.0\n",
      "Summary: ONNX Runtime is a runtime accelerator for Machine Learning models\n",
      "Home-page: https://onnxruntime.ai\n",
      "Author: Microsoft Corporation\n",
      "Author-email: onnxruntime@microsoft.com\n",
      "License: MIT License\n",
      "Location: /opt/conda/lib/python3.12/site-packages\n",
      "Requires: coloredlogs, flatbuffers, numpy, packaging, protobuf, sympy\n",
      "Required-by: \n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip show onnxruntime-gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c5213f7-05ad-477f-9f2f-c62818856e77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1f15b7-89e7-4ce1-98c3-81e0ff8f4a55",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
