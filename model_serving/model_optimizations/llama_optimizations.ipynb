{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609101f5-edea-41aa-bb80-67fd5fde8f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gzip, json, time\n",
    "import torch \n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from datasets import Dataset, load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForCausalLM, \n",
    "    TrainingArguments, \n",
    "    Trainer, \n",
    "    DataCollatorForLanguageModeling,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "from peft import get_peft_model, LoraConfig, TaskType, PeftModel, PeftConfig\n",
    "from bert_score import score\n",
    "import onnxruntime as ort\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import onnx\n",
    "import onnxruntime as ort\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "from bert_score import score as bert_score\n",
    "import neural_compressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60c1c905-6bcd-4c5f-9b2e-48aab4e05361",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"codellama/CodeLlama-7b-Instruct-hf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f8e04a2-c735-4113-87a4-d5829364f4a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc2df3231ad24fc3bedc0bf0a8cff2f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"codellama/CodeLlama-7b-Instruct-hf\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model = AutoModelForCausalLM.from_pretrained(\"codellama/CodeLlama-7b-Instruct-hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b61073e-8434-4354-9cc9-d9f96b8980c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_size(model):\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    dtype_size = torch.finfo(model.dtype).bits // 8\n",
    "    \n",
    "    model_size_gb = total_params * dtype_size / (1024 ** 3)\n",
    "    # print(f\"Model size: {model_size_gb:2f} GB\")\n",
    "    return model_size_gb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "92a5638b-c6f5-4b57-ae13-bc71af4bfbdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25.103042602539062"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_model_size(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b98fb771-3636-4796-aedb-f98f2e79656b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c1d4467ece845b0a54667e1e752538e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"codellama/CodeLlama-7b-Instruct-hf\",\n",
    "    torch_dtype=torch.float16,\n",
    "    # device_map={\"\": 0},\n",
    "    # low_cpu_mem_usage=True\n",
    "    device_map=\"cuda:0\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e3320a68-d7a8-476b-835c-455938e706b1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon May 12 02:31:32 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.144.03             Driver Version: 550.144.03     CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA H100 80GB HBM3          On  |   00000000:0F:00.0 Off |                    0 |\n",
      "| N/A   29C    P0            114W /  700W |   13384MiB /  81559MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "211e08b0-05b7-4bc0-9417-3b4e71982177",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_path = \"/mnt/object_group/data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b75590c9-22a8-4aad-95fc-3d67c60cd386",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    {\"comment_id\": 74291026, \"comment_user_login\": \"vikerman\", \"comment_body\": \"Is there a better way to avoid having this in different places?\\n\", \"comment_created_at\": \"2016-08-10T17:30:42+00:00\", \"comment_html_url\": \"https://github.com/angular/angular/pull/10620#discussion_r74291026\", \"comment_path\": \"modules/@angular/common/src/forms-deprecated/directives/abstract_control_directive.ts\", \"comment_position\": 10, \"comment_original_position\": 10, \"comment_commit_id\": \"ee8e802b7bc25eafbe54109b3924e9dbc36dce11\", \"comment_original_commit_id\": \"ee8e802b7bc25eafbe54109b3924e9dbc36dce11\", \"diff_line_content\": \"}\\n\", \"diff_line_type\": \"added\", \"diff_line_source_no\": null, \"diff_line_target_no\": 15, \"diff_hunk_header\": \"\", \"diff\": \"@@ -6,10 +6,13 @@\\n  * found in the LICENSE file at https://angular.io/license\\n  */\\n \\n-import {unimplemented} from '../../facade/exceptions';\\n+import {BaseException} from '@angular/core';\\n import {isPresent} from '../../facade/lang';\\n import {AbstractControl} from '../model';\\n \\n+function unimplemented(): any {\\n+  throw new BaseException('unimplemented');\", \"side\": \"RIGHT\", \"line_offset\": 9, \"diff_file_source\": \"a/modules/@angular/common/src/forms-deprecated/directives/abstract_control_directive.ts\", \"diff_file_target\": \"b/modules/@angular/common/src/forms-deprecated/directives/abstract_control_directive.ts\", \"repo\": \"angular/angular\"}\n",
      "    {\"comment_id\": 74305223, \"comment_user_login\": \"alexeagle\", \"comment_body\": \"we could export it in the public API but that's not nice.\\nOr we could inline the method and delete it.\\n\", \"comment_created_at\": \"2016-08-10T18:42:59+00:00\", \"comment_html_url\": \"https://github.com/angular/angular/pull/10620#discussion_r74305223\", \"comment_path\": \"modules/@angular/common/src/forms-deprecated/directives/abstract_control_directive.ts\", \"comment_position\": 10, \"comment_original_position\": 10, \"comment_commit_id\": \"ee8e802b7bc25eafbe54109b3924e9dbc36dce11\", \"comment_original_commit_id\": \"ee8e802b7bc25eafbe54109b3924e9dbc36dce11\", \"diff_line_content\": \"}\\n\", \"diff_line_type\": \"added\", \"diff_line_source_no\": null, \"diff_line_target_no\": 15, \"diff_hunk_header\": \"\", \"diff\": \"@@ -6,10 +6,13 @@\\n  * found in the LICENSE file at https://angular.io/license\\n  */\\n \\n-import {unimplemented} from '../../facade/exceptions';\\n+import {BaseException} from '@angular/core';\\n import {isPresent} from '../../facade/lang';\\n import {AbstractControl} from '../model';\\n \\n+function unimplemented(): any {\\n+  throw new BaseException('unimplemented');\", \"side\": \"RIGHT\", \"line_offset\": 9, \"diff_file_source\": \"a/modules/@angular/common/src/forms-deprecated/directives/abstract_control_directive.ts\", \"diff_file_target\": \"b/modules/@angular/common/src/forms-deprecated/directives/abstract_control_directive.ts\", \"repo\": \"angular/angular\"}\n",
      "    {\"comment_id\": 74523490, \"comment_user_login\": \"vicb\", \"comment_body\": \"const\\n\", \"comment_created_at\": \"2016-08-11T23:43:20+00:00\", \"comment_html_url\": \"https://github.com/angular/angular/pull/10622#discussion_r74523490\", \"comment_path\": \"modules/@angular/compiler-cli/integrationtest/test/basic_spec.ts\", \"comment_position\": 36, \"comment_original_position\": 36, \"comment_commit_id\": \"616c5c56924b77bc0ef539d01d1cf3596c0da87f\", \"comment_original_commit_id\": \"91f50b2f513f99cf56b7406b3a74066e128b9c7d\", \"diff_line_content\": \"    it('should support i18n for content tags', () => {\\n\", \"diff_line_type\": \"added\", \"diff_line_source_no\": null, \"diff_line_target_no\": 79, \"diff_hunk_header\": \"describe('template codegen output', () => {\", \"diff\": \"@@ -44,24 +44,40 @@ describe('template codegen output', () => {\\n   it('should support ngIf', () => {\\n     var compFixture = createComponent(BasicComp);\\n     var debugElement = compFixture.debugElement;\\n-    expect(debugElement.children.length).toBe(2);\\n+    expect(debugElement.children.length).toBe(3);\\n \\n     compFixture.componentInstance.ctxBool = true;\\n     compFixture.detectChanges();\\n-    expect(debugElement.children.length).toBe(3);\\n+    expect(debugElement.children.length).toBe(4);\\n     expect(debugElement.children[2].injector.get(MultipleComponentsMyComp)).toBeTruthy();\\n   });\\n \\n   it('should support ngFor', () => {\\n     var compFixture = createComponent(BasicComp);\\n     var debugElement = compFixture.debugElement;\\n-    expect(debugElement.children.length).toBe(2);\\n+    expect(debugElement.children.length).toBe(3);\\n \\n     // test NgFor\\n     compFixture.componentInstance.ctxArr = [1, 2];\\n     compFixture.detectChanges();\\n-    expect(debugElement.children.length).toBe(4);\\n+    expect(debugElement.children.length).toBe(5);\\n     expect(debugElement.children[2].attributes['value']).toBe('1');\\n     expect(debugElement.children[3].attributes['value']).toBe('2');\\n   });\\n+\\n+  describe('i18n', () => {\\n+    it('should support i18n for content tags', () => {\\n+      var compFixture = createComponent(BasicComp);\\n+      expect(compFixture.componentInstance.ctxLocale).toEqual('FI');\\n+    });\\n+\\n+    it('should support i18n for content tags', () => {\\n+      var compFixture = createComponent(BasicComp);\", \"side\": \"RIGHT\", \"line_offset\": 35, \"diff_file_source\": \"a/modules/@angular/compiler-cli/integrationtest/test/basic_spec.ts\", \"diff_file_target\": \"b/modules/@angular/compiler-cli/integrationtest/test/basic_spec.ts\", \"repo\": \"angular/angular\"}\n",
      "    {\"comment_id\": 74523504, \"comment_user_login\": \"vicb\", \"comment_body\": \"const\\n\", \"comment_created_at\": \"2016-08-11T23:43:27+00:00\", \"comment_html_url\": \"https://github.com/angular/angular/pull/10622#discussion_r74523504\", \"comment_path\": \"modules/@angular/compiler-cli/integrationtest/test/basic_spec.ts\", \"comment_position\": 31, \"comment_original_position\": 31, \"comment_commit_id\": \"616c5c56924b77bc0ef539d01d1cf3596c0da87f\", \"comment_original_commit_id\": \"91f50b2f513f99cf56b7406b3a74066e128b9c7d\", \"diff_line_content\": \"    it('should inject the translations format into the component', () => {\\n\", \"diff_line_type\": \"added\", \"diff_line_source_no\": null, \"diff_line_target_no\": 74, \"diff_hunk_header\": \"describe('template codegen output', () => {\", \"diff\": \"@@ -44,24 +44,40 @@ describe('template codegen output', () => {\\n   it('should support ngIf', () => {\\n     var compFixture = createComponent(BasicComp);\\n     var debugElement = compFixture.debugElement;\\n-    expect(debugElement.children.length).toBe(2);\\n+    expect(debugElement.children.length).toBe(3);\\n \\n     compFixture.componentInstance.ctxBool = true;\\n     compFixture.detectChanges();\\n-    expect(debugElement.children.length).toBe(3);\\n+    expect(debugElement.children.length).toBe(4);\\n     expect(debugElement.children[2].injector.get(MultipleComponentsMyComp)).toBeTruthy();\\n   });\\n \\n   it('should support ngFor', () => {\\n     var compFixture = createComponent(BasicComp);\\n     var debugElement = compFixture.debugElement;\\n-    expect(debugElement.children.length).toBe(2);\\n+    expect(debugElement.children.length).toBe(3);\\n \\n     // test NgFor\\n     compFixture.componentInstance.ctxArr = [1, 2];\\n     compFixture.detectChanges();\\n-    expect(debugElement.children.length).toBe(4);\\n+    expect(debugElement.children.length).toBe(5);\\n     expect(debugElement.children[2].attributes['value']).toBe('1');\\n     expect(debugElement.children[3].attributes['value']).toBe('2');\\n   });\\n+\\n+  describe('i18n', () => {\\n+    it('should support i18n for content tags', () => {\\n+      var compFixture = createComponent(BasicComp);\", \"side\": \"RIGHT\", \"line_offset\": 30, \"diff_file_source\": \"a/modules/@angular/compiler-cli/integrationtest/test/basic_spec.ts\", \"diff_file_target\": \"b/modules/@angular/compiler-cli/integrationtest/test/basic_spec.ts\", \"repo\": \"angular/angular\"}\n",
      "    {\"comment_id\": 74526501, \"comment_user_login\": \"vicb\", \"comment_body\": \"fi\\n\", \"comment_created_at\": \"2016-08-12T00:18:35+00:00\", \"comment_html_url\": \"https://github.com/angular/angular/pull/10622#discussion_r74526501\", \"comment_path\": \"modules/@angular/compiler-cli/integrationtest/test/basic_spec.ts\", \"comment_position\": 32, \"comment_original_position\": 32, \"comment_commit_id\": \"616c5c56924b77bc0ef539d01d1cf3596c0da87f\", \"comment_original_commit_id\": \"7333b4c6d9527a1a6b812ab6eea2797f228d03bd\", \"diff_line_content\": \"      const compFixture = createComponent(BasicComp);\\n\", \"diff_line_type\": \"added\", \"diff_line_source_no\": null, \"diff_line_target_no\": 75, \"diff_hunk_header\": \"describe('template codegen output', () => {\", \"diff\": \"@@ -44,24 +44,45 @@ describe('template codegen output', () => {\\n   it('should support ngIf', () => {\\n     var compFixture = createComponent(BasicComp);\\n     var debugElement = compFixture.debugElement;\\n-    expect(debugElement.children.length).toBe(2);\\n+    expect(debugElement.children.length).toBe(3);\\n \\n     compFixture.componentInstance.ctxBool = true;\\n     compFixture.detectChanges();\\n-    expect(debugElement.children.length).toBe(3);\\n+    expect(debugElement.children.length).toBe(4);\\n     expect(debugElement.children[2].injector.get(MultipleComponentsMyComp)).toBeTruthy();\\n   });\\n \\n   it('should support ngFor', () => {\\n     var compFixture = createComponent(BasicComp);\\n     var debugElement = compFixture.debugElement;\\n-    expect(debugElement.children.length).toBe(2);\\n+    expect(debugElement.children.length).toBe(3);\\n \\n     // test NgFor\\n     compFixture.componentInstance.ctxArr = [1, 2];\\n     compFixture.detectChanges();\\n-    expect(debugElement.children.length).toBe(4);\\n+    expect(debugElement.children.length).toBe(5);\\n     expect(debugElement.children[2].attributes['value']).toBe('1');\\n     expect(debugElement.children[3].attributes['value']).toBe('2');\\n   });\\n+\\n+  describe('i18n', () => {\\n+    it('should inject the locale into the component', () => {\\n+      const compFixture = createComponent(BasicComp);\\n+      expect(compFixture.componentInstance.ctxLocale).toEqual('FI');\", \"side\": \"RIGHT\", \"line_offset\": 31, \"diff_file_source\": \"a/modules/@angular/compiler-cli/integrationtest/test/basic_spec.ts\", \"diff_file_target\": \"b/modules/@angular/compiler-cli/integrationtest/test/basic_spec.ts\", \"repo\": \"angular/angular\"}\n",
      "    {\"comment_id\": 74526531, \"comment_user_login\": \"vicb\", \"comment_body\": \"remove !\\n\", \"comment_created_at\": \"2016-08-12T00:19:06+00:00\", \"comment_html_url\": \"https://github.com/angular/angular/pull/10622#discussion_r74526531\", \"comment_path\": \"modules/@angular/compiler-cli/src/codegen.ts\", \"comment_position\": 36, \"comment_original_position\": 36, \"comment_commit_id\": \"616c5c56924b77bc0ef539d01d1cf3596c0da87f\", \"comment_original_commit_id\": \"7333b4c6d9527a1a6b812ab6eea2797f228d03bd\", \"diff_line_content\": \"    const reflectorHost = new ReflectorHost(program, compilerHost, options, reflectorHostContext);\\n\", \"diff_line_type\": \"context\", \"diff_line_source_no\": 132, \"diff_line_target_no\": 142, \"diff_hunk_header\": \"export class CodeGenerator {\", \"diff\": \"@@ -88,6 +93,35 @@ export class CodeGenerator {\\n     return path.join(this.options.genDir, relativePath);\\n   }\\n \\n+  private _constructTranslationBundle(fileMetas: any[], analyzedNgModules: any): Promise<any> {\", \"side\": \"RIGHT\", \"line_offset\": 11, \"diff_file_source\": \"a/modules/@angular/compiler-cli/src/codegen.ts\", \"diff_file_target\": \"b/modules/@angular/compiler-cli/src/codegen.ts\", \"repo\": \"angular/angular\"}\n"
     ]
    }
   ],
   "source": [
    "train = \"train.jsonl.gz\"\n",
    "with gzip.open(train, 'rt', encoding='utf-8') as f:\n",
    "    for i, line in enumerate(f):\n",
    "        print(\"    \" + line.strip())\n",
    "        if i >= 5:\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "63ceac15-4e09-483e-8acf-e8c194d643e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_ROOT = Path(\"./\")  # Adjust as needed\n",
    "\n",
    "def load_samples(limit_per_split=10):\n",
    "    diff_samples = []\n",
    "    for repo_dir in DATA_ROOT.iterdir():\n",
    "        if not repo_dir.is_dir():\n",
    "            continue\n",
    "        for pr_dir in (repo_dir / \"diff\").iterdir():\n",
    "            pr_id = pr_dir.name\n",
    "            diff_file = pr_dir\n",
    "            comments_file = DATA_ROOT / repo_dir.name / \"comments\" / f\"{repo_dir.name}_{pr_id}_comments.jsonl\"\n",
    "\n",
    "            if not diff_file.exists() or not comments_file.exists():\n",
    "                continue\n",
    "\n",
    "            with open(diff_file, \"r\", encoding=\"utf-8\") as df:\n",
    "                diff_content = df.read()\n",
    "\n",
    "            with open(comments_file, \"r\", encoding=\"utf-8\") as cf:\n",
    "                for line in cf:\n",
    "                    try:\n",
    "                        comment = json.loads(line)\n",
    "                        offset = comment.get(\"original_position\")\n",
    "                        side = comment.get(\"side\", \"RIGHT\")\n",
    "                        body = comment.get(\"body\", \"\").strip()\n",
    "                        if offset is not None and body:\n",
    "                            sample = {\n",
    "                                \"input\": f\"<DIFF>\\n{diff_content}\\n</DIFF>\\n<COMMENT side=\\\"{side}\\\" offset=\\\"{offset}\\\">\",\n",
    "                                \"output\": body\n",
    "                            }\n",
    "                            diff_samples.append(sample)\n",
    "                    except json.JSONDecodeError:\n",
    "                        continue\n",
    "\n",
    "    random.shuffle(diff_samples)\n",
    "    return {\n",
    "        \"train\": diff_samples[:limit_per_split],\n",
    "        \"test\": diff_samples[limit_per_split:2*limit_per_split],\n",
    "        \"eval\": diff_samples[2*limit_per_split:3*limit_per_split],\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0c22430a-7941-4d00-8ce3-66d7998c4c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_jsonl_gz(file_path, limit):\n",
    "    samples = []\n",
    "    with gzip.open(file_path, 'rt', encoding='utf-8') as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if i >= limit:\n",
    "                break\n",
    "            entry = json.loads(line)\n",
    "            prompt = f\"<DIFF>\\n{entry['diff']}\\n</DIFF>\\n\"\n",
    "            comment = f\"<COMMENT side=\\\"{entry['side']}\\\" offset=\\\"{entry['line_offset']}\\\">{entry['comment_body']}\"\n",
    "            samples.append({'input': prompt, 'label': comment})\n",
    "    return samples\n",
    "\n",
    "train_data = load_jsonl_gz(\"train.jsonl.gz\", 1000000000)\n",
    "test_data  = load_jsonl_gz(\"test.jsonl.gz\", 1000000000)\n",
    "eval_data  = load_jsonl_gz(\"val.jsonl.gz\", 1000000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "29b90dde-203f-42dc-8ea6-2796f6fa60db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8952\n",
      "8308\n"
     ]
    }
   ],
   "source": [
    "print(len(test_data))\n",
    "print(len(eval_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "983dcadf-1621-44d5-9656-e90ab7b3ae1a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input', 'label'])\n",
      "<DIFF>\n",
      "@@ -6,10 +6,13 @@\n",
      "  * found in the LICENSE file at https://angular.io/license\n",
      "  */\n",
      " \n",
      "-import {unimplemented} from '../../facade/exceptions';\n",
      "+import {BaseException} from '@angular/core';\n",
      " import {isPresent} from '../../facade/lang';\n",
      " import {AbstractControl} from '../model';\n",
      " \n",
      "+function unimplemented(): any {\n",
      "+  throw new BaseException('unimplemented');\n",
      "</DIFF>\n",
      "\n",
      "<COMMENT side=\"RIGHT\" offset=\"9\">Is there a better way to avoid having this in different places?\n",
      "\n",
      "<DIFF>\n",
      "@@ -6,10 +6,13 @@\n",
      "  * found in the LICENSE file at https://angular.io/license\n",
      "  */\n",
      " \n",
      "-import {unimplemented} from '../../facade/exceptions';\n",
      "+import {BaseException} from '@angular/core';\n",
      " import {isPresent} from '../../facade/lang';\n",
      " import {AbstractControl} from '../model';\n",
      " \n",
      "+function unimplemented(): any {\n",
      "+  throw new BaseException('unimplemented');\n",
      "</DIFF>\n",
      "\n",
      "<COMMENT side=\"RIGHT\" offset=\"9\">we could export it in the public API but that's not nice.\n",
      "Or we could inline the method and delete it.\n",
      "\n",
      "<DIFF>\n",
      "@@ -44,24 +44,40 @@ describe('template codegen output', () => {\n",
      "   it('should support ngIf', () => {\n",
      "     var compFixture = createComponent(BasicComp);\n",
      "     var debugElement = compFixture.debugElement;\n",
      "-    expect(debugElement.children.length).toBe(2);\n",
      "+    expect(debugElement.children.length).toBe(3);\n",
      " \n",
      "     compFixture.componentInstance.ctxBool = true;\n",
      "     compFixture.detectChanges();\n",
      "-    expect(debugElement.children.length).toBe(3);\n",
      "+    expect(debugElement.children.length).toBe(4);\n",
      "     expect(debugElement.children[2].injector.get(MultipleComponentsMyComp)).toBeTruthy();\n",
      "   });\n",
      " \n",
      "   it('should support ngFor', () => {\n",
      "     var compFixture = createComponent(BasicComp);\n",
      "     var debugElement = compFixture.debugElement;\n",
      "-    expect(debugElement.children.length).toBe(2);\n",
      "+    expect(debugElement.children.length).toBe(3);\n",
      " \n",
      "     // test NgFor\n",
      "     compFixture.componentInstance.ctxArr = [1, 2];\n",
      "     compFixture.detectChanges();\n",
      "-    expect(debugElement.children.length).toBe(4);\n",
      "+    expect(debugElement.children.length).toBe(5);\n",
      "     expect(debugElement.children[2].attributes['value']).toBe('1');\n",
      "     expect(debugElement.children[3].attributes['value']).toBe('2');\n",
      "   });\n",
      "+\n",
      "+  describe('i18n', () => {\n",
      "+    it('should support i18n for content tags', () => {\n",
      "+      var compFixture = createComponent(BasicComp);\n",
      "+      expect(compFixture.componentInstance.ctxLocale).toEqual('FI');\n",
      "+    });\n",
      "+\n",
      "+    it('should support i18n for content tags', () => {\n",
      "+      var compFixture = createComponent(BasicComp);\n",
      "</DIFF>\n",
      "\n",
      "<COMMENT side=\"RIGHT\" offset=\"35\">const\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(train_data[0].keys())\n",
    "\n",
    "for i in range(3):\n",
    "    print(train_data[i][\"input\"])\n",
    "    print(train_data[i][\"label\"])\n",
    "# print(train_data[0][\"input\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a6e9d09b-82b8-486b-9f30-f5270629666f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt template\n",
    "instruction = (\n",
    "    \"You are a helpful code reviewer. \"\n",
    "    \"Given a code diff, generate all relevant review comments. \"\n",
    "    \"Each comment must be in the format: \"\n",
    "    \"<COMMENT side=\\\"RIGHT\\\" offset=\\\"X\\\">Your comment here.\\n\"\n",
    "    \"Only output comments, nothing else.\\n\\n\"\n",
    "    \"### Code Diff:\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "62aff53f-c803-4e5b-82e1-d36d2aff2c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_comments(text):\n",
    "    pattern = re.compile(r'<COMMENT side=\"(?P<side>[^\"]+)\" offset=\"(?P<offset>\\d+)\">(?P<comment>.+)')\n",
    "    results = []\n",
    "    for line in text.strip().splitlines():\n",
    "        match = pattern.match(line.strip())\n",
    "        if match:\n",
    "            results.append({\n",
    "                \"side\": match.group(\"side\"),\n",
    "                \"offset\": int(match.group(\"offset\")),\n",
    "                \"comment\": match.group(\"comment\").strip()\n",
    "            })\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "94ac0f91-3db2-4856-9b29-f9d6a0317fab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32016, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32016, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8a32b9cf-f2de-4cb1-8023-fd55d538e9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def offset_match(pred_comments, true_comments, window=3):\n",
    "    correct = 0\n",
    "    total = len(pred_comments)\n",
    "\n",
    "    gt_pairs = {(c[\"side\"], c[\"offset\"]) for c in true_comments}\n",
    "    matched = set()\n",
    "\n",
    "    for pred in pred_comments:\n",
    "        pred_side, pred_offset = pred[\"side\"], pred[\"offset\"]\n",
    "        for gt in gt_pairs:\n",
    "            gt_side, gt_offset = gt\n",
    "            if pred_side == gt_side and abs(pred_offset - gt_offset) <= window:\n",
    "                matched.add(gt)\n",
    "                correct += 1\n",
    "                break  # count each pred only once\n",
    "\n",
    "    return correct, total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ce267183-140b-472c-9f0f-744065b1cf83",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/30 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "  3%|▎         | 1/30 [00:05<02:39,  5.51s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "  7%|▋         | 2/30 [00:11<02:34,  5.51s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 10%|█         | 3/30 [00:14<01:59,  4.41s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 13%|█▎        | 4/30 [00:19<02:06,  4.85s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 17%|█▋        | 5/30 [00:25<02:07,  5.09s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 20%|██        | 6/30 [00:30<02:06,  5.25s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 23%|██▎       | 7/30 [00:36<02:02,  5.34s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 27%|██▋       | 8/30 [00:41<01:58,  5.39s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 30%|███       | 9/30 [00:47<01:54,  5.43s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 33%|███▎      | 10/30 [00:52<01:49,  5.45s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 37%|███▋      | 11/30 [00:58<01:43,  5.47s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 40%|████      | 12/30 [01:03<01:38,  5.48s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 43%|████▎     | 13/30 [01:09<01:33,  5.49s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 47%|████▋     | 14/30 [01:14<01:27,  5.50s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 50%|█████     | 15/30 [01:20<01:22,  5.50s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 53%|█████▎    | 16/30 [01:24<01:11,  5.08s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 57%|█████▋    | 17/30 [01:29<01:07,  5.21s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 60%|██████    | 18/30 [01:35<01:03,  5.30s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 63%|██████▎   | 19/30 [01:40<00:58,  5.36s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 67%|██████▋   | 20/30 [01:45<00:52,  5.22s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 70%|███████   | 21/30 [01:50<00:46,  5.12s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 73%|███████▎  | 22/30 [01:56<00:41,  5.24s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 77%|███████▋  | 23/30 [02:01<00:37,  5.32s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 80%|████████  | 24/30 [02:07<00:32,  5.38s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 83%|████████▎ | 25/30 [02:12<00:27,  5.42s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 87%|████████▋ | 26/30 [02:18<00:21,  5.44s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 90%|█████████ | 27/30 [02:23<00:16,  5.46s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 93%|█████████▎| 28/30 [02:29<00:10,  5.47s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 97%|█████████▋| 29/30 [02:34<00:05,  5.48s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "100%|██████████| 30/30 [02:40<00:00,  5.34s/it]\n"
     ]
    }
   ],
   "source": [
    "references = []\n",
    "candidates = []\n",
    "offset_match_total = 0\n",
    "offset_match_correct = 0\n",
    "latencies = []\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./models/llama_onnx\")\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "for i, sample in enumerate(tqdm(test_data[:30])):  # Full test set if needed\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        [{\"role\": \"user\", \"content\": instruction + sample[\"input\"]}],\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "    inputs = tokenizer(\n",
    "        prompt,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=4096\n",
    "    ).to(\"cuda:0\")\n",
    "\n",
    "    true_comments = extract_comments(sample[\"label\"])\n",
    "\n",
    "    # Measure inference latency\n",
    "    with torch.no_grad():\n",
    "        start = time.time()\n",
    "        output_ids = model.generate(**inputs, max_new_tokens=256, do_sample=False)\n",
    "        latencies.append(time.time() - start)\n",
    "\n",
    "    # Decode and evaluate\n",
    "    generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True).strip()\n",
    "    generated_text = generated_text.split(\"Setting `pad_token_id\")[0].strip()\n",
    "    # print(f\"\\n=== Sample {i} ===\\n{generated_text}\")\n",
    "\n",
    "    pred_comments = extract_comments(generated_text)\n",
    "\n",
    "    for gt in true_comments:\n",
    "        references.append(gt[\"comment\"])\n",
    "        best_pred = max(pred_comments, key=lambda p: p[\"comment\"], default={\"comment\": \"\"})\n",
    "        # print(best_pred)\n",
    "        candidates.append(best_pred[\"comment\"] if best_pred else \"\")\n",
    "\n",
    "    correct, total = offset_match(pred_comments, true_comments, window=3)\n",
    "    offset_match_correct += correct\n",
    "    offset_match_total += total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6d1779f3-0812-4afb-abfd-220aee4f6662",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Inference Latency Stats ===\n",
      "Median latency:        5502.59 ms\n",
      "95th percentile:       5514.66 ms\n",
      "99th percentile:       5544.22 ms\n",
      "Throughput (1 sample): 0.19 samples/sec\n"
     ]
    }
   ],
   "source": [
    "# Latency stats\n",
    "print(\"\\n=== Inference Latency Stats ===\")\n",
    "print(f\"Median latency:        {np.percentile(latencies, 50) * 1000:.2f} ms\")\n",
    "print(f\"95th percentile:       {np.percentile(latencies, 95) * 1000:.2f} ms\")\n",
    "print(f\"99th percentile:       {np.percentile(latencies, 99) * 1000:.2f} ms\")\n",
    "print(f\"Throughput (1 sample): {len(latencies) / np.sum(latencies):.2f} samples/sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b312f7bd-c9f5-4364-9b4c-4a3a92f58961",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d48fcc3e11e43b2997a761cc56947af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dda69e169deb4fbe83275fa77be42316",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 0.21 seconds, 146.01 sentences/sec\n",
      "\n",
      "Evaluation Results\n",
      "Avg BERTScore F1: 0.8023\n",
      "Offset Precision: 2/98 = 0.0204\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n"
     ]
    }
   ],
   "source": [
    "# BERTScore\n",
    "P, R, F1 = bert_score(candidates, references, lang=\"en\", verbose=True)\n",
    "print(\"\\nEvaluation Results\")\n",
    "print(f\"Avg BERTScore F1: {F1.mean().item():.4f}\")\n",
    "print(f\"Offset Precision: {offset_match_correct}/{offset_match_total} = {offset_match_correct/offset_match_total:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8aea87c-43d2-4f62-b2c0-9c7794fff44c",
   "metadata": {},
   "source": [
    "# Batch throughput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1d882c78-56df-4717-8ee8-8a355fb0cab4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "batch_size = 4\n",
    "num_batches = 10\n",
    "batch_inputs = [test_data[i][\"input\"] for i in range(batch_size)]\n",
    "batch_prompts = [\n",
    "    tokenizer.apply_chat_template(\n",
    "        [{\"role\": \"user\", \"content\": instruction + inp}],\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    for inp in batch_inputs\n",
    "]\n",
    "\n",
    "encoded = tokenizer(\n",
    "    batch_prompts,\n",
    "    return_tensors=\"pt\",\n",
    "    padding=\"max_length\",\n",
    "    truncation=True,\n",
    "    max_length=4096\n",
    ").to(\"cuda:0\")\n",
    "\n",
    "# Warm-up\n",
    "with torch.no_grad():\n",
    "    _ = model.generate(**encoded, max_new_tokens=256, do_sample=False)\n",
    "\n",
    "batch_times = []\n",
    "with torch.no_grad():\n",
    "    for _ in range(num_batches):\n",
    "        start = time.time()\n",
    "        _ = model.generate(**encoded, max_new_tokens=256, do_sample=False)\n",
    "        batch_times.append(time.time() - start)\n",
    "\n",
    "batch_fps = (batch_size * num_batches) / np.sum(batch_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "07a9abff-d717-43b6-893e-73e05beab6a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Final Evaluation Summary ===\n",
      "Model Size on Disk:               12.55\n",
      "BERTScore F1 (avg):               0.8023\n",
      "Location Precision:               2/98 = 0.0204\n",
      "Inference Latency (median):       5502.59 ms\n",
      "Inference Latency (95th pct):     5514.66 ms\n",
      "Inference Latency (99th pct):     5544.22 ms\n",
      "Inference Throughput (1 sample):  0.19 samples/sec\n",
      "Batch Throughput (4):  0.44 samples/sec\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== Final Evaluation Summary ===\")\n",
    "print(f\"Model Size on Disk:               {get_model_size(model):.2f}\")\n",
    "print(f\"BERTScore F1 (avg):               {F1.mean().item():.4f}\")\n",
    "print(f\"Location Precision:               {offset_match_correct}/{offset_match_total} = {offset_match_correct / offset_match_total:.4f}\")\n",
    "print(f\"Inference Latency (median):       {np.percentile(latencies, 50) * 1000:.2f} ms\")\n",
    "print(f\"Inference Latency (95th pct):     {np.percentile(latencies, 95) * 1000:.2f} ms\")\n",
    "print(f\"Inference Latency (99th pct):     {np.percentile(latencies, 99) * 1000:.2f} ms\")\n",
    "print(f\"Inference Throughput (1 sample):  {len(latencies) / np.sum(latencies):.2f} samples/sec\")\n",
    "print(f\"Batch Throughput ({batch_size}):  {batch_fps:.2f} samples/sec\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0d71a166-11e2-4895-908e-d403d3ee3735",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon May 12 02:39:32 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.144.03             Driver Version: 550.144.03     CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA H100 80GB HBM3          On  |   00000000:0F:00.0 Off |                    0 |\n",
      "| N/A   42C    P0            382W /  700W |   35146MiB /  81559MiB |     98%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "212e1be4-95f6-485a-9286-e5af78790ccf",
   "metadata": {},
   "source": [
    "### Converting to ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2c6a1375-9b5f-4cfd-b8f2-76156d33845d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./models/llama_onnx/tokenizer_config.json',\n",
       " './models/llama_onnx/special_tokens_map.json',\n",
       " './models/llama_onnx/tokenizer.json')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained(\"./models/llama_onnx\")\n",
    "# tokenizer.save_pretrained(\"mnt/object/llama_onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8d28623e-5c1d-463c-90a1-32bf86e431c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32016, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32016, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dbb83c77-2266-420b-85ca-dc0d2fb96b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_review_prompt(diff_text: str) -> str:\n",
    "    instruction = (\n",
    "        \"You are a helpful code reviewer. \"\n",
    "        \"Given a code diff, generate all relevant review comments. \"\n",
    "        \"Each comment must be in the format: \"\n",
    "        \"<COMMENT side=\\\"RIGHT\\\" offset=\\\"X\\\">Your comment here.\\n\"\n",
    "        \"Only output comments, nothing else.\\n\\n\"\n",
    "        \"### Code Diff:\\n\"\n",
    "    )\n",
    "    \n",
    "    # Use the same tokenizer chat template as in inference\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        [{\"role\": \"user\", \"content\": instruction + diff_text}],\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    return prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "310f1733-066e-4897-91ed-6869860b06fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be4d535bec7545b09f4bf4da8d9097c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:1089: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if sequence_length != 1:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Export complete (GPU path)\n"
     ]
    }
   ],
   "source": [
    "# Load directly onto the first CUDA device\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16,      # halve VRAM use\n",
    "    device_map={\"\": 0},             # force everything to cuda:0\n",
    "    use_cache=False                 # strip DynamicCache to keep export simple\n",
    ").eval()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Dummy input on GPU\n",
    "inputs = tokenizer(test_data[0][\"input\"], return_tensors=\"pt\").to(\"cuda:0\")\n",
    "input_ids, attention_mask = inputs[\"input_ids\"], inputs[\"attention_mask\"]\n",
    "\n",
    "# Export\n",
    "torch.onnx.export(\n",
    "    model,\n",
    "    (input_ids, attention_mask),\n",
    "    \"models/llama_onnx/codellama_7b_gpu.onnx\",\n",
    "    input_names  = [\"input_ids\", \"attention_mask\"],\n",
    "    output_names = [\"logits\"],\n",
    "    dynamic_axes = {\n",
    "        \"input_ids\":      {0: \"batch\", 1: \"seq\"},\n",
    "        \"attention_mask\": {0: \"batch\", 1: \"seq\"},\n",
    "        \"logits\":         {0: \"batch\", 1: \"seq\"},\n",
    "    },\n",
    "    opset_version       = 17,\n",
    "    export_params       = True,\n",
    "    do_constant_folding = True,\n",
    "    external_data_format= True\n",
    ")\n",
    "print(\"Export complete (GPU path)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5ec61068-bf93-4dec-85af-5567c73aa77f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total size: 13482.13 MB (13.48 GB)\n"
     ]
    }
   ],
   "source": [
    "def get_total_folder_size(path):\n",
    "    total_bytes = 0\n",
    "    for dirpath, dirnames, filenames in os.walk(path):\n",
    "        for f in filenames:\n",
    "            fp = os.path.join(dirpath, f)\n",
    "            total_bytes += os.path.getsize(fp)\n",
    "    return total_bytes\n",
    "\n",
    "size_bytes = get_total_folder_size(\"models/llama_onnx\")\n",
    "print(f\"Total size: {size_bytes / 1e6:.2f} MB ({size_bytes / 1e9:.2f} GB)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "653ae9a3-cc15-4c73-a653-3614cd2c9ae2",
   "metadata": {},
   "source": [
    "### Graph Optimizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6885a299-0a41-41ed-ba25-8a9675683399",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c9a33281-2a7e-4efd-89b9-83f6a66f6650",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph-optimized model saved to models/codellama_7b_graph_opt.onnx\n"
     ]
    }
   ],
   "source": [
    "import onnxruntime as ort\n",
    "\n",
    "optimized_path = \"models/codellama_7b_graph_opt.onnx\"\n",
    "session_options = ort.SessionOptions()\n",
    "session_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_EXTENDED\n",
    "session_options.optimized_model_filepath = optimized_path\n",
    "\n",
    "_ = ort.InferenceSession(model_path, sess_options=session_options, providers=[\"CUDAExecutionProvider\"])\n",
    "print(f\"Graph-optimized model saved to {optimized_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa15f09-294e-452d-8a5e-67ae88481e6e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Benchmark Results ===\n",
      "Average Latency: 4.98 ms\n"
     ]
    }
   ],
   "source": [
    "def benchmark_model_latency(model_path, tokenizer_path, prompts, max_length=128):\n",
    "    # Load ONNX model\n",
    "    session = ort.InferenceSession(model_path, providers=[\"CUDAExecutionProvider\", \"CPUExecutionProvider\"])\n",
    "\n",
    "    # Load tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    latencies = []\n",
    "\n",
    "    for prompt in prompts:\n",
    "        input_data = tokenizer(prompt, return_tensors=\"np\", padding=\"max_length\", truncation=True, max_length=max_length)\n",
    "        input_ids = input_data[\"input_ids\"]\n",
    "        attention_mask = input_data[\"attention_mask\"]\n",
    "\n",
    "        start = time.time()\n",
    "        _ = session.run([\"logits\"], {\"input_ids\": input_ids, \"attention_mask\": attention_mask})\n",
    "        latency = (time.time() - start) * 1000  # in milliseconds\n",
    "        latencies.append(latency)\n",
    "\n",
    "    print(\"=== Benchmark Results ===\")\n",
    "    for i, prompt in enumerate(prompts):\n",
    "        print(f\"Prompt {i+1}: {latencies[i]:.2f} ms\")\n",
    "\n",
    "    print(f\"Average Latency: {np.mean(latencies):.2f} ms\")\n",
    "\n",
    "    return latencies, np.mean(latencies)\n",
    "\n",
    "\n",
    "benchmark_model_latency(\"/mnt/object/llama_onnx/codellama_7b_gpu.onnx\", \"/mnt/object/llama_onnx\", test_prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5a6f2359-1ba8-46e0-aa95-0742fa392830",
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_model_path = \"models/llama_onnx/codellama_7b_gpu.onnx\"\n",
    "optimized_model_path = \"models/llama_onnx/codellama_7b_gpu_graph_optimized.onnx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f9e7aa6-8cf2-41a7-9c60-6d04cbf075f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac2c17ea-b09c-4a1f-b61a-d06f9fdca229",
   "metadata": {},
   "source": [
    "### Dynamic Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3075be78-27e5-4004-89e5-e5ea0f4f8585",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Benchmark Results ===\n",
      "Average Latency: 3.28 ms\n"
     ]
    }
   ],
   "source": [
    "fp32_model = neural_compressor.model.onnx_model.ONNXModel(model_path)\n",
    "config_ptq = neural_compressor.PostTrainingQuantConfig(\n",
    "    approach=\"dynamic\"\n",
    ")\n",
    "\n",
    "q_model = quantization.fit(\n",
    "    model=fp32_model, \n",
    "    conf=config_ptq\n",
    ")\n",
    "\n",
    "q_model.save_model_to_file(\"models/codellama/codellama_7b_gpu_dynamic.onnx\")\n",
    "benchmark_session(\"models/llama_onnx, codellama_7b_gpu_dynamic.onnx\", \"models/llama_onnx\", test_prompts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c52219-b8c4-43e0-8678-a59b564cd300",
   "metadata": {},
   "source": [
    "### Static Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b02bdf-bec6-4f0f-9c30-53ad3ba2578b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Benchmark Results ===\n",
      "Average Latency: 4.87 ms\n"
     ]
    }
   ],
   "source": [
    "fp32_model = neural_compressor.model.onnx_model.ONNXModel(model_path)\n",
    "config_ptq = neural_compressor.PostTrainingQuantConfig(\n",
    "    approach=\"static\"\n",
    ")\n",
    "\n",
    "q_model = quantization.fit(\n",
    "    model=fp32_model, \n",
    "    conf=config_ptq\n",
    ")\n",
    "\n",
    "q_model.save_model_to_file(\"models/codellama/codellama_7b_gpu_static.onnx\")\n",
    "benchmark_session(\"models/llama_onnx, codellama_7b_gpu_static.onnx\", \"models/llama_onnx\", test_prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539fe4d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
