{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1zbVsVMrtAKV"
   },
   "source": [
    "### Open the notebook on Colab\n",
    "\n",
    "We should have already started a notebook server in a container on a Chameleon GPU host, and set up an SSH tunnel to this notebook server. Now, we will connect this notebook to the runtime that you have in Chameleon. This is a convenient way to work, because the notebook and its outputs will be saved automatically in your Google Drive.\n",
    "\n",
    "-   Next to the “Connect” button in the top right, there is a ▼ symbol. Click on this symbol to expand the menu, and choose “Connect to a local runtime”.\n",
    "-   Paste the `http://127.0.0.1:8888/lab?token=XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX` you copied earlier into this space, and choose “Connect”.\n",
    "\n",
    "**Alternatively, if you prefer not to use Colab** (or can’t, for some reason): just put the `http://127.0.0.1:8888/lab?token=XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX` URL you copied earlier into your browser to open the Jupyter interface directly. But, then you’ll have to open a terminal in that Jupyter interface and run\n",
    "\n",
    "    wget https://raw.githubusercontent.com/teaching-on-testbeds/llm-chi/refs/heads/main/workspace/2_single_gpu_a100.ipynb\n",
    "\n",
    "to get a copy of this notebook in that workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zHZYnVIv-0Tp",
    "outputId": "c5d3bcdb-a6fd-49e3-c4f5-0a7334aed03e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.12/site-packages (4.51.3)\n",
      "Requirement already satisfied: datasets in /opt/conda/lib/python3.12/site-packages (3.6.0)\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.12/site-packages (2.7.0+cu128)\n",
      "Requirement already satisfied: accelerate in /opt/conda/lib/python3.12/site-packages (1.6.0)\n",
      "Requirement already satisfied: bitsandbytes in /opt/conda/lib/python3.12/site-packages (0.45.5)\n",
      "Requirement already satisfied: sentencepiece in /opt/conda/lib/python3.12/site-packages (0.2.0)\n",
      "Requirement already satisfied: peft in /opt/conda/lib/python3.12/site-packages (0.15.2)\n",
      "Requirement already satisfied: trl in /opt/conda/lib/python3.12/site-packages (0.17.0)\n",
      "Requirement already satisfied: bert-score in /opt/conda/lib/python3.12/site-packages (0.3.13)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.12/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /opt/conda/lib/python3.12/site-packages (from transformers) (0.31.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.12/site-packages (from transformers) (2.2.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.12/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.12/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.12/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.12/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /opt/conda/lib/python3.12/site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /opt/conda/lib/python3.12/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.12/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /opt/conda/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.12/site-packages (from datasets) (19.0.1)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.12/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.12/site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.12/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /opt/conda/lib/python3.12/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /opt/conda/lib/python3.12/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.11.18)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.12/site-packages (from torch) (80.1.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /opt/conda/lib/python3.12/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.12/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.12/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.61 in /opt/conda/lib/python3.12/site-packages (from torch) (12.8.61)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.57 in /opt/conda/lib/python3.12/site-packages (from torch) (12.8.57)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.57 in /opt/conda/lib/python3.12/site-packages (from torch) (12.8.57)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.7.1.26 in /opt/conda/lib/python3.12/site-packages (from torch) (9.7.1.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.3.14 in /opt/conda/lib/python3.12/site-packages (from torch) (12.8.3.14)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.41 in /opt/conda/lib/python3.12/site-packages (from torch) (11.3.3.41)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.55 in /opt/conda/lib/python3.12/site-packages (from torch) (10.3.9.55)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.2.55 in /opt/conda/lib/python3.12/site-packages (from torch) (11.7.2.55)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.7.53 in /opt/conda/lib/python3.12/site-packages (from torch) (12.5.7.53)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /opt/conda/lib/python3.12/site-packages (from torch) (0.6.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in /opt/conda/lib/python3.12/site-packages (from torch) (2.26.2)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.55 in /opt/conda/lib/python3.12/site-packages (from torch) (12.8.55)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.61 in /opt/conda/lib/python3.12/site-packages (from torch) (12.8.61)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.0.11 in /opt/conda/lib/python3.12/site-packages (from torch) (1.13.0.11)\n",
      "Requirement already satisfied: triton==3.3.0 in /opt/conda/lib/python3.12/site-packages (from torch) (3.3.0)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.12/site-packages (from accelerate) (7.0.0)\n",
      "Requirement already satisfied: rich in /opt/conda/lib/python3.12/site-packages (from trl) (14.0.0)\n",
      "Requirement already satisfied: matplotlib in /opt/conda/lib/python3.12/site-packages (from bert-score) (3.10.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/conda/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.6.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.4.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/conda/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/conda/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.0)\n",
      "Requirement already satisfied: idna>=2.0 in /opt/conda/lib/python3.12/site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.10)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.12/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.12/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.12/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.12/site-packages (from requests->transformers) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.12/site-packages (from requests->transformers) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.12/site-packages (from requests->transformers) (2025.1.31)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.12/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.12/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.12/site-packages (from matplotlib->bert-score) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.12/site-packages (from matplotlib->bert-score) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.12/site-packages (from matplotlib->bert-score) (4.57.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/conda/lib/python3.12/site-packages (from matplotlib->bert-score) (1.4.8)\n",
      "Requirement already satisfied: pillow>=8 in /opt/conda/lib/python3.12/site-packages (from matplotlib->bert-score) (11.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.12/site-packages (from matplotlib->bert-score) (3.2.3)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.12/site-packages (from rich->trl) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.12/site-packages (from rich->trl) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich->trl) (0.1.2)\n"
     ]
    }
   ],
   "source": [
    "# Install necessary packages\n",
    "!pip install transformers datasets torch accelerate bitsandbytes sentencepiece peft trl bert-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "xla7QT9E-1rt"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "import time\n",
    "import accelerate\n",
    "from accelerate import infer_auto_device_map, dispatch_model\n",
    "from types import MethodType\n",
    "import gc\n",
    "from transformers import BitsAndBytesConfig, AutoTokenizer, AutoModelForCausalLM, TrainingArguments, GenerationConfig\n",
    "import mlflow\n",
    "import mlflow.pytorch\n",
    "import subprocess\n",
    "from transformers.cache_utils import Cache, DynamicCache\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from trl import SFTTrainer\n",
    "from bert_score import score as bertscore\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "T2y1q_In-4B1"
   },
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Configure the training job \n",
    "# All hyperparameters will be set here, in one convenient place\n",
    "DTYPE_MAP = {\n",
    "    # 'float32': torch.float32,\n",
    "    'float16': torch.float16,\n",
    "    'bfloat16': torch.bfloat16\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "FgGpCXdKGxO5"
   },
   "outputs": [],
   "source": [
    "# Function to load models\n",
    "\n",
    "\n",
    "def load_model_for_qlora(model_name, dtype_str='float16'):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True, use_fast=False)\n",
    "\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=DTYPE_MAP.get(dtype_str)\n",
    "    )\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "\n",
    "    model = prepare_model_for_kbit_training(model)  # Required for QLoRA\n",
    "    return tokenizer, model\n",
    "\n",
    "\n",
    "\n",
    "def patched_prepare_inputs_for_generation(\n",
    "    self,\n",
    "    input_ids,\n",
    "    past_key_values=None,\n",
    "    attention_mask=None,\n",
    "    inputs_embeds=None,\n",
    "    **kwargs,\n",
    "):\n",
    "    if past_key_values is not None:\n",
    "        if isinstance(past_key_values, Cache):\n",
    "            cache_length = past_key_values.get_seq_length()\n",
    "            past_length = past_key_values.seen_tokens\n",
    "            max_cache_length = past_key_values.get_max_cache_shape()\n",
    "        else:\n",
    "            cache_length = past_length = past_key_values[0][0].shape[2]\n",
    "            max_cache_length = None\n",
    "\n",
    "        # Keep only the unprocessed tokens:\n",
    "        # 1 - If the length of the attention_mask exceeds the length of input_ids, then we are in a setting where\n",
    "        # some of the inputs are exclusivelly passed as part of the cache (e.g. when passing input_embeds as\n",
    "        # input)\n",
    "        if (\n",
    "            attention_mask is not None\n",
    "            and attention_mask.shape[1] > input_ids.shape[1]\n",
    "        ):\n",
    "            input_ids = input_ids[:, -(attention_mask.shape[1] - past_length) :]\n",
    "        # 2 - If the past_length is smaller than input_ids', then input_ids holds all input tokens. We can discard\n",
    "        # input_ids based on the past_length.\n",
    "        elif past_length < input_ids.shape[1]:\n",
    "            input_ids = input_ids[:, past_length:]\n",
    "        # 3 - Otherwise (past_length >= input_ids.shape[1]), let's assume input_ids only has unprocessed tokens.\n",
    "\n",
    "        # If we are about to go beyond the maximum cache length, we need to crop the input attention mask.\n",
    "        if (\n",
    "            max_cache_length is not None\n",
    "            and attention_mask is not None\n",
    "            and cache_length + input_ids.shape[1] > max_cache_length\n",
    "        ):\n",
    "            attention_mask = attention_mask[:, -max_cache_length:]\n",
    "\n",
    "    position_ids = kwargs.get(\"position_ids\", None)\n",
    "    if attention_mask is not None and position_ids is None:\n",
    "        # create position_ids on the fly for batch generation\n",
    "        position_ids = attention_mask.long().cumsum(-1) - 1\n",
    "        position_ids.masked_fill_(attention_mask == 0, 1)\n",
    "        if past_key_values:\n",
    "            position_ids = position_ids[:, -input_ids.shape[1] :]\n",
    "\n",
    "    # if `inputs_embeds` are passed, we only want to use them in the 1st generation step\n",
    "    if inputs_embeds is not None and past_key_values is None:\n",
    "        model_inputs = {\"inputs_embeds\": inputs_embeds}\n",
    "    else:\n",
    "        model_inputs = {\"input_ids\": input_ids}\n",
    "\n",
    "    model_inputs.update(\n",
    "        {\n",
    "            \"position_ids\": position_ids,\n",
    "            \"past_key_values\": past_key_values,\n",
    "            \"use_cache\": kwargs.get(\"use_cache\"),\n",
    "            \"attention_mask\": attention_mask,\n",
    "        }\n",
    "    )\n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "def load_and_patch_model(path,  dtype = 'bfloat16'):\n",
    "    tokenizer, model = load_model_for_qlora(path,  dtype)\n",
    "    model.prepare_inputs_for_generation = MethodType(patched_prepare_inputs_for_generation, model)\n",
    "    return tokenizer, model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "6JH2y6p3JRtg"
   },
   "outputs": [],
   "source": [
    "def print_gpu_memory(note=\"\"):\n",
    "    result = subprocess.run(\n",
    "        [\"nvidia-smi\", \"--query-gpu=memory.used,memory.total\", \"--format=csv,nounits,noheader\"],\n",
    "        stdout=subprocess.PIPE,\n",
    "        text=True\n",
    "    )\n",
    "    used, total = map(int, result.stdout.strip().split(','))\n",
    "    print(f\"{note} GPU memory: {used} MiB / {total} MiB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "quI83E-574lJ"
   },
   "outputs": [],
   "source": [
    "def generate_code_with_profiling(model, tokenizer, prompt, max_new_tokens=64):\n",
    "    # Tokenize input and move to model's device\n",
    "    device = next(model.parameters()).device\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    # Clear GPU memory and reset peak memory stats\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.reset_peak_memory_stats(device)\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Generate output\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=False,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            use_cache=True\n",
    "        )\n",
    "\n",
    "        torch.cuda.synchronize()\n",
    "        end_time = time.time()\n",
    "\n",
    "    # Calculate stats\n",
    "    inference_time = end_time - start_time\n",
    "    peak_memory_usage = torch.cuda.max_memory_allocated(device)  # Peak memory used during generate()\n",
    "\n",
    "    decoded_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    print(f\"⏱️ Inference time: {inference_time:.4f} seconds\")\n",
    "    print(f\"📈 Peak memory usage: {peak_memory_usage} bytes\")\n",
    "\n",
    "    return decoded_output, inference_time, peak_memory_usage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def clear_previous_model():\n",
    "#     if 'loaded_model' in locals() and hasattr(loaded_model, 'clear_cache'):\n",
    "#         loaded_model.clear_cache()\n",
    "#     try:\n",
    "#         del loaded_model\n",
    "#         del tokenizer\n",
    "#     except NameError:\n",
    "#         pass\n",
    "#     gc.collect()\n",
    "#     torch.cuda.empty_cache()\n",
    "#     print_gpu_memory(\"After freeing previous model\")\n",
    "def clear_previous_model():\n",
    "    global model, tokenizer\n",
    "\n",
    "    try:\n",
    "        if 'loaded_model' in globals():\n",
    "            if hasattr(model, 'clear_cache'):\n",
    "                model.clear_cache()\n",
    "            del model\n",
    "    except Exception as e:\n",
    "        print(\"Error clearing model:\", e)\n",
    "\n",
    "    try:\n",
    "        if 'tokenizer' in globals():\n",
    "            del tokenizer\n",
    "    except Exception as e:\n",
    "        print(\"Error clearing tokenizer:\", e)\n",
    "\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    print_gpu_memory(\"After freeing previous model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # DeepSeek Coder V2\n",
    "# model_name = \"Lite-Base\"\n",
    "# model_address = \"deepseek-ai/DeepSeek-Coder-V2-Lite-Base\"\n",
    "\n",
    "\n",
    "# for dtype_name, torch_dtype in DTYPE_MAP.items():\n",
    "#     clear_previous_model()\n",
    "\n",
    "#     torch.cuda.empty_cache()\n",
    "#     gc.collect()\n",
    "#     print_gpu_memory(\"Before loading model with dtype \",dtype_name)\n",
    "#     tokenizer, loaded_model = load_and_patch_model(model_address, dtype_name)\n",
    "#     print_gpu_memory(\"After loading model with dtype \",dtype_name)\n",
    "\n",
    "#     for idx, sample in enumerate(dataset):\n",
    "#         print(f\"\\n==================== Sample {idx + 1} ====================\")\n",
    "#         print(f\"Intent: {sample['intent']}\")\n",
    "#         prompt = f\"### Instruction:\\n{sample['intent']}\\n\\n### Response:\"\n",
    "        \n",
    "#         with mlflow.start_run(run_name=f\"{model_name}-sample-{idx}\"):\n",
    "#             output, inference_time, memory_usage = generate_code_with_profiling(loaded_model, tokenizer, prompt)\n",
    "        \n",
    "#             # Log to MLflow\n",
    "#             mlflow.log_param(\"model_name\", model_name)\n",
    "#             mlflow.log_param(\"dtype\", dtype)\n",
    "#             mlflow.log_metric(\"inference_time\", inference_time)\n",
    "#             mlflow.log_metric(\"memory_usage_bytes\", memory_usage)\n",
    "#             mlflow.log_metric(\"reserved_memory_usage_bytes\", reserved_memory_usage)\n",
    "\n",
    "#         print(f\"\\n🔹 Output from {model_name}:\\n{output}\")\n",
    "#         print(f\"⏱️ Inference time: {inference_time:.4f} seconds\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name = \"Lite-Instruct\"\n",
    "# model_address = \"deepseek-ai/DeepSeek-Coder-V2-Lite-Instruct\"\n",
    "# dtype_name = 'float16'\n",
    "# tokenizer, loaded_model = load_and_patch_model(model_address, dtype_name)\n",
    "# print_gpu_memory(\"After loading model with dtype \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_bertscore(eval_dataset, model, tokenizer, max_samples=None):\n",
    "    model.eval()\n",
    "    preds, refs = [], []\n",
    "\n",
    "    for i, sample in enumerate(eval_dataset):\n",
    "        if max_samples and i >= max_samples:\n",
    "            break\n",
    "\n",
    "        input_text = sample[\"input\"]\n",
    "        reference = sample[\"output\"]\n",
    "\n",
    "        inputs = tokenizer(input_text, return_tensors=\"pt\", truncation=True, padding=True).to(model.device)\n",
    "        with torch.no_grad():\n",
    "            output_ids = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=128,\n",
    "                do_sample=False,\n",
    "                num_beams=4\n",
    "            )\n",
    "        prediction = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "        preds.append(prediction)\n",
    "        refs.append(reference)\n",
    "\n",
    "    # Compute BERTScore\n",
    "    P, R, F1 = bertscore(preds, refs, lang=\"en\")\n",
    "    return {\n",
    "        \"bertscore_precision\": P.mean().item(),\n",
    "        \"bertscore_recall\": R.mean().item(),\n",
    "        \"bertscore_f1\": F1.mean().item()\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>instruction</th>\n",
       "      <th>input</th>\n",
       "      <th>output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>You are a code reviewer for a Jenkins plugin. ...</td>\n",
       "      <td>Full Diff:\\ndiff --git a/src/test/java/org/jen...</td>\n",
       "      <td>Please don't make whiitespace changes to exist...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>You are a code reviewer for a Jenkins plugin. ...</td>\n",
       "      <td>Full Diff:\\ndiff --git a/src/test/java/org/jen...</td>\n",
       "      <td>Please don't make white space changes in exist...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>You are a code reviewer for a Jenkins plugin. ...</td>\n",
       "      <td>Full Diff:\\ndiff --git a/src/test/java/org/jen...</td>\n",
       "      <td>Please remove the pure white space changes in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>You are a code reviewer for a Jenkins plugin. ...</td>\n",
       "      <td>Full Diff:\\ndiff --git a/Jenkinsfile b/Jenkins...</td>\n",
       "      <td>I know you're only updating the jenkins versio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You are a code reviewer for a Jenkins plugin. ...</td>\n",
       "      <td>Full Diff:\\ndiff --git a/Jenkinsfile b/Jenkins...</td>\n",
       "      <td>Good question.  I will add a comment explainin...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         instruction  \\\n",
       "0  You are a code reviewer for a Jenkins plugin. ...   \n",
       "1  You are a code reviewer for a Jenkins plugin. ...   \n",
       "2  You are a code reviewer for a Jenkins plugin. ...   \n",
       "3  You are a code reviewer for a Jenkins plugin. ...   \n",
       "4  You are a code reviewer for a Jenkins plugin. ...   \n",
       "\n",
       "                                               input  \\\n",
       "0  Full Diff:\\ndiff --git a/src/test/java/org/jen...   \n",
       "1  Full Diff:\\ndiff --git a/src/test/java/org/jen...   \n",
       "2  Full Diff:\\ndiff --git a/src/test/java/org/jen...   \n",
       "3  Full Diff:\\ndiff --git a/Jenkinsfile b/Jenkins...   \n",
       "4  Full Diff:\\ndiff --git a/Jenkinsfile b/Jenkins...   \n",
       "\n",
       "                                              output  \n",
       "0  Please don't make whiitespace changes to exist...  \n",
       "1  Please don't make white space changes in exist...  \n",
       "2  Please remove the pure white space changes in ...  \n",
       "3  I know you're only updating the jenkins versio...  \n",
       "4  Good question.  I will add a comment explainin...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_comment_jsons(folder_path):\n",
    "    data = []\n",
    "    for fname in os.listdir(folder_path):\n",
    "        if fname.endswith(\"_comments.jsonl\"):\n",
    "            repo_base = fname.replace(\"_comments.jsonl\", \"\")\n",
    "            diff_path = os.path.join(folder_path, f\"{repo_base}.diff\")\n",
    "            if not os.path.exists(diff_path):\n",
    "                continue\n",
    "\n",
    "            # Load full diff file\n",
    "            with open(diff_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                full_diff = f.read()\n",
    "\n",
    "            # Load comment JSONL\n",
    "            with open(os.path.join(folder_path, fname), \"r\", encoding=\"utf-8\") as f:\n",
    "                for line in f:\n",
    "                    entry = json.loads(line)\n",
    "                    if \"diff_hunk\" in entry and \"body\" in entry:\n",
    "                        data.append({\n",
    "                            \"instruction\": \"\"\"You are a code reviewer for a Jenkins plugin. Review the following diff for potential improvements or guideline violations.\n",
    "\n",
    "Key guidelines to follow:\n",
    "- Use standard Java libraries instead of external ones like Commons I/O when possible.\n",
    "- Avoid deprecated APIs, especially in Jenkins core and plugins.\n",
    "- Write clear, descriptive method and variable names.\n",
    "- Add or update tests when modifying functionality or fixing bugs.\n",
    "- Do not include commented-out code or leftover TODOs.\n",
    "- Update documentation if user-facing behavior changes.\n",
    "- Keep commits focused and avoid mixing unrelated changes.\n",
    "- Code must compile cleanly and pass all tests.\n",
    "- Maintain consistent formatting and follow Jenkins coding style.\n",
    "\n",
    "Also consider other good practices not explicitly listed above.\"\"\",\n",
    "                            \"input\": f\"Full Diff:\\n{full_diff}\\n\\nFocused Hunk:\\n{entry['diff_hunk']}\",\n",
    "                            \"output\": entry[\"body\"]\n",
    "                        })\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "\n",
    "df = load_comment_jsons(\"git-client-plugin\")\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = \"pr_review_dataset.json\"\n",
    "df.to_json(output_path, orient=\"records\", lines=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b932441f9104ef4b1f4b05199b0c527",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Load dataset\n",
    "dataset = load_dataset(\"json\", data_files=\"pr_review_dataset.json\", split=\"train\")\n",
    "dataset = dataset.train_test_split(test_size=0.1, seed=42)\n",
    "train_dataset = dataset[\"train\"]\n",
    "val_dataset = dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After freeing previous model GPU memory: 1 MiB / 81920 MiB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "628c09310e6640a99c8c5f156a56dc4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After loading model with dtype  GPU memory: 15779 MiB / 81920 MiB\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "clear_previous_model()\n",
    "# Load tokenizer + model\n",
    "# model_name = \"deepseek-ai/DeepSeek-Coder-V2-Lite-Instruct\"  # already loaded by you, skip this if in memory\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     model_name,\n",
    "#     load_in_4bit=True,\n",
    "#     device_map=\"auto\",\n",
    "#     trust_remote_code=True\n",
    "# )\n",
    "\n",
    "model_name = \"Lite-Instruct\"\n",
    "model_address = \"deepseek-ai/DeepSeek-Coder-V2-Lite-Instruct\"\n",
    "dtype_name = 'float16'\n",
    "tokenizer, model = load_and_patch_model(model_address, dtype_name)\n",
    "print_gpu_memory(\"After loading model with dtype \")\n",
    "\n",
    "# Prepare model for QLoRA\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# LoRA config\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],  # tune depending on DeepSeek architecture\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Training args\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./qlora-output\",\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=2,\n",
    "    num_train_epochs=3,\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-4,\n",
    "    fp16=True,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf9f4a4d691e4f24a9e74aa2ce03bb09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying formatting function to train dataset:   0%|          | 0/528 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a936783833b4e9d97ff8771c5d1691c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Converting train dataset to ChatML:   0%|          | 0/528 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f0efd4ad5604c778e480741a48f3265",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding EOS to train dataset:   0%|          | 0/528 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89670851c1ba4341a455ab25d353ebb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset:   0%|          | 0/528 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (19245 > 16384). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45abbb35784a4900b1fde801d8b9e772",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating train dataset:   0%|          | 0/528 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a9d40dbd8a74f5fadd40d269e904cec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying formatting function to eval dataset:   0%|          | 0/59 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e5636ed69bc4bf8b480a6628abc917f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Converting eval dataset to ChatML:   0%|          | 0/59 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2287b80f39304a7ba495ed7290942f58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding EOS to eval dataset:   0%|          | 0/59 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba4171f0ef7347ad8ededeabaf6f1a9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing eval dataset:   0%|          | 0/59 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0967b50c1f9448c48ce5825188da8c78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating eval dataset:   0%|          | 0/59 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    }
   ],
   "source": [
    "def formatting_func(example):\n",
    "    return f\"### Instruction:\\n{example['instruction']}\\n\\n### Input:\\n{example['input']}\\n\\n### Response:\\n{example['output']}\"\n",
    "\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    args=training_args,\n",
    "    formatting_func=formatting_func\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='mlflow-artifacts:/1', creation_time=1746855207918, experiment_id='1', last_update_time=1746855207918, lifecycle_stage='active', name='pr_reviewer_model_exp', tags={}>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlflow.set_experiment(\"pr_reviewer_model_exp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from accelerate.state import AcceleratorState\n",
    "\n",
    "# # Reset global accelerator state\n",
    "# AcceleratorState._reset_state()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Epoch 1 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='55' max='198' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 55/198 09:16 < 25:00, 0.10 it/s, Epoch 0.82/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.155200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.997900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.932600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.840100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.727900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with mlflow.start_run(run_name=\"qlora-deepseek-pr-review\"):\n",
    "\n",
    "    mlflow.log_params({\n",
    "        \"model_name\": model_name,\n",
    "        \"batch_size\": training_args.per_device_train_batch_size,\n",
    "        \"epochs\": training_args.num_train_epochs,\n",
    "        \"learning_rate\": training_args.learning_rate,\n",
    "        \"gradient_accumulation_steps\": training_args.gradient_accumulation_steps,\n",
    "        \"lora_r\": lora_config.r,\n",
    "        \"lora_alpha\": lora_config.lora_alpha\n",
    "    })\n",
    "\n",
    "    for epoch in range(int(training_args.num_train_epochs)):\n",
    "        print(f\"\\n--- Epoch {epoch + 1} ---\")\n",
    "\n",
    "        # Reset and measure peak memory before training epoch\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "        torch.cuda.synchronize()\n",
    "        start_time = time.time()\n",
    "\n",
    "        trainer.train()\n",
    "\n",
    "        torch.cuda.synchronize()\n",
    "        end_time = time.time()\n",
    "        peak_memory = torch.cuda.max_memory_allocated()\n",
    "        train_time = end_time - start_time\n",
    "\n",
    "        # Log timing and memory\n",
    "        mlflow.log_metric(f\"train_time_epoch_{epoch+1}\", train_time)\n",
    "        mlflow.log_metric(f\"peak_memory_epoch_{epoch+1}\", peak_memory)\n",
    "\n",
    "        # Save model checkpoint\n",
    "        # epoch_ckpt_path = os.path.join(checkpoint_dir, f\"epoch_{epoch+1}\")\n",
    "        # trainer.save_model(epoch_ckpt_path)\n",
    "        # mlflow.log_artifact(epoch_ckpt_path)\n",
    "\n",
    "        # Evaluate\n",
    "        print(\"\\nEvaluating with BERTScore...\")\n",
    "        bert_metrics = compute_bertscore(val_dataset, model, tokenizer, max_samples=100)\n",
    "        mlflow.log_metrics({f\"{k}_epoch_{epoch+1}\": v for k, v in bert_metrics.items()})\n",
    "        print(f\"📊 BERTScore F1 (Epoch {epoch + 1}): {bert_metrics['bertscore_f1']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"my_finetuned_model.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.eval()\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "response = pipe({\n",
    "    \"instruction\": \"Review the following diff for guideline violations.\",\n",
    "    \"input\": \"@@ -536,7 +536,7 @@ public void tag(String name, String message) throws GitException {\\n- Ref tag = repo.getRefDatabase().getRef(R_TAGS + tagName);\\n+ Ref tag = repo.getRefDatabase().exactRef(R_TAGS + tagName);\"\n",
    "})\n",
    "\n",
    "print(response[0]['generated_text'])"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "01bba12c03e74891b28bb338c4d3968f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "2.0.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_allow_html": false,
      "layout": "IPY_MODEL_799dec9522d54ba6b5201d61c01fb8e2",
      "placeholder": "​",
      "style": "IPY_MODEL_df6f0fe1d3ac4a0abd77f77a5c679bef",
      "tabbable": null,
      "tooltip": null,
      "value": " 4/4 [00:02&lt;00:00,  1.29it/s]"
     }
    },
    "169d733b095e4fca96b652c57e5bc7de": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "HTMLStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "HTMLStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "StyleView",
      "background": null,
      "description_width": "",
      "font_size": null,
      "text_color": null
     }
    },
    "25488b6e2e934e20abe409bbee4e5969": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "2.0.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_allow_html": false,
      "layout": "IPY_MODEL_5aa7564ae9c248a8b5b8b1dc38c8f88d",
      "max": 4,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_605c7d2e8b0c49689b89c266e082c124",
      "tabbable": null,
      "tooltip": null,
      "value": 4
     }
    },
    "5aa7564ae9c248a8b5b8b1dc38c8f88d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "2.0.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "2.0.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border_bottom": null,
      "border_left": null,
      "border_right": null,
      "border_top": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5f087d26f22d4db1947e471926b544a8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "2.0.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "2.0.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border_bottom": null,
      "border_left": null,
      "border_right": null,
      "border_top": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "605c7d2e8b0c49689b89c266e082c124": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "799dec9522d54ba6b5201d61c01fb8e2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "2.0.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "2.0.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border_bottom": null,
      "border_left": null,
      "border_right": null,
      "border_top": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a02fa0ff23af423786fb82bf26086f16": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "2.0.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_f51895fe8c7e488bb1dbe6c65d034601",
       "IPY_MODEL_25488b6e2e934e20abe409bbee4e5969",
       "IPY_MODEL_01bba12c03e74891b28bb338c4d3968f"
      ],
      "layout": "IPY_MODEL_5f087d26f22d4db1947e471926b544a8",
      "tabbable": null,
      "tooltip": null
     }
    },
    "df6f0fe1d3ac4a0abd77f77a5c679bef": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "HTMLStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "HTMLStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "StyleView",
      "background": null,
      "description_width": "",
      "font_size": null,
      "text_color": null
     }
    },
    "f51895fe8c7e488bb1dbe6c65d034601": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "2.0.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_allow_html": false,
      "layout": "IPY_MODEL_fba750810c774dc288fa0d908b22efba",
      "placeholder": "​",
      "style": "IPY_MODEL_169d733b095e4fca96b652c57e5bc7de",
      "tabbable": null,
      "tooltip": null,
      "value": "Loading checkpoint shards: 100%"
     }
    },
    "fba750810c774dc288fa0d908b22efba": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "2.0.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "2.0.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border_bottom": null,
      "border_left": null,
      "border_right": null,
      "border_top": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
