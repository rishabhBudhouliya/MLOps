{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c27193b2-d1a3-4bb0-a0f4-43a1d86a7c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install necessary packages\n",
    "!pip install transformers datasets torch accelerate bitsandbytes sentencepiece peft trl bert-score mlflow\n",
    "!pip install evaluate bert-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f51947f0-2f03-41c0-8ad4-dccde63b331f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Libraries\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import gzip\n",
    "import gc\n",
    "import subprocess\n",
    "from types import MethodType\n",
    "\n",
    "# Core Libraries\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Hugging Face Transformers & Datasets\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    GenerationConfig,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    DataCollatorForSeq2Seq,\n",
    ")\n",
    "from transformers.cache_utils import Cache, DynamicCache\n",
    "\n",
    "# PEFT (Parameter-Efficient Fine-Tuning)\n",
    "from peft import (\n",
    "    get_peft_model,\n",
    "    LoraConfig,\n",
    "    TaskType,\n",
    "    prepare_model_for_kbit_training,\n",
    ")\n",
    "\n",
    "# TRL (Transformers Reinforcement Learning)\n",
    "from trl import SFTTrainer\n",
    "\n",
    "# Accelerate\n",
    "import accelerate\n",
    "from accelerate import infer_auto_device_map, dispatch_model\n",
    "\n",
    "# BERTScore\n",
    "from bert_score import score as bertscore\n",
    "\n",
    "# MLflow\n",
    "import mlflow\n",
    "import mlflow.pytorch\n",
    "\n",
    "import json\n",
    "import gzip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "40412002-1b79-4d0e-a258-131788abad70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ced1c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!~/rclone-v1.69.2-linux-amd64/rclone ls object:object-persist-project32\n",
    "\n",
    "!~/rclone-v1.69.2-linux-amd64/rclone lsd object_group:object-persist-group32/data/processed\n",
    "\n",
    "!~/rclone-v1.69.2-linux-amd64/rclone copy object_group:object-persist-group32/data/processed/train.jsonl.gz .\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "33c1d2bf-ebae-45f4-ada3-44d689c36219",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and process dataset\n",
    "guidelines = \"\"\"Key guidelines to follow:\n",
    "- Use standard Java libraries instead of external ones like Commons I/O when possible.\n",
    "- Avoid deprecated APIs, especially in Jenkins core and plugins.\n",
    "- Write clear, descriptive method and variable names.\n",
    "- Add or update tests when modifying functionality or fixing bugs.\n",
    "- Do not include commented-out code or leftover TODOs.\n",
    "- Update documentation if user-facing behavior changes.\n",
    "- Keep commits focused and avoid mixing unrelated changes.\n",
    "- Code must compile cleanly and pass all tests.\n",
    "- Maintain consistent formatting and follow Jenkins coding style.\n",
    "Also consider other good practices not explicitly listed above.\"\"\"\n",
    "def format_prompt(example):\n",
    "    offset = example.get('offset')\n",
    "    offset_info = f\"The comment refers to line {offset} in the diff.\" if offset is not None else \"\"\n",
    "\n",
    "    formatted_comment = (\n",
    "        f\"<COMMENT offset=\\\"{offset}\\\">{example['comment']}\\n\"\n",
    "        if offset is not None and example.get('comment')\n",
    "        else example.get('comment', '')\n",
    "    )\n",
    "\n",
    "    prompt = f\"\"\"### Instruction:\n",
    "You are a code reviewer for a Jenkins plugin. Review the following diff for potential improvements or guideline violations.\n",
    "{offset_info}\n",
    "\n",
    "{guidelines}\n",
    "\n",
    "### Input:\n",
    "Diff snippet:\n",
    "{example['diff']}\n",
    "\n",
    "### Response:\n",
    "{formatted_comment}\"\"\"\n",
    "\n",
    "    tokens = tokenizer(prompt, truncation=True, padding='max_length', max_length=1024)\n",
    "    tokens[\"labels\"] = tokens[\"input_ids\"].copy()\n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c89e1da-9612-4108-9a36-805b667ee7f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69a17000385d4d74bfe70f3888117b02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load tokenizer and model\n",
    "model_name = \"codellama/CodeLlama-7b-Instruct-hf\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Load the model with proper device mapping\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",\n",
    "    load_in_8bit=True,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# Prepare model for QLoRA training\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# Apply LoRA configuration\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    # task_type=TaskType.CAUSAL_LM\n",
    ")\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Set up data collator\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False  # This is key for causal LMs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061a9798-2462-4ba5-b2a4-fae50a72342f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First record of the processed dataset:\n",
      "{\n",
      "    \"diff\": \"@@ -6,10 +6,13 @@\\n  * found in the LICENSE file at https://angular.io/license\\n  */\\n \\n-import {unimplemented} from '../../facade/exceptions';\\n+import {BaseException} from '@angular/core';\\n import {isPresent} from '../../facade/lang';\\n import {AbstractControl} from '../model';\\n \\n+function unimplemented(): any {\\n+  throw new BaseException('unimplemented');\",\n",
      "    \"comment\": \"Is there a better way to avoid having this in different places?\\n\",\n",
      "    \"offset\": 9,\n",
      "    \"comment_commit_id\": \"ee8e802b7bc25eafbe54109b3924e9dbc36dce11\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def load_and_filter_dataset(file_path):\n",
    "    \"\"\"\n",
    "    Loads a gzipped JSONL file and extracts specific fields. Processes only the first comment\n",
    "    encountered for each unique 'comment_commit_id'.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): The path to the .jsonl.gz file.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of dictionaries, where each dictionary contains\n",
    "              'diff', 'comment', and 'offset' for each record.\n",
    "    \"\"\"\n",
    "    filtered_data = []\n",
    "    seen_commit_ids = set()  # Track seen commit IDs\n",
    "\n",
    "    with gzip.open(file_path, \"rt\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            try:\n",
    "                record = json.loads(line)\n",
    "\n",
    "                # Skip if we've already seen this comment_commit_id\n",
    "                comment_commit_id = record.get('comment_commit_id')\n",
    "                if comment_commit_id and comment_commit_id in seen_commit_ids:\n",
    "                    continue  # Skip this record\n",
    "\n",
    "                # Mark this comment_commit_id as seen\n",
    "                if comment_commit_id:\n",
    "                    seen_commit_ids.add(comment_commit_id)\n",
    "\n",
    "                # The 'diff' in your original data is the main part of the diff.\n",
    "                # The 'diff_hunk_header' is the line that usually starts with '@@'.\n",
    "                # We'll combine them if both exist, or use whichever is present.\n",
    "                \n",
    "                diff_parts = []\n",
    "                if 'diff_hunk_header' in record and record['diff_hunk_header']:\n",
    "                    diff_parts.append(record['diff_hunk_header'])\n",
    "                if 'diff' in record and record['diff']:\n",
    "                    diff_parts.append(record['diff'])\n",
    "                \n",
    "                full_diff = \"\\n\".join(diff_parts) if diff_parts else None\n",
    "\n",
    "                filtered_record = {\n",
    "                    'diff': full_diff,\n",
    "                    'comment': record.get('comment_body'),  # Use .get() for safety if key might be missing\n",
    "                    'offset': record.get('line_offset'),    # Use .get() for safety\n",
    "                    'comment_commit_id': comment_commit_id  # Keep track of the commit ID\n",
    "                }\n",
    "                filtered_data.append(filtered_record)\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Skipping line due to JSON decode error: {e} - Line: {line.strip()}\")\n",
    "            except KeyError as e:\n",
    "                print(f\"Skipping record due to missing key: {e} - Record: {record}\")\n",
    "\n",
    "    return filtered_data\n",
    "\n",
    "# --- --- --- --- --- --- --- --- --- --- --- --- --- --- ---\n",
    "#                  Example Usage:\n",
    "# --- --- --- --- --- --- --- --- --- --- --- --- --- --- ---\n",
    "\n",
    "# 1. Make sure 'train.jsonl.gz' is in the same directory as your script,\n",
    "#    or provide the full path to the file.\n",
    "file_path = \"train.jsonl.gz\"\n",
    "processed_dataset = load_and_filter_dataset(file_path)\n",
    "\n",
    "# 2. View the first record of your new dataset\n",
    "if processed_dataset:\n",
    "    print(\"First record of the processed dataset:\")\n",
    "    print(json.dumps(processed_dataset[0], indent=4)) # Pretty print the JSON\n",
    "else:\n",
    "    print(\"No data was processed. Check your file path and file content.\")\n",
    "\n",
    "\n",
    "file_path = \"train.jsonl.gz\"\n",
    "processed_dataset = load_and_filter_dataset(file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c7f0e2e5-9612-4a75-874d-2e6815716bff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3140\n"
     ]
    }
   ],
   "source": [
    "print(len(processed_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00375ea3-5056-4133-8a31-0fcbbc99af43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize data\n",
    "tokenized_data = [format_prompt(e) for e in processed_dataset if e['diff'] and e['comment']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c5eab1e-163b-4c06-a1f5-f8ce94fc2b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Dataset.from_list(tokenized_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "732d9127-e669-43f4-8a16-ffb3402bee40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERTScore metric for evaluation\n",
    "import evaluate\n",
    "bertscore = evaluate.load(\"bertscore\")\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    scores = bertscore.compute(predictions=decoded_preds, references=decoded_labels, lang=\"en\")\n",
    "    avg_f1_score = sum(scores[\"f1\"]) / len(scores[\"f1\"])\n",
    "\n",
    "    # Log BERTScore to MLflow\n",
    "    mlflow.log_metric(\"bertscore_f1\", avg_f1_score)\n",
    "\n",
    "    # Return metrics for Trainer\n",
    "    return {\"bertscore_f1\": avg_f1_score}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688730ac-8e1a-4ff1-aeb3-968b4dbf097e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2971/2883527182.py:41: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "No label_names provided for model class `PeftModel`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 7, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='49' max='49' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [49/49 49:09, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.657200</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.224900</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.858400</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.745300</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 7, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 7, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 7, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from transformers import TrainingArguments, Trainer, IntervalStrategy\n",
    "\n",
    "# Define your local path and object storage path\n",
    "local_mlflow_path = \"/mnt/mlflow\"\n",
    "remote_object_store_path = \"object_group:object-persist-group32/data/processed/\"\n",
    "\n",
    "# Copy MLflow artifacts to object store after training\n",
    "os.system(f\"~/rclone-v1.69.2-linux-amd64/rclone copy {local_mlflow_path} {remote_object_store_path}\")\n",
    "\n",
    "    \n",
    "# Training Arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./qlora_output\",\n",
    "    per_device_train_batch_size=16,         # Increase batch size for better GPU utilization\n",
    "    gradient_accumulation_steps=4,         # Accumulate gradients over 2 steps\n",
    "    learning_rate=2e-4,\n",
    "    num_train_epochs=1,                    # Increase epochs for the full dataset\n",
    "    logging_steps=10,\n",
    "    save_steps=10,                       # Save the model every 1000 steps\n",
    "    eval_steps=10,                        # Evaluate every 100 steps\n",
    "    # save_total_limit=2,                    # Keep the latest 2 checkpoints\n",
    "    eval_strategy=IntervalStrategy.STEPS,  # Evaluate after every 100 steps\n",
    "    save_strategy=IntervalStrategy.STEPS, # Save after every 1000 steps\n",
    "    load_best_model_at_end=False,           # Load the best model based on evaluation\n",
    "    report_to=[\"mlflow\"],\n",
    "    logging_dir=\"./logs\",\n",
    "    fp16=True,                             # Ensure FP16 is enabled\n",
    "    dataloader_num_workers=8,        # Number of CPU workers for loading data\n",
    "    dataloader_pin_memory=True,   \n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=train_dataset.select(range(20)),  # Evaluate on a subset, can be changed\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,  # Use custom BERTScore metric\n",
    ")\n",
    "\n",
    "import mlflow\n",
    "\n",
    "# Set the experiment name to a valid name (make sure it's not '.ipynb_checkpoints')\n",
    "mlflow.set_tracking_uri(\"mlflow_updated\")  # or custom URI if you're using one\n",
    "mlflow.set_experiment(\"qlora_finetuning\")  # Set a custom experiment name\n",
    "\n",
    "# Now proceed with starting the MLflow run and training\n",
    "with mlflow.start_run():\n",
    "    # Log model and other information\n",
    "    trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "807c575a-5797-42ed-a3b7-298862f6ae11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3138\n"
     ]
    }
   ],
   "source": [
    "print(len(train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "875b1c28-03d8-4459-b9f2-0939b7b88ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the fine-tuned model and tokenizer\n",
    "trainer.save_model(\"./qlora_output2\")  # This saves the model\n",
    "tokenizer.save_pretrained(\"./qlora_output2\")  # Optionally save the tokenizer\n",
    "\n",
    "# Optionally save the config\n",
    "model.config.to_json_file(\"./qlora_output2/config.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2672dfca-8e8c-4ce5-888c-13aac7c1c9e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"final_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48411ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "\n",
    "# Define the local path to the trained model checkpoint\n",
    "local_model_path = './qlora_output/pytorch_model.bin'  # Adjust filename if needed\n",
    "\n",
    "# Define the destination in the object store\n",
    "object_store_path = 'object:object-persist-project32'  # Your object store destination\n",
    "\n",
    "# Check if the file exists\n",
    "if os.path.exists(local_model_path):\n",
    "    print(f\"Found the model at {local_model_path}, copying to object store...\")\n",
    "\n",
    "    # Run rclone to copy the model to the object store\n",
    "    subprocess.run([\n",
    "        \"/path/to/rclone\",  # Adjust path if rclone is not in your PATH\n",
    "        \"copy\", \n",
    "        local_model_path, \n",
    "        f\"{object_store_path}/model.pth\"  # This is the path where the model will be saved\n",
    "    ], check=True)\n",
    "\n",
    "    print(\"Model successfully copied to the object store.\")\n",
    "else:\n",
    "    print(f\"Model not found at {local_model_path}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6cc8b9c5-997d-4a8c-8a3b-fab734ca141b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/mlflow_updated/169100839237642283/dbef07ad4607444ca89baa0d24060cf5/metrics.zip'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import shutil\n",
    "\n",
    "# Replace 'my_folder' with your folder name\n",
    "shutil.make_archive('mlflow_updated/169100839237642283/dbef07ad4607444ca89baa0d24060cf5/metrics', 'zip', 'mlflow_metrics_log')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa0bc1a-cba1-4893-b829-b6de77eeca3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Replace 'my_folder' with your folder name\n",
    "shutil.make_archive('qlora_output2', 'zip', 'qlora_output2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "33d00999-67a8-4497-843e-815f280fc00c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_prompt_for_inference(diff):\n",
    "    prompt = f\"\"\"### Instruction:\n",
    "You are a code reviewer for a Jenkins plugin. Review the following diff for potential improvements or guideline violations.\n",
    "\n",
    "Your response must follow this format exactly:\n",
    "<COMMENT offset=\"LINE_NUMBER\">Your review comment here.\n",
    "\n",
    "Where offset is the line number the review comment is talking about. If no issues are found, respond with: <COMMENT offset=\"None\">.\n",
    "\n",
    "{guidelines}\n",
    "\n",
    "### Input:\n",
    "Diff snippet:\n",
    "{diff}\n",
    "\n",
    "### Response:\"\"\"\n",
    "    return prompt\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Prepare prompts\n",
    "inference_prompts = [\n",
    "    format_prompt_for_inference(e['diff'])\n",
    "    for e in processed_dataset if e['diff'] and e['comment']\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a8156307-90fe-4e3d-a51d-bacbb2d7a509",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Instruction:\n",
      "You are a code reviewer for a Jenkins plugin. Review the following diff for potential improvements or guideline violations.\n",
      "\n",
      "Your response must follow this format exactly:\n",
      "<COMMENT offset=\"LINE_NUMBER\">Your review comment here.\n",
      "\n",
      "Where offset is the line number the review comment is talking about. If no issues are found, respond with: <COMMENT offset=\"None\">.\n",
      "\n",
      "Key guidelines to follow:\n",
      "- Use standard Java libraries instead of external ones like Commons I/O when possible.\n",
      "- Avoid deprecated APIs, especially in Jenkins core and plugins.\n",
      "- Write clear, descriptive method and variable names.\n",
      "- Add or update tests when modifying functionality or fixing bugs.\n",
      "- Do not include commented-out code or leftover TODOs.\n",
      "- Update documentation if user-facing behavior changes.\n",
      "- Keep commits focused and avoid mixing unrelated changes.\n",
      "- Code must compile cleanly and pass all tests.\n",
      "- Maintain consistent formatting and follow Jenkins coding style.\n",
      "Also consider other good practices not explicitly listed above.\n",
      "\n",
      "### Input:\n",
      "Diff snippet:\n",
      "@@ -6,10 +6,13 @@\n",
      "  * found in the LICENSE file at https://angular.io/license\n",
      "  */\n",
      " \n",
      "-import {unimplemented} from '../../facade/exceptions';\n",
      "+import {BaseException} from '@angular/core';\n",
      " import {isPresent} from '../../facade/lang';\n",
      " import {AbstractControl} from '../model';\n",
      " \n",
      "+function unimplemented(): any {\n",
      "+  throw new BaseException('unimplemented');\n",
      "\n",
      "### Response:\n"
     ]
    }
   ],
   "source": [
    "print(inference_prompts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ec6c3487-70d9-43d3-b2df-4c6063dbb684",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Example 1 ---\n",
      "Input diff:\n",
      " @@ -6,10 +6,13 @@\n",
      "  * found in the LICENSE file at https://angular.io/license\n",
      "  */\n",
      " \n",
      "-import {unimplemented} from '../../facade/exceptions';\n",
      "+import {BaseException} from '@angular/core';\n",
      " import {isPresent} from '../../facade/lang';\n",
      " import {AbstractControl} from '../model';\n",
      " \n",
      "+function unimplemented(): any {\n",
      "+  throw new BaseException('unimplemented');\n",
      "\n",
      "Actual comment:\n",
      " Is there a better way to avoid having this in different places?\n",
      "\n",
      "\n",
      "Generated comment:\n",
      " package:angular/src/forms/validators.ts\n",
      "<COMMENT offset=\"11\">I think this is a bug in the compiler.\n",
      "\n",
      "### Input:\n",
      "Diff snippet:\n",
      "@@ -10,11 +10,11 @@\n",
      "  * found in the LICENSE file at https://angular.io/license\n",
      "  */\n",
      " \n",
      "-import {unimplemented} from '../../facade/exceptions';\n",
      "+import {BaseException} from '@angular/core';\n",
      " import {isPresent} from '../../facade/\n",
      "\n",
      "--- Example 2 ---\n",
      "Input diff:\n",
      " describe('template codegen output', () => {\n",
      "@@ -44,24 +44,40 @@ describe('template codegen output', () => {\n",
      "   it('should support ngIf', () => {\n",
      "     var compFixture = createComponent(BasicComp);\n",
      "     var debugElement = compFixture.debugElement;\n",
      "-    expect(debugElement.children.length).toBe(2);\n",
      "+    expect(debugElement.children.length).toBe(3);\n",
      " \n",
      "     compFixture.componentInstance.ctxBool = true;\n",
      "     compFixture.detectChanges();\n",
      "-    expect(debugElement.children.length).toBe(3);\n",
      "+    expect(debugElement.children.length).toBe(4);\n",
      "     expect(debugElement.children[2].injector.get(MultipleComponentsMyComp)).toBeTruthy();\n",
      "   });\n",
      " \n",
      "   it('should support ngFor', () => {\n",
      "     var compFixture = createComponent(BasicComp);\n",
      "     var debugElement = compFixture.debugElement;\n",
      "-    expect(debugElement.children.length).toBe(2);\n",
      "+    expect(debugElement.children.length).toBe(3);\n",
      " \n",
      "     // test NgFor\n",
      "     compFixture.componentInstance.ctxArr = [1, 2];\n",
      "     compFixture.detectChanges();\n",
      "-    expect(debugElement.children.length).toBe(4);\n",
      "+    expect(debugElement.children.length).toBe(5);\n",
      "     expect(debugElement.children[2].attributes['value']).toBe('1');\n",
      "     expect(debugElement.children[3].attributes['value']).toBe('2');\n",
      "   });\n",
      "+\n",
      "+  describe('i18n', () => {\n",
      "+    it('should support i18n for content tags', () => {\n",
      "+      var compFixture = createComponent(BasicComp);\n",
      "+      expect(compFixture.componentInstance.ctxLocale).toEqual('FI');\n",
      "+    });\n",
      "+\n",
      "+    it('should support i18n for content tags', () => {\n",
      "+      var compFixture = createComponent(BasicComp);\n",
      "\n",
      "Actual comment:\n",
      " const\n",
      "\n",
      "\n",
      "Generated comment:\n",
      " <COMMENT offset=\"None\">This is a good test, but it's not clear what the purpose of the test is.\n",
      "\n",
      "The test is checking that the `i18n` attribute is set correctly on the `div` element.\n",
      "\n",
      "However, the `i18n` attribute is not actually used by Angular.\n",
      "\n",
      "The `i18n` attribute is used by the Angular compiler to generate a `i18n` instruction in the generated code.\n",
      "\n",
      "The `i18n` instruction is used by the Angular runtime to translate the text.\n",
      "\n",
      "So\n",
      "\n",
      "--- Example 3 ---\n",
      "Input diff:\n",
      " @@ -7,8 +7,8 @@\n",
      "  */\n",
      " \n",
      " import {XHR} from '@angular/compiler';\n",
      "-import {Component, bind} from '@angular/core';\n",
      "-import {TestBed, async, fakeAsync, flushMicrotasks, inject, tick} from '@angular/core/testing';\n",
      "+import {Component} from '@angular/core';\n",
      "+import {TestBed, TestComponentBuilder, addProviders, async, fakeAsync, inject, tick} from '@angular/core/testing';\n",
      "\n",
      "Actual comment:\n",
      " rebase error.\n",
      "\n",
      "\n",
      "Generated comment:\n",
      " package:angular2/src/\n",
      "\n",
      "--- Example 4 ---\n",
      "Input diff:\n",
      " export interface NameResolver {\n",
      "@@ -21,25 +21,44 @@ export interface NameResolver {\n",
      " }\n",
      " \n",
      " export class ExpressionWithWrappedValueInfo {\n",
      "-  constructor(public expression: o.Expression, public needsValueUnwrapper: boolean) {}\n",
      "+  constructor(\n",
      "+      public expression: o.Expression, public needsValueUnwrapper: boolean,\n",
      "+      public temporaries: number) {}\n",
      " }\n",
      " \n",
      " export function convertCdExpressionToIr(\n",
      "     nameResolver: NameResolver, implicitReceiver: o.Expression, expression: cdAst.AST,\n",
      "-    valueUnwrapper: o.ReadVarExpr): ExpressionWithWrappedValueInfo {\n",
      "-  const visitor = new _AstToIrVisitor(nameResolver, implicitReceiver, valueUnwrapper);\n",
      "+    valueUnwrapper: o.ReadVarExpr, bindingIndex: number): ExpressionWithWrappedValueInfo {\n",
      "+  const visitor = new _AstToIrVisitor(nameResolver, implicitReceiver, valueUnwrapper, bindingIndex);\n",
      "   const irAst: o.Expression = expression.visit(visitor, _Mode.Expression);\n",
      "-  return new ExpressionWithWrappedValueInfo(irAst, visitor.needsValueUnwrapper);\n",
      "+  return new ExpressionWithWrappedValueInfo(\n",
      "+      irAst, visitor.needsValueUnwrapper, visitor.temporaries);\n",
      " }\n",
      " \n",
      " export function convertCdStatementToIr(\n",
      "-    nameResolver: NameResolver, implicitReceiver: o.Expression, stmt: cdAst.AST): o.Statement[] {\n",
      "-  const visitor = new _AstToIrVisitor(nameResolver, implicitReceiver, null);\n",
      "+    nameResolver: NameResolver, implicitReceiver: o.Expression, stmt: cdAst.AST,\n",
      "+    bindingIndex: number): o.Statement[] {\n",
      "+  const visitor = new _AstToIrVisitor(nameResolver, implicitReceiver, null, bindingIndex);\n",
      "   let statements: o.Statement[] = [];\n",
      "   flattenStatements(stmt.visit(visitor, _Mode.Statement), statements);\n",
      "+  declareTemporaries(visitor.temporaries, bindingIndex, statements);\n",
      "   return statements;\n",
      " }\n",
      " \n",
      "+function temporaryName(bindingIndex: number, temporaryNumber: number): string {\n",
      "+  return `tmp_${bindingIndex}_${temporaryNumber}`;\n",
      "+}\n",
      "+\n",
      "+export function temporaryDeclaration(bindingIndex: number, temporaryNumber: number): o.Statement {\n",
      "+  return new o.DeclareVarStmt(temporaryName(bindingIndex, temporaryNumber), o.NULL_EXPR);\n",
      "+}\n",
      "+\n",
      "+function declareTemporaries(temporaries: number, bindingIndex: number, statements: o.Statement[]) {\n",
      "\n",
      "Actual comment:\n",
      " `prependTmpDecl` or something like that ? \n",
      "(so that we're sure at call site)\n",
      "\n",
      "\n",
      "Generated comment:\n",
      " <COMMENT offset=\"None\">I think this is a good example of a case where we should not use a temporary.\n",
      "\n",
      "The temporary is only used in the `convertCdExpressionToIr` function, and it is only used to store the result of a call to `o.variable`.\n",
      "\n",
      "The `o.variable` function returns a `ReadVarExpr` that is used to read the value of a variable. The `ReadVarExpr` is then used to create a new `ExpressionWithWrappedValueInfo` object.\n",
      "\n",
      "The `ExpressionWithWrappedValueInfo`\n",
      "\n",
      "--- Example 5 ---\n",
      "Input diff:\n",
      " @@ -6,18 +6,19 @@\n",
      "  * found in the LICENSE file at https://angular.io/license\n",
      "  */\n",
      " \n",
      "-import {ChangeDetectionStrategy, SchemaMetadata, Type, ViewEncapsulation} from '@angular/core';\n",
      "+import {BaseException, ChangeDetectionStrategy, SchemaMetadata, Type, ViewEncapsulation} from '@angular/core';\n",
      " \n",
      " import {LifecycleHooks, reflector} from '../core_private';\n",
      " \n",
      " import {ListWrapper, StringMapWrapper} from './facade/collection';\n",
      "-import {BaseException, unimplemented} from './facade/exceptions';\n",
      " import {isBlank, isPresent, isStringMap, normalizeBlank, normalizeBool} from './facade/lang';\n",
      " import {CssSelector} from './selector';\n",
      " import {getUrlScheme} from './url_resolver';\n",
      " import {sanitizeIdentifier, splitAtColon} from './util';\n",
      " \n",
      "-\n",
      "+function unimplemented(): any {\n",
      "\n",
      "Actual comment:\n",
      " new Error()\n",
      "\n",
      "\n",
      "Generated comment:\n",
      " package:angular/src/core/change_detection/interfaces.ts\n",
      "<COMMENT offset=\"101\">@angular/core\n",
      "<COMMENT offset=\"102\">\n",
      "<COMMENT offset=\"103\">\n",
      "<COMMENT offset=\"104\">\n",
      "<COMMENT offset=\"105\">\n",
      "<COMMENT offset=\"106\">\n",
      "<COMMENT offset=\"107\">\n",
      "<COMMENT offset=\"108\">\n",
      "<COMMENT offset=\"109\">\n",
      "<COMMENT offset\n",
      "\n",
      "--- Example 6 ---\n",
      "Input diff:\n",
      " export function main() {\n",
      "@@ -42,6 +42,13 @@ export function main() {\n",
      "         expect(headers.get('foo')).toBe('bar');\n",
      "         expect(headers.getAll('foo')).toEqual(['bar']);\n",
      "       });\n",
      "+      it('should not alter the values of a provided header template', () => {\n",
      "+        // Spec at https://fetch.spec.whatwg.org/#concept-headers-fill\n",
      "+        var firstHeaders = new Headers();\n",
      "\n",
      "Actual comment:\n",
      " var -> const\n",
      "\n",
      "\n",
      "Generated comment:\n",
      " package:\n",
      "<COMMENT\n",
      "\n",
      "--- Example 7 ---\n",
      "Input diff:\n",
      " export class DatePipe implements PipeTransform {\n",
      "@@ -104,7 +104,7 @@ export class DatePipe implements PipeTransform {\n",
      "     }\n",
      " \n",
      "     if (NumberWrapper.isNumeric(value)) {\n",
      "-      value = DateWrapper.fromMillis(NumberWrapper.parseInt(value, 10));\n",
      "+      value = DateWrapper.fromMillis(NumberWrapper.parseFloat(value));\n",
      "\n",
      "Actual comment:\n",
      " pls remove the use of NumberWrapper\n",
      "needs test\n",
      "\n",
      "\n",
      "Generated comment:\n",
      " package:\n",
      "<COMMENT offset=\"107\">I'm glad I could help.\n",
      "\n",
      "###\n",
      "\n",
      "--- Example 8 ---\n",
      "Input diff:\n",
      " class ApplyRedirects {\n",
      "@@ -244,10 +244,14 @@ class ApplyRedirects {\n",
      "     } else if (route.loadChildren) {\n",
      "       return runGuards(injector, route).mergeMap(shouldLoad => {\n",
      "         if (shouldLoad) {\n",
      "-          return this.configLoader.load(injector, route.loadChildren).map(r => {\n",
      "-            (<any>route)._loadedConfig = r;\n",
      "-            return r;\n",
      "-          });\n",
      "+          if ((<any>route)._loadedConfig) {\n",
      "+            return of ((<any>route)._loadedConfig);\n",
      "\n",
      "Actual comment:\n",
      " have you reported to clang ?\n",
      "\n",
      "\n",
      "Generated comment:\n",
      " <COMMENT offset=\"None\">\n",
      "\n",
      "--- Example 9 ---\n",
      "Input diff:\n",
      " @@ -9,40 +9,45 @@\n",
      " import {AfterContentChecked, AfterContentInit, AfterViewChecked, AfterViewInit, DoCheck, OnChanges, OnInit} from '@angular/core';\n",
      " import {Component, Directive} from '@angular/core/src/metadata';\n",
      " import {ViewMetadata} from '@angular/core/src/metadata/view';\n",
      "+import {TestBed} from '@angular/core/testing';\n",
      " import {AsyncTestCompleter, Log, TestComponentBuilder, beforeEach, beforeEachProviders, ddescribe, describe, expect, iit, inject, it, xdescribe, xit} from '@angular/core/testing/testing_internal';\n",
      " \n",
      " export function main() {\n",
      "   describe('directive lifecycle integration spec', () => {\n",
      "-\n",
      "+    let log: Log;\n",
      "     beforeEachProviders(() => { return [Log]; });\n",
      " \n",
      "+    beforeEach(() => {\n",
      "\n",
      "Actual comment:\n",
      " `compileComponents()` should always be in an `async`\n",
      "\n",
      "\n",
      "Generated comment:\n",
      " package:angular2/src/core/testing/testing_internal.d.ts\n",
      "<COMMENT offset=\"10\">@angular/core/testing/testing_internal.d.ts\n",
      "<COMMENT offset=\"11\">export function main() {\n",
      "<COMMENT offset=\"12\">  describe('directive lifecycle integration spec', () => {\n",
      "<COMMENT offset=\"13\">    let log: Log;\n",
      "<COMMENT offset=\"14\">    beforeEachProviders(() => { return [Log]; });\n",
      "<COMMENT offset=\"1\n",
      "\n",
      "--- Example 10 ---\n",
      "Input diff:\n",
      " module.exports = function(config) {\n",
      "@@ -6,7 +6,7 @@ var internalAngularReporter = require('./tools/karma/reporter.js');\n",
      " module.exports = function(config) {\n",
      "   config.set({\n",
      " \n",
      "-    frameworks: ['jasmine'],\n",
      "+    frameworks: ['jasmine', 'zonejs'],\n",
      "\n",
      "Actual comment:\n",
      " Please push `karma-zonejs` master to github :)\n",
      "\n",
      "\n",
      "Generated comment:\n",
      " <COMMENT offset=\"None\">\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(inference_prompts[:10], return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "model.eval()\n",
    "# Generate responses\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=128,\n",
    "        do_sample=False,\n",
    "        return_dict_in_generate=True,\n",
    "        output_scores=False\n",
    "    )\n",
    "\n",
    "# Decode and print results\n",
    "decoded = tokenizer.batch_decode(outputs.sequences, skip_special_tokens=True)\n",
    "\n",
    "for i, output_text in enumerate(decoded):\n",
    "    print(f\"\\n--- Example {i+1} ---\")\n",
    "    print(\"Input diff:\\n\", processed_dataset[i]['diff'])\n",
    "    print(\"\\nActual comment:\\n\", processed_dataset[i]['comment'])\n",
    "    print(\"\\nGenerated comment:\\n\", output_text.split(\"### Response:\")[-1].strip())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
