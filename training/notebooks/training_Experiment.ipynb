{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c27193b2-d1a3-4bb0-a0f4-43a1d86a7c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install necessary packages\n",
    "!pip install transformers datasets torch accelerate bitsandbytes sentencepiece peft trl bert-score mlflow\n",
    "!pip install evaluate bert-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f51947f0-2f03-41c0-8ad4-dccde63b331f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Libraries\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import gzip\n",
    "import gc\n",
    "import subprocess\n",
    "from types import MethodType\n",
    "\n",
    "# Core Libraries\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Hugging Face Transformers & Datasets\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    GenerationConfig,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    DataCollatorForSeq2Seq,\n",
    ")\n",
    "from transformers.cache_utils import Cache, DynamicCache\n",
    "\n",
    "# PEFT (Parameter-Efficient Fine-Tuning)\n",
    "from peft import (\n",
    "    get_peft_model,\n",
    "    LoraConfig,\n",
    "    TaskType,\n",
    "    prepare_model_for_kbit_training,\n",
    ")\n",
    "\n",
    "# TRL (Transformers Reinforcement Learning)\n",
    "from trl import SFTTrainer\n",
    "\n",
    "# Accelerate\n",
    "import accelerate\n",
    "from accelerate import infer_auto_device_map, dispatch_model\n",
    "\n",
    "# BERTScore\n",
    "from bert_score import score as bertscore\n",
    "\n",
    "# MLflow\n",
    "import mlflow\n",
    "import mlflow.pytorch\n",
    "\n",
    "import json\n",
    "import gzip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "40412002-1b79-4d0e-a258-131788abad70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a189512a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!~/rclone-v1.69.2-linux-amd64/rclone ls object:object-persist-project32\n",
    "\n",
    "!~/rclone-v1.69.2-linux-amd64/rclone lsd object_group:object-persist-group32/data/processed\n",
    "\n",
    "!~/rclone-v1.69.2-linux-amd64/rclone copy object_group:object-persist-group32/data/processed/train.jsonl.gz .\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "33c1d2bf-ebae-45f4-ada3-44d689c36219",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and process dataset\n",
    "guidelines = \"\"\"Key guidelines to follow:\n",
    "- Use standard Java libraries instead of external ones like Commons I/O when possible.\n",
    "- Avoid deprecated APIs, especially in Jenkins core and plugins.\n",
    "- Write clear, descriptive method and variable names.\n",
    "- Add or update tests when modifying functionality or fixing bugs.\n",
    "- Do not include commented-out code or leftover TODOs.\n",
    "- Update documentation if user-facing behavior changes.\n",
    "- Keep commits focused and avoid mixing unrelated changes.\n",
    "- Code must compile cleanly and pass all tests.\n",
    "- Maintain consistent formatting and follow Jenkins coding style.\n",
    "Also consider other good practices not explicitly listed above.\"\"\"\n",
    "def format_prompt(example):\n",
    "    offset = example.get('offset')\n",
    "    offset_info = f\"The comment refers to line {offset} in the diff.\" if offset is not None else \"\"\n",
    "\n",
    "    formatted_comment = (\n",
    "        f\"<COMMENT offset=\\\"{offset}\\\">{example['comment']}\\n\"\n",
    "        if offset is not None and example.get('comment')\n",
    "        else example.get('comment', '')\n",
    "    )\n",
    "\n",
    "    prompt = f\"\"\"### Instruction:\n",
    "You are a code reviewer for a Jenkins plugin. Review the following diff for potential improvements or guideline violations.\n",
    "{offset_info}\n",
    "\n",
    "{guidelines}\n",
    "\n",
    "### Input:\n",
    "Diff snippet:\n",
    "{example['diff']}\n",
    "\n",
    "### Response:\n",
    "{formatted_comment}\"\"\"\n",
    "\n",
    "    tokens = tokenizer(prompt, truncation=True, padding='max_length', max_length=1024)\n",
    "    tokens[\"labels\"] = tokens[\"input_ids\"].copy()\n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e056550-ce05-41da-b551-a8d89756aba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments, DataCollatorForLanguageModeling, DataCollatorForSeq2Seq\n",
    "from peft import get_peft_model, LoraConfig, TaskType, prepare_model_for_kbit_training\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5c89e1da-9612-4108-9a36-805b667ee7f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a40673fb3c494616bb2de90246f56934",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load tokenizer and model\n",
    "model_name = \"codellama/CodeLlama-7b-Instruct-hf\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Load the model with proper device mapping\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",\n",
    "    load_in_8bit=True,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# Prepare model for QLoRA training\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# Apply LoRA configuration\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    # task_type=TaskType.CAUSAL_LM\n",
    ")\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Set up data collator\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False  # This is key for causal LMs\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "061a9798-2462-4ba5-b2a4-fae50a72342f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First record of the processed dataset:\n",
      "{\n",
      "    \"diff\": \"@@ -6,10 +6,13 @@\\n  * found in the LICENSE file at https://angular.io/license\\n  */\\n \\n-import {unimplemented} from '../../facade/exceptions';\\n+import {BaseException} from '@angular/core';\\n import {isPresent} from '../../facade/lang';\\n import {AbstractControl} from '../model';\\n \\n+function unimplemented(): any {\\n+  throw new BaseException('unimplemented');\",\n",
      "    \"comment\": \"Is there a better way to avoid having this in different places?\\n\",\n",
      "    \"offset\": 9\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import gzip\n",
    "\n",
    "def load_and_filter_dataset(file_path):\n",
    "    \"\"\"\n",
    "    Loads a gzipped JSONL file and extracts specific fields.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): The path to the .jsonl.gz file.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of dictionaries, where each dictionary contains\n",
    "              'diff', 'comment', and 'offset' for each record.\n",
    "    \"\"\"\n",
    "    filtered_data = []\n",
    "    with gzip.open(file_path, \"rt\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            try:\n",
    "                record = json.loads(line)\n",
    "                # The 'diff' in your original data is the main part of the diff.\n",
    "                # The 'diff_hunk_header' is the line that usually starts with '@@'.\n",
    "                # We'll combine them if both exist, or use whichever is present.\n",
    "                \n",
    "                diff_parts = []\n",
    "                if 'diff_hunk_header' in record and record['diff_hunk_header']:\n",
    "                    diff_parts.append(record['diff_hunk_header'])\n",
    "                if 'diff' in record and record['diff']:\n",
    "                    diff_parts.append(record['diff'])\n",
    "                \n",
    "                full_diff = \"\\n\".join(diff_parts) if diff_parts else None\n",
    "\n",
    "                filtered_record = {\n",
    "                    'diff': full_diff,\n",
    "                    'comment': record.get('comment_body'), # Use .get() for safety if key might be missing\n",
    "                    'offset': record.get('line_offset')   # Use .get() for safety\n",
    "                }\n",
    "                filtered_data.append(filtered_record)\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Skipping line due to JSON decode error: {e} - Line: {line.strip()}\")\n",
    "            except KeyError as e:\n",
    "                print(f\"Skipping record due to missing key: {e} - Record: {record}\")\n",
    "    return filtered_data\n",
    "\n",
    "# --- --- --- --- --- --- --- --- --- --- --- --- --- --- ---\n",
    "#                  Example Usage:\n",
    "# --- --- --- --- --- --- --- --- --- --- --- --- --- --- ---\n",
    "\n",
    "# 1. Make sure 'train.jsonl.gz' is in the same directory as your script,\n",
    "#    or provide the full path to the file.\n",
    "file_path = \"train.jsonl.gz\"\n",
    "processed_dataset = load_and_filter_dataset(file_path)\n",
    "\n",
    "# 2. View the first record of your new dataset\n",
    "if processed_dataset:\n",
    "    print(\"First record of the processed dataset:\")\n",
    "    print(json.dumps(processed_dataset[0], indent=4)) # Pretty print the JSON\n",
    "else:\n",
    "    print(\"No data was processed. Check your file path and file content.\")\n",
    "\n",
    "# 3. To see more records (e.g., the first 5):\n",
    "# if processed_dataset:\n",
    "#     print(\"\\nFirst 5 records of the processed dataset:\")\n",
    "#     for i, record in enumerate(processed_dataset[:5]):\n",
    "#         print(f\"\\n--- Record {i+1} ---\")\n",
    "#         print(json.dumps(record, indent=4))\n",
    "\n",
    "file_path = \"train.jsonl.gz\"\n",
    "processed_dataset = load_and_filter_dataset(file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "00375ea3-5056-4133-8a31-0fcbbc99af43",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Tokenize data\n",
    "tokenized_data = [format_prompt(e) for e in processed_dataset if e['diff'] and e['comment']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9c5eab1e-163b-4c06-a1f5-f8ce94fc2b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Dataset.from_list(tokenized_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "732d9127-e669-43f4-8a16-ffb3402bee40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERTScore metric for evaluation\n",
    "import evaluate\n",
    "bertscore = evaluate.load(\"bertscore\")\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    scores = bertscore.compute(predictions=decoded_preds, references=decoded_labels, lang=\"en\")\n",
    "    avg_f1_score = sum(scores[\"f1\"]) / len(scores[\"f1\"])\n",
    "\n",
    "    # Log BERTScore to MLflow\n",
    "    mlflow.log_metric(\"bertscore_f1\", avg_f1_score)\n",
    "\n",
    "    # Return metrics for Trainer\n",
    "    return {\"bertscore_f1\": avg_f1_score}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ba9cdc60-d15e-4104-adaa-4db8221d412c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import mlflow\n",
    "# from transformers import Trainer, TrainingArguments\n",
    "# from datetime import datetime\n",
    "# import torch\n",
    "# import time\n",
    "\n",
    "# def run_experiment(exp_name, config):\n",
    "#     run_name = f\"{exp_name}-{datetime.now().strftime('%H%M%S')}\"\n",
    "#     output_dir = f\"./qlora_output/{run_name}\"\n",
    "\n",
    "#     mlflow.set_tracking_uri(\"mlexp\")  # or custom URI\n",
    "#     mlflow.set_experiment(\"qlora-fast-experiments-valid\")  # A valid name\n",
    "\n",
    "\n",
    "#     args = TrainingArguments(\n",
    "#         output_dir=output_dir,\n",
    "#         per_device_train_batch_size=config[\"batch_size\"],\n",
    "#         gradient_accumulation_steps=config[\"grad_accum\"],\n",
    "#         learning_rate=config[\"lr\"],\n",
    "#         num_train_epochs=1,\n",
    "#         logging_steps=10,\n",
    "#         save_steps=200,\n",
    "#         eval_steps=200,\n",
    "#         save_total_limit=1,\n",
    "#         eval_strategy=\"steps\",\n",
    "#         save_strategy=\"steps\",\n",
    "#         load_best_model_at_end=False,\n",
    "#         fp16=config.get(\"fp16\", False),\n",
    "#         bf16=config.get(\"bf16\", False),\n",
    "#         gradient_checkpointing=config.get(\"grad_ckpt\", False),\n",
    "#         report_to=[\"mlflow\"],\n",
    "#         logging_dir=f\"./logs/{run_name}\",\n",
    "#         dataloader_num_workers=2\n",
    "#     )\n",
    "\n",
    "#     trainer = Trainer(\n",
    "#         model=model,\n",
    "#         args=args,\n",
    "#         train_dataset=train_dataset.select(range(20)),\n",
    "#         eval_dataset=train_dataset.select(range(5)),\n",
    "#         tokenizer=tokenizer,\n",
    "#         data_collator=data_collator,\n",
    "#         compute_metrics=compute_metrics,\n",
    "#     )\n",
    "\n",
    "#     with mlflow.start_run(run_name=run_name):\n",
    "#         print(f\"\\nðŸ”§ Running {run_name} with config: {config}\")\n",
    "#         train_result = trainer.train()\n",
    "        \n",
    "#         # Measure inference time for evaluation\n",
    "#         start_time = time.time()\n",
    "#         trainer.evaluate()\n",
    "#         inference_time = time.time() - start_time\n",
    "\n",
    "#         # Log metrics to MLflow\n",
    "#         mlflow.log_params(config)\n",
    "#         mlflow.log_metric(\"inference_time\", inference_time)\n",
    "#         # mlflow.pytorch.log_model(model, artifact_path=\"model\")\n",
    "\n",
    "#         # Print and return summary\n",
    "#         print(f\"âœ… {run_name} finished.\")\n",
    "#         print(f\"   â†’ Inference Time: {inference_time:.4f} seconds\")\n",
    "#         return {**config, \"inference_time\": inference_time}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0bd7c14b-a71f-4328-9e05-8f855182cda5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# experiments = [\n",
    "#     {\"batch_size\": 4, \"grad_accum\": 2, \"lr\": 2e-4, \"fp16\": True},\n",
    "#     {\"batch_size\": 8, \"grad_accum\": 1, \"lr\": 2e-4, \"fp16\": True},\n",
    "#     {\"batch_size\": 6, \"grad_accum\": 1, \"lr\": 1e-4, \"fp16\": True, \"grad_ckpt\": True},\n",
    "#     {\"batch_size\": 8, \"grad_accum\": 1, \"lr\": 2e-4, \"fp16\": False, \"bf16\": True, \"grad_ckpt\": True},\n",
    "# ]\n",
    "\n",
    "# results = []\n",
    "# for i, config in enumerate(experiments):\n",
    "#     result = run_experiment(f\"exp{i+1}\", config)\n",
    "#     results.append(result)\n",
    "\n",
    "# # Find best by Inference Time (lower is better)\n",
    "# best = min(results, key=lambda r: r[\"inference_time\"])\n",
    "# print(\"\\nðŸ† Best Config (Shortest Inference Time):\")\n",
    "# print(best)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aab5448f-ddd2-45e7-bafb-a1c2f28e6f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import shutil\n",
    "\n",
    "# # Specify the folder you want to zip\n",
    "# folder_path = 'mlexp/697636671313419820'\n",
    "# zip_name = 'mlexp.zip'\n",
    "\n",
    "# # Create a zip file from the folder\n",
    "# shutil.make_archive(zip_name, 'zip', folder_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b561c8d6-6892-4a55-87f3-4fd26cbb4d41",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_946/4178041547.py:39: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "No label_names provided for model class `PeftModel`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer, IntervalStrategy\n",
    "\n",
    "# Training Arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./qlora_output\",\n",
    "    per_device_train_batch_size=8,         # Increase batch size for better GPU utilization\n",
    "    gradient_accumulation_steps=8,         # Accumulate gradients over 2 steps\n",
    "    learning_rate=2e-4,\n",
    "    num_train_epochs=1,                    # Increase epochs for the full dataset\n",
    "    logging_steps=100,\n",
    "    # save_steps=1000,                       # Save the model every 1000 steps\n",
    "    # eval_steps=100,                        # Evaluate every 100 steps\n",
    "    # save_total_limit=2,                    # Keep the latest 2 checkpoints\n",
    "    eval_strategy=\"no\",  # Evaluate after every 100 steps\n",
    "    save_strategy=\"no\", # Save after every 1000 steps\n",
    "    load_best_model_at_end=False,           # Load the best model based on evaluation\n",
    "    report_to=[\"mlflow\"],\n",
    "    logging_dir=\"./logs\",\n",
    "    fp16=True,                             # Ensure FP16 is enabled\n",
    ")\n",
    "\n",
    "# Function to compute BERTScore for evaluation\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Compute BERTScore\n",
    "    scores = bertscore.compute(predictions=decoded_preds, references=decoded_labels, lang=\"en\")\n",
    "    avg_f1_score = sum(scores[\"f1\"]) / len(scores[\"f1\"])\n",
    "\n",
    "    # Log BERTScore to MLflow\n",
    "    mlflow.log_metric(\"bertscore_f1\", avg_f1_score)\n",
    "\n",
    "    # Return metrics for Trainer\n",
    "    return {\"bertscore_f1\": avg_f1_score}\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=train_dataset.select(range(100)),  # Evaluate on a subset, can be changed\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,  # Use custom BERTScore metric\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "41c146b4-c90b-4240-8298-f1b42911d678",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "device() received an invalid combination of arguments - got (NoneType), but expected one of:\n * (torch.device device)\n      didn't match because some of the arguments have invalid types: (!NoneType!)\n * (str type, int index = -1)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Now proceed with starting the MLflow run and training\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m mlflow\u001b[38;5;241m.\u001b[39mstart_run():\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;66;03m# Log model and other information\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m     \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/transformers/trainer.py:2245\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2243\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2244\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2245\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2246\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2247\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2248\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2249\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2250\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/transformers/trainer.py:2374\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2372\u001b[0m         model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mprepare(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel)\n\u001b[1;32m   2373\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2374\u001b[0m         model, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprepare\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2375\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2376\u001b[0m     \u001b[38;5;66;03m# to handle cases wherein we pass \"DummyScheduler\" such as when it is specified in DeepSpeed config.\u001b[39;00m\n\u001b[1;32m   2377\u001b[0m     model, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlr_scheduler \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mprepare(\n\u001b[1;32m   2378\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlr_scheduler\n\u001b[1;32m   2379\u001b[0m     )\n",
      "File \u001b[0;32m/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/accelerate/accelerator.py:1446\u001b[0m, in \u001b[0;36mAccelerator.prepare\u001b[0;34m(self, device_placement, *args)\u001b[0m\n\u001b[1;32m   1444\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp8_backend \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMSAMP\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1445\u001b[0m         args, device_placement \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_msamp(\u001b[38;5;241m*\u001b[39margs, device_placement\u001b[38;5;241m=\u001b[39mdevice_placement)\n\u001b[0;32m-> 1446\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1447\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prepare_one\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfirst_pass\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_placement\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43md\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_placement\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1448\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1449\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_one(obj, device_placement\u001b[38;5;241m=\u001b[39md) \u001b[38;5;28;01mfor\u001b[39;00m obj, d \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(result, device_placement))\n\u001b[1;32m   1450\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_fix_optimizer:\n\u001b[1;32m   1451\u001b[0m     \u001b[38;5;66;03m# 2. grabbing new model parameters\u001b[39;00m\n",
      "File \u001b[0;32m/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/accelerate/accelerator.py:1447\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1444\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp8_backend \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMSAMP\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1445\u001b[0m         args, device_placement \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_msamp(\u001b[38;5;241m*\u001b[39margs, device_placement\u001b[38;5;241m=\u001b[39mdevice_placement)\n\u001b[1;32m   1446\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(\n\u001b[0;32m-> 1447\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prepare_one\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfirst_pass\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_placement\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43md\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m obj, d \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(args, device_placement)\n\u001b[1;32m   1448\u001b[0m     )\n\u001b[1;32m   1449\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_one(obj, device_placement\u001b[38;5;241m=\u001b[39md) \u001b[38;5;28;01mfor\u001b[39;00m obj, d \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(result, device_placement))\n\u001b[1;32m   1450\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_fix_optimizer:\n\u001b[1;32m   1451\u001b[0m     \u001b[38;5;66;03m# 2. grabbing new model parameters\u001b[39;00m\n",
      "File \u001b[0;32m/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/accelerate/accelerator.py:1289\u001b[0m, in \u001b[0;36mAccelerator._prepare_one\u001b[0;34m(self, obj, first_pass, device_placement)\u001b[0m\n\u001b[1;32m   1287\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_data_loader(obj, device_placement\u001b[38;5;241m=\u001b[39mdevice_placement)\n\u001b[1;32m   1288\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mModule):\n\u001b[0;32m-> 1289\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprepare_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_placement\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_placement\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1290\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mOptimizer):\n\u001b[1;32m   1291\u001b[0m     optimizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_optimizer(obj, device_placement\u001b[38;5;241m=\u001b[39mdevice_placement)\n",
      "File \u001b[0;32m/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/accelerate/accelerator.py:1556\u001b[0m, in \u001b[0;36mAccelerator.prepare_model\u001b[0;34m(self, model, device_placement, evaluation_mode)\u001b[0m\n\u001b[1;32m   1553\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m is_bitsandbytes_multi_backend_available():\n\u001b[1;32m   1554\u001b[0m     \u001b[38;5;66;03m# bnb with multi-backend supports CPU which don't need to check index.\u001b[39;00m\n\u001b[1;32m   1555\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m-> 1556\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurrent_device_index\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice:\n\u001b[1;32m   1557\u001b[0m     \u001b[38;5;66;03m# if on the first device (GPU 0) we don't care\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mindex \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mor\u001b[39;00m (current_device_index \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m):\n\u001b[1;32m   1559\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1560\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou can\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt train a model that has been loaded in 8-bit or 4-bit precision on a different device than the one \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1561\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mre training on. Make sure you loaded the model on the correct device using for example `device_map=\u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m:torch.cuda.current_device()}` or `device_map=\u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m:torch.xpu.current_device()}`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1562\u001b[0m         )\n",
      "\u001b[0;31mTypeError\u001b[0m: device() received an invalid combination of arguments - got (NoneType), but expected one of:\n * (torch.device device)\n      didn't match because some of the arguments have invalid types: (!NoneType!)\n * (str type, int index = -1)\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "\n",
    "# Set the experiment name to a valid name (make sure it's not '.ipynb_checkpoints')\n",
    "mlflow.set_tracking_uri(\"mlruns\")  # or custom URI if you're using one\n",
    "mlflow.set_experiment(\"qlora_finetune_experiment\")  # Set a custom experiment name\n",
    "\n",
    "# Now proceed with starting the MLflow run and training\n",
    "with mlflow.start_run():\n",
    "    # Log model and other information\n",
    "    trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e1d39a-01ab-45a2-ac29-c486624580f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
