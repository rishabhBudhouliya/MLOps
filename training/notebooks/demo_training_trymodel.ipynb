{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1zbVsVMrtAKV"
   },
   "source": [
    "### Open the notebook on Colab\n",
    "\n",
    "We should have already started a notebook server in a container on a Chameleon GPU host, and set up an SSH tunnel to this notebook server. Now, we will connect this notebook to the runtime that you have in Chameleon. This is a convenient way to work, because the notebook and its outputs will be saved automatically in your Google Drive.\n",
    "\n",
    "-   Next to the ‚ÄúConnect‚Äù button in the top right, there is a ‚ñº symbol. Click on this symbol to expand the menu, and choose ‚ÄúConnect to a local runtime‚Äù.\n",
    "-   Paste the `http://127.0.0.1:8888/lab?token=XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX` you copied earlier into this space, and choose ‚ÄúConnect‚Äù.\n",
    "\n",
    "**Alternatively, if you prefer not to use Colab** (or can‚Äôt, for some reason): just put the `http://127.0.0.1:8888/lab?token=XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX` URL you copied earlier into your browser to open the Jupyter interface directly. But, then you‚Äôll have to open a terminal in that Jupyter interface and run\n",
    "\n",
    "    wget https://raw.githubusercontent.com/teaching-on-testbeds/llm-chi/refs/heads/main/workspace/2_single_gpu_a100.ipynb\n",
    "\n",
    "to get a copy of this notebook in that workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zHZYnVIv-0Tp",
    "outputId": "c5d3bcdb-a6fd-49e3-c4f5-0a7334aed03e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.12/site-packages (4.51.3)\n",
      "Requirement already satisfied: datasets in /opt/conda/lib/python3.12/site-packages (3.5.1)\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.12/site-packages (2.5.1+cu124)\n",
      "Requirement already satisfied: accelerate in /opt/conda/lib/python3.12/site-packages (1.6.0)\n",
      "Requirement already satisfied: bitsandbytes in /opt/conda/lib/python3.12/site-packages (0.45.5)\n",
      "Requirement already satisfied: sentencepiece in /opt/conda/lib/python3.12/site-packages (0.2.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.12/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /opt/conda/lib/python3.12/site-packages (from transformers) (0.30.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.12/site-packages (from transformers) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.12/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.12/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.12/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.12/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /opt/conda/lib/python3.12/site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /opt/conda/lib/python3.12/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.12/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.12/site-packages (from datasets) (19.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.12/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.12/site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.12/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /opt/conda/lib/python3.12/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /opt/conda/lib/python3.12/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2024.12.0)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.12/site-packages (from datasets) (3.11.18)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.12/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.12/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.12/site-packages (from torch) (3.1.5)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /opt/conda/lib/python3.12/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /opt/conda/lib/python3.12/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /opt/conda/lib/python3.12/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /opt/conda/lib/python3.12/site-packages (from torch) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /opt/conda/lib/python3.12/site-packages (from torch) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /opt/conda/lib/python3.12/site-packages (from torch) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /opt/conda/lib/python3.12/site-packages (from torch) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /opt/conda/lib/python3.12/site-packages (from torch) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /opt/conda/lib/python3.12/site-packages (from torch) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /opt/conda/lib/python3.12/site-packages (from torch) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /opt/conda/lib/python3.12/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /opt/conda/lib/python3.12/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: triton==3.1.0 in /opt/conda/lib/python3.12/site-packages (from torch) (3.1.0)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.12/site-packages (from torch) (75.8.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /opt/conda/lib/python3.12/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.12/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.12/site-packages (from accelerate) (6.1.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/conda/lib/python3.12/site-packages (from aiohttp->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.12/site-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.12/site-packages (from aiohttp->datasets) (25.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.12/site-packages (from aiohttp->datasets) (1.6.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.12/site-packages (from aiohttp->datasets) (6.4.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/conda/lib/python3.12/site-packages (from aiohttp->datasets) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/conda/lib/python3.12/site-packages (from aiohttp->datasets) (1.20.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.12/site-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.12/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.12/site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.12/site-packages (from requests->transformers) (2024.12.14)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.12/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.12/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.12/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.12/site-packages (from pandas->datasets) (2025.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "# Install necessary packages\n",
    "!pip install transformers datasets torch accelerate bitsandbytes sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "xla7QT9E-1rt"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "from transformers import GenerationConfig\n",
    "import time\n",
    "import accelerate\n",
    "from accelerate import infer_auto_device_map, dispatch_model\n",
    "from types import MethodType\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "T2y1q_In-4B1"
   },
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "FgGpCXdKGxO5"
   },
   "outputs": [],
   "source": [
    "# Function to load models and apply quantization\n",
    "from transformers import BitsAndBytesConfig\n",
    "def load_and_quantize_model(model_name):\n",
    "    # Load tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True, use_fast=False)\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=torch.bfloat16,  # You can use float16, but A100 supports bfloat16 very well\n",
    "        device_map=\"auto\",\n",
    "        low_cpu_mem_usage=True,\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "    model.eval()\n",
    "    return tokenizer, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "6JH2y6p3JRtg"
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "def print_gpu_memory(note=\"\"):\n",
    "    result = subprocess.run(\n",
    "        [\"nvidia-smi\", \"--query-gpu=memory.used,memory.total\", \"--format=csv,nounits,noheader\"],\n",
    "        stdout=subprocess.PIPE,\n",
    "        text=True\n",
    "    )\n",
    "    used, total = map(int, result.stdout.strip().split(','))\n",
    "    print(f\"{note} GPU memory: {used} MiB / {total} MiB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "quI83E-574lJ"
   },
   "outputs": [],
   "source": [
    "\n",
    "def generate_code_with_profiling(model, tokenizer, prompt, max_new_tokens=64):\n",
    "    # Tokenize input and move to model's device\n",
    "    device = next(model.parameters()).device  # Get the assigned device\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)  # Move all inputs to device\n",
    "\n",
    "    # Clear the GPU cache to ensure accurate measurement\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.synchronize()  # Ensure previous GPU tasks are done\n",
    "\n",
    "    with torch.no_grad():\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Track memory usage before\n",
    "        memory_before = torch.cuda.memory_allocated(device)\n",
    "        memory_reserved_before = torch.cuda.memory_reserved(device)\n",
    "\n",
    "        # Generate output\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=False,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            use_cache=True\n",
    "        )\n",
    "\n",
    "        torch.cuda.synchronize()  # Ensure generation completes\n",
    "\n",
    "        # Track memory usage after\n",
    "        memory_after = torch.cuda.memory_allocated(device)\n",
    "        memory_reserved_after = torch.cuda.memory_reserved(device)\n",
    "\n",
    "        end_time = time.time()\n",
    "\n",
    "    # Calculate stats\n",
    "    inference_time = end_time - start_time\n",
    "    memory_usage = memory_after - memory_before\n",
    "    reserved_memory_usage = memory_reserved_after - memory_reserved_before  # Optional, but useful\n",
    "\n",
    "    decoded_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    # Optionally print out reserved memory for diagnostics\n",
    "    print(f\"Memory Usage: {memory_usage} bytes\")\n",
    "    print(f\"Reserved Memory: {reserved_memory_usage} bytes\")\n",
    "\n",
    "    return decoded_output, inference_time, memory_usage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "8kA0j3Sd-uaY"
   },
   "outputs": [],
   "source": [
    "# Load CoNaLa dataset\n",
    "dataset = load_dataset(\"neulab/conala\", split=\"train[:2]\", trust_remote_code=True)  # Just a few samples for quick test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53,
     "referenced_widgets": [
      "a02fa0ff23af423786fb82bf26086f16",
      "f51895fe8c7e488bb1dbe6c65d034601",
      "25488b6e2e934e20abe409bbee4e5969",
      "01bba12c03e74891b28bb338c4d3968f",
      "5f087d26f22d4db1947e471926b544a8",
      "fba750810c774dc288fa0d908b22efba",
      "169d733b095e4fca96b652c57e5bc7de",
      "5aa7564ae9c248a8b5b8b1dc38c8f88d",
      "605c7d2e8b0c49689b89c266e082c124",
      "799dec9522d54ba6b5201d61c01fb8e2",
      "df6f0fe1d3ac4a0abd77f77a5c679bef"
     ]
    },
    "id": "wEVIzX42QfLf",
    "outputId": "57da6083-2ab7-4d7d-e19a-5309644501e9"
   },
   "outputs": [],
   "source": [
    "# # OpenCoder-Instruct\n",
    "# model_name = \"OpenCoder-Instruct\"\n",
    "# model_address = \"infly/OpenCoder-8B-Instruct\"\n",
    "# print_gpu_memory(\"Before loading model\")\n",
    "# tokenizer, loaded_model = load_and_quantize_model(model_address)\n",
    "# print_gpu_memory(\"After loading model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 327
    },
    "id": "ezx9QhRsZcQC",
    "outputId": "789722fb-06d9-4086-cbf6-0c184b11acbd"
   },
   "outputs": [],
   "source": [
    "# for idx, sample in enumerate(dataset):\n",
    "#     print(f\"\\n==================== Sample {idx + 1} ====================\")\n",
    "#     print(f\"Intent: {sample['intent']}\")\n",
    "#     prompt = f\"### Instruction:\\n{sample['intent']}\\n\\n### Response:\"\n",
    "#     output, inference_time, memory_usage = generate_code_with_profiling(loaded_model, tokenizer, prompt)\n",
    "#     print(f\"\\nüîπ Output from {model_name}:\\n{output}\")\n",
    "#     print(f\"‚è±Ô∏è Inference time: {inference_time:.4f} seconds\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del loaded_model \n",
    "# del tokenizer\n",
    "# gc.collect()\n",
    "# torch.cuda.empty_cache()\n",
    "# print_gpu_memory(\"After freeing previous model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def patched_prepare_inputs_for_generation(\n",
    "    self,\n",
    "    input_ids,\n",
    "    past_key_values=None,\n",
    "    attention_mask=None,\n",
    "    inputs_embeds=None,\n",
    "    **kwargs,\n",
    "):\n",
    "    if past_key_values is not None:\n",
    "        if isinstance(past_key_values, Cache):\n",
    "            cache_length = past_key_values.get_seq_length()\n",
    "            past_length = past_key_values.seen_tokens\n",
    "            max_cache_length = past_key_values.get_max_cache_shape()\n",
    "        else:\n",
    "            cache_length = past_length = past_key_values[0][0].shape[2]\n",
    "            max_cache_length = None\n",
    "\n",
    "        # Keep only the unprocessed tokens:\n",
    "        # 1 - If the length of the attention_mask exceeds the length of input_ids, then we are in a setting where\n",
    "        # some of the inputs are exclusivelly passed as part of the cache (e.g. when passing input_embeds as\n",
    "        # input)\n",
    "        if (\n",
    "            attention_mask is not None\n",
    "            and attention_mask.shape[1] > input_ids.shape[1]\n",
    "        ):\n",
    "            input_ids = input_ids[:, -(attention_mask.shape[1] - past_length) :]\n",
    "        # 2 - If the past_length is smaller than input_ids', then input_ids holds all input tokens. We can discard\n",
    "        # input_ids based on the past_length.\n",
    "        elif past_length < input_ids.shape[1]:\n",
    "            input_ids = input_ids[:, past_length:]\n",
    "        # 3 - Otherwise (past_length >= input_ids.shape[1]), let's assume input_ids only has unprocessed tokens.\n",
    "\n",
    "        # If we are about to go beyond the maximum cache length, we need to crop the input attention mask.\n",
    "        if (\n",
    "            max_cache_length is not None\n",
    "            and attention_mask is not None\n",
    "            and cache_length + input_ids.shape[1] > max_cache_length\n",
    "        ):\n",
    "            attention_mask = attention_mask[:, -max_cache_length:]\n",
    "\n",
    "    position_ids = kwargs.get(\"position_ids\", None)\n",
    "    if attention_mask is not None and position_ids is None:\n",
    "        # create position_ids on the fly for batch generation\n",
    "        position_ids = attention_mask.long().cumsum(-1) - 1\n",
    "        position_ids.masked_fill_(attention_mask == 0, 1)\n",
    "        if past_key_values:\n",
    "            position_ids = position_ids[:, -input_ids.shape[1] :]\n",
    "\n",
    "    # if `inputs_embeds` are passed, we only want to use them in the 1st generation step\n",
    "    if inputs_embeds is not None and past_key_values is None:\n",
    "        model_inputs = {\"inputs_embeds\": inputs_embeds}\n",
    "    else:\n",
    "        model_inputs = {\"input_ids\": input_ids}\n",
    "\n",
    "    model_inputs.update(\n",
    "        {\n",
    "            \"position_ids\": position_ids,\n",
    "            \"past_key_values\": past_key_values,\n",
    "            \"use_cache\": kwargs.get(\"use_cache\"),\n",
    "            \"attention_mask\": attention_mask,\n",
    "        }\n",
    "    )\n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "def load_and_patch_model(path):\n",
    "    tokenizer, model = load_and_quantize_model(path)\n",
    "    model.prepare_inputs_for_generation = MethodType(patched_prepare_inputs_for_generation, model)\n",
    "    return tokenizer, model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before loading model GPU memory: 4 MiB / 81920 MiB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d19e8808e324e9ab4109aa2760c1c44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After loading model GPU memory: 31469 MiB / 81920 MiB\n"
     ]
    }
   ],
   "source": [
    "# DeepSeek Coder V2\n",
    "model_name = \"Lite-Base\"\n",
    "model_address = \"deepseek-ai/DeepSeek-Coder-V2-Lite-Base\"\n",
    "print_gpu_memory(\"Before loading model\")\n",
    "tokenizer, loaded_model = load_and_patch_model(model_address)\n",
    "print_gpu_memory(\"After loading model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(loaded_model.prepare_inputs_for_generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.cache_utils import Cache, DynamicCache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n",
      "The `seen_tokens` attribute is deprecated and will be removed in v4.41. Use the `cache_position` model input instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== Sample 1 ====================\n",
      "Intent: How to convert a list of multiple integers into a single integer?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory Usage: -1123333120 bytes\n",
      "Reserved Memory: 50331648 bytes\n",
      "\n",
      "üîπ Output from Lite-Base:\n",
      "### Instruction:\n",
      "How to convert a list of multiple integers into a single integer?\n",
      "\n",
      "### Response:\n",
      "To convert a list of multiple integers into a single integer, you can use the following steps:\n",
      "\n",
      "1. Initialize an empty string variable to store the result.\n",
      "2. Iterate over the list of integers.\n",
      "3. For each integer, convert it to a string and append it to the result string\n",
      "‚è±Ô∏è Inference time: 4.4278 seconds\n",
      "\n",
      "\n",
      "==================== Sample 2 ====================\n",
      "Intent: How to convert a list of multiple integers into a single integer?\n",
      "Memory Usage: 1024 bytes\n",
      "Reserved Memory: 14680064 bytes\n",
      "\n",
      "üîπ Output from Lite-Base:\n",
      "### Instruction:\n",
      "How to convert a list of multiple integers into a single integer?\n",
      "\n",
      "### Response:\n",
      "To convert a list of multiple integers into a single integer, you can use the following steps:\n",
      "\n",
      "1. Initialize an empty string variable to store the result.\n",
      "2. Iterate over the list of integers.\n",
      "3. For each integer, convert it to a string and append it to the result string\n",
      "‚è±Ô∏è Inference time: 3.6950 seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for idx, sample in enumerate(dataset):\n",
    "    print(f\"\\n==================== Sample {idx + 1} ====================\")\n",
    "    print(f\"Intent: {sample['intent']}\")\n",
    "    prompt = f\"### Instruction:\\n{sample['intent']}\\n\\n### Response:\"\n",
    "    output, inference_time, memory_usage = generate_code_with_profiling(loaded_model, tokenizer, prompt)\n",
    "    print(f\"\\nüîπ Output from {model_name}:\\n{output}\")\n",
    "    print(f\"‚è±Ô∏è Inference time: {inference_time:.4f} seconds\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if hasattr(model, 'clear_cache'):\n",
    "#     loaded_model.clear_cache()\n",
    "# del loaded_model \n",
    "# del tokenizer\n",
    "# gc.collect()\n",
    "# torch.cuda.empty_cache()\n",
    "# print_gpu_memory(\"After freeing previous model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "01bba12c03e74891b28bb338c4d3968f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "2.0.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_allow_html": false,
      "layout": "IPY_MODEL_799dec9522d54ba6b5201d61c01fb8e2",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_df6f0fe1d3ac4a0abd77f77a5c679bef",
      "tabbable": null,
      "tooltip": null,
      "value": "‚Äá4/4‚Äá[00:02&lt;00:00,‚Äá‚Äá1.29it/s]"
     }
    },
    "169d733b095e4fca96b652c57e5bc7de": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "HTMLStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "HTMLStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "StyleView",
      "background": null,
      "description_width": "",
      "font_size": null,
      "text_color": null
     }
    },
    "25488b6e2e934e20abe409bbee4e5969": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "2.0.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_allow_html": false,
      "layout": "IPY_MODEL_5aa7564ae9c248a8b5b8b1dc38c8f88d",
      "max": 4,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_605c7d2e8b0c49689b89c266e082c124",
      "tabbable": null,
      "tooltip": null,
      "value": 4
     }
    },
    "5aa7564ae9c248a8b5b8b1dc38c8f88d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "2.0.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "2.0.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border_bottom": null,
      "border_left": null,
      "border_right": null,
      "border_top": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5f087d26f22d4db1947e471926b544a8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "2.0.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "2.0.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border_bottom": null,
      "border_left": null,
      "border_right": null,
      "border_top": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "605c7d2e8b0c49689b89c266e082c124": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "799dec9522d54ba6b5201d61c01fb8e2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "2.0.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "2.0.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border_bottom": null,
      "border_left": null,
      "border_right": null,
      "border_top": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a02fa0ff23af423786fb82bf26086f16": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "2.0.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_f51895fe8c7e488bb1dbe6c65d034601",
       "IPY_MODEL_25488b6e2e934e20abe409bbee4e5969",
       "IPY_MODEL_01bba12c03e74891b28bb338c4d3968f"
      ],
      "layout": "IPY_MODEL_5f087d26f22d4db1947e471926b544a8",
      "tabbable": null,
      "tooltip": null
     }
    },
    "df6f0fe1d3ac4a0abd77f77a5c679bef": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "HTMLStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "HTMLStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "StyleView",
      "background": null,
      "description_width": "",
      "font_size": null,
      "text_color": null
     }
    },
    "f51895fe8c7e488bb1dbe6c65d034601": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "2.0.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_allow_html": false,
      "layout": "IPY_MODEL_fba750810c774dc288fa0d908b22efba",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_169d733b095e4fca96b652c57e5bc7de",
      "tabbable": null,
      "tooltip": null,
      "value": "Loading‚Äácheckpoint‚Äáshards:‚Äá100%"
     }
    },
    "fba750810c774dc288fa0d908b22efba": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "2.0.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "2.0.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border_bottom": null,
      "border_left": null,
      "border_right": null,
      "border_top": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
