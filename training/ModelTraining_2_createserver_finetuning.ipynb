{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AwdS5R320DZw"
   },
   "source": [
    "#Launching a Jupyter Notebook Manually on Chameleon\n",
    "1. Go to the Chameleon Dashboard\n",
    "Visit: https://www.chameleoncloud.org/\n",
    "\n",
    "Sign in and choose your project allocation (if you’re in a project group, select the correct one)\n",
    "\n",
    "2. Launch a Jupyter Environment\n",
    "In the top menu, click \"User Interfaces\" → \"Jupyter\"\n",
    "\n",
    "Choose your site (e.g., UC/TACC/CHI@TACC)\n",
    "\n",
    "Click \"Launch Jupyter\"\n",
    "\n",
    "Select a CUDA-enabled environment (like CC-Ubuntu24.04-CUDA or CC-Ubuntu20.04-CUDA)\n",
    "\n",
    "Under Extra Options, you can specify:\n",
    "\n",
    "Number of vCPUs (4–8 is fine)\n",
    "\n",
    "RAM (16GB+ recommended)\n",
    "\n",
    "GPU (A100 if available and you plan to fine-tune)\n",
    "\n",
    "3. Once It’s Launched\n",
    "You’ll be redirected to a JupyterLab interface\n",
    "\n",
    "Open a terminal inside JupyterLab\n",
    "\n",
    "Clone your repo (or download notebooks directly):\n",
    "\n",
    "bash\n",
    "Copy\n",
    "Edit\n",
    "git clone https://github.com/your-username/your-repo.git\n",
    "cd your-repo\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hDL7NwIYx7Ob"
   },
   "source": [
    "## Bring up a GPU server\n",
    "\n",
    "At the beginning of the lease time, we will bring up our GPU server. We will use the `python-chi` Python API to Chameleon to provision our server.\n",
    "\n",
    "We will execute the cells in this notebook inside the Chameleon Jupyter environment.\n",
    "\n",
    "Run the following cell, and make sure the correct project is selected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "V8EKIQ6yxhYc",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09559d788836487d9127201c31495d18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Dropdown(description='Select Project', options=('CHI-251409',), value='CHI-251409'), Output()))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92e1a80eba374826962400e38c21a714",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Dropdown(description='Select Site', index=1, options=('CHI@TACC', 'CHI@UC', 'CHI@EVL', 'CHI@NCA…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from chi import server, context, lease\n",
    "import os\n",
    "\n",
    "context.version = \"1.0\"\n",
    "context.choose_project()\n",
    "context.choose_site(default=\"CHI@UC\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change the string in the following cell to reflect the name of *your* lease (**with your own net ID**), then run it to get your lease:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd3bbd7c22dc4753a51d79797cedba91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value='\\n        <h2>Lease Details</h2>\\n        <table>\\n            <tr><th>Name</th><td>project32_fine…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lease Details:\n",
      "Name: project32_finetune\n",
      "ID: 76e1d149-5f25-4ad2-8f5c-7c8be8175e87\n",
      "Status: ACTIVE\n",
      "Start Date: 2025-04-30 14:00:00\n",
      "End Date: 2025-04-30 23:00:00\n",
      "User ID: 56d5c544b0f21fe6899983b50641e15cc9362aa5af4f0452f1c4818715e10e3f\n",
      "Project ID: 7c0a7a1952e44c94aa75cae1ff5dc9b4\n",
      "\n",
      "Node Reservations:\n",
      "ID: a5680ee8-bca7-45a6-8247-99744be87fda, Status: active, Min: 1, Max: 1\n",
      "\n",
      "Floating IP Reservations:\n",
      "\n",
      "Network Reservations:\n",
      "\n",
      "Events:\n"
     ]
    }
   ],
   "source": [
    "l = lease.get_lease(f\"finetune_exp_project32\") # or llm_single_netID, or llm_multi_netID\n",
    "l.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zfuztI7KyE4G"
   },
   "source": [
    "\n",
    "The status should show as “ACTIVE” now that we are past the lease start time.\n",
    "\n",
    "The rest of this notebook can be executed without any interactions from you, so at this point, you can save time by clicking on this cell, then selecting Run \\> Run Selected Cell and All Below from the Jupyter menu.\n",
    "\n",
    "As the notebook executes, monitor its progress to make sure it does not get stuck on any execution error, and also to see what it is doing!\n",
    "We will use the lease to bring up a server with the `CC-Ubuntu24.04-CUDA` disk image. (Note that the reservation information is passed when we create the instance!) This will take up to 10 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "rLlEVkmjyHsI",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for server node-llm-ks7406_nyu_edu's status to become ACTIVE. This typically takes 10 minutes, but can take up to 20 minutes.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9df2cbe38058488889a3efa4f5654c1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Label(value=''), IntProgress(value=0, bar_style='success')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Server has moved to status ACTIVE\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table style='border-collapse: collapse; width: 100%;'><tr style='background-color: #f2f2f2;'><th style='border: 1px solid #ddd; padding: 8px;'>Attribute</th><th style='border: 1px solid #ddd; padding: 8px;'>node-llm-ks7406_nyu_edu</th></tr><tr><td style='border: 1px solid #ddd; padding: 8px;'>Id</td><td style='border: 1px solid #ddd; padding: 8px;'>650a117c-78ac-4158-a447-490c04587b81</td></tr><tr><td style='border: 1px solid #ddd; padding: 8px;'>Status</td><td style='border: 1px solid #ddd; padding: 8px;'>ACTIVE</td></tr><tr><td style='border: 1px solid #ddd; padding: 8px;'>Image Name</td><td style='border: 1px solid #ddd; padding: 8px;'>CC-Ubuntu24.04-CUDA</td></tr><tr><td style='border: 1px solid #ddd; padding: 8px;'>Flavor Name</td><td style='border: 1px solid #ddd; padding: 8px;'>baremetal</td></tr><tr><td style='border: 1px solid #ddd; padding: 8px;'>Addresses</td><td style='border: 1px solid #ddd; padding: 8px;'><strong>sharednet1:</strong><br>&nbsp;&nbsp;IP: 10.140.82.109 (v4)<br>&nbsp;&nbsp;Type: fixed<br>&nbsp;&nbsp;MAC: 14:23:f2:a3:f8:00<br></td></tr><tr><td style='border: 1px solid #ddd; padding: 8px;'>Network Name</td><td style='border: 1px solid #ddd; padding: 8px;'>sharednet1</td></tr><tr><td style='border: 1px solid #ddd; padding: 8px;'>Created At</td><td style='border: 1px solid #ddd; padding: 8px;'>2025-04-30T19:15:51Z</td></tr><tr><td style='border: 1px solid #ddd; padding: 8px;'>Keypair</td><td style='border: 1px solid #ddd; padding: 8px;'>ks7406_nyu_edu-jupyter</td></tr><tr><td style='border: 1px solid #ddd; padding: 8px;'>Reservation Id</td><td style='border: 1px solid #ddd; padding: 8px;'>a5680ee8-bca7-45a6-8247-99744be87fda</td></tr><tr><td style='border: 1px solid #ddd; padding: 8px;'>Host Id</td><td style='border: 1px solid #ddd; padding: 8px;'>c15c5d0cd98629a41c320d11364f137f4320899eed52f609fb88500c</td></tr><tr><td style='border: 1px solid #ddd; padding: 8px;'>Host Status</td><td style='border: 1px solid #ddd; padding: 8px;'>None</td></tr><tr><td style='border: 1px solid #ddd; padding: 8px;'>Hypervisor Hostname</td><td style='border: 1px solid #ddd; padding: 8px;'>bc6a158a-14fd-4e99-b64f-742ac13f59b8</td></tr><tr><td style='border: 1px solid #ddd; padding: 8px;'>Is Locked</td><td style='border: 1px solid #ddd; padding: 8px;'>False</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "username = os.getenv('USER') # all exp resources will have this prefix\n",
    "s = server.Server(\n",
    "    f\"node-llm-{username}\",\n",
    "    reservation_id=l.node_reservations[0][\"id\"],\n",
    "    image_name=\"CC-Ubuntu24.04-CUDA\"\n",
    ")\n",
    "s.submit(idempotent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: security groups are not used at Chameleon bare metal sites, so we do not have to configure any security groups on this instance.\n",
    "Then, we’ll associate a floating IP with the instance, so that we can access it over SSH.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking connectivity to 192.5.87.113 port 22.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "453e56b88ad14edb9d1c7b47a133ccbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Label(value=''), IntProgress(value=0, bar_style='success')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection successful\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table style='border-collapse: collapse; width: 100%;'><tr style='background-color: #f2f2f2;'><th style='border: 1px solid #ddd; padding: 8px;'>Attribute</th><th style='border: 1px solid #ddd; padding: 8px;'>node-llm-ks7406_nyu_edu</th></tr><tr><td style='border: 1px solid #ddd; padding: 8px;'>Id</td><td style='border: 1px solid #ddd; padding: 8px;'>650a117c-78ac-4158-a447-490c04587b81</td></tr><tr><td style='border: 1px solid #ddd; padding: 8px;'>Status</td><td style='border: 1px solid #ddd; padding: 8px;'>ACTIVE</td></tr><tr><td style='border: 1px solid #ddd; padding: 8px;'>Image Name</td><td style='border: 1px solid #ddd; padding: 8px;'>CC-Ubuntu24.04-CUDA</td></tr><tr><td style='border: 1px solid #ddd; padding: 8px;'>Flavor Name</td><td style='border: 1px solid #ddd; padding: 8px;'>baremetal</td></tr><tr><td style='border: 1px solid #ddd; padding: 8px;'>Addresses</td><td style='border: 1px solid #ddd; padding: 8px;'><strong>sharednet1:</strong><br>&nbsp;&nbsp;IP: 10.140.82.109 (v4)<br>&nbsp;&nbsp;Type: fixed<br>&nbsp;&nbsp;MAC: 14:23:f2:a3:f8:00<br>&nbsp;&nbsp;IP: 192.5.87.113 (v4)<br>&nbsp;&nbsp;Type: floating<br>&nbsp;&nbsp;MAC: 14:23:f2:a3:f8:00<br></td></tr><tr><td style='border: 1px solid #ddd; padding: 8px;'>Network Name</td><td style='border: 1px solid #ddd; padding: 8px;'>sharednet1</td></tr><tr><td style='border: 1px solid #ddd; padding: 8px;'>Created At</td><td style='border: 1px solid #ddd; padding: 8px;'>2025-04-30T19:15:51Z</td></tr><tr><td style='border: 1px solid #ddd; padding: 8px;'>Keypair</td><td style='border: 1px solid #ddd; padding: 8px;'>ks7406_nyu_edu-jupyter</td></tr><tr><td style='border: 1px solid #ddd; padding: 8px;'>Reservation Id</td><td style='border: 1px solid #ddd; padding: 8px;'>a5680ee8-bca7-45a6-8247-99744be87fda</td></tr><tr><td style='border: 1px solid #ddd; padding: 8px;'>Host Id</td><td style='border: 1px solid #ddd; padding: 8px;'>c15c5d0cd98629a41c320d11364f137f4320899eed52f609fb88500c</td></tr><tr><td style='border: 1px solid #ddd; padding: 8px;'>Host Status</td><td style='border: 1px solid #ddd; padding: 8px;'>None</td></tr><tr><td style='border: 1px solid #ddd; padding: 8px;'>Hypervisor Hostname</td><td style='border: 1px solid #ddd; padding: 8px;'>bc6a158a-14fd-4e99-b64f-742ac13f59b8</td></tr><tr><td style='border: 1px solid #ddd; padding: 8px;'>Is Locked</td><td style='border: 1px solid #ddd; padding: 8px;'>False</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "s.associate_floating_ip()\n",
    "s.refresh()\n",
    "s.check_connectivity()\n",
    "s.refresh()\n",
    "s.show(type=\"widget\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rRhasFyIyOrS"
   },
   "source": [
    "## Set up Docker with NVIDIA container toolkit\n",
    "\n",
    "To use common deep learning frameworks like Tensorflow or PyTorch, we can run containers that have all the prerequisite libraries necessary for these frameworks. Here, we will set up the container framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "08s4VxU-yQs9",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/paramiko/client.py:889: UserWarning: Unknown ssh-ed25519 host key for 192.5.87.113: b'70850babb67a470a450265948f898eaf'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Executing docker install script, commit: 53a22f61c0628e58e1d6680b49e82993d304b449\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "+ sh -c apt-get -qq update >/dev/null\n",
      "+ sh -c DEBIAN_FRONTEND=noninteractive apt-get -y -qq install ca-certificates curl >/dev/null\n",
      "+ sh -c install -m 0755 -d /etc/apt/keyrings\n",
      "+ sh -c curl -fsSL \"https://download.docker.com/linux/ubuntu/gpg\" -o /etc/apt/keyrings/docker.asc\n",
      "+ sh -c chmod a+r /etc/apt/keyrings/docker.asc\n",
      "+ sh -c echo \"deb [arch=amd64 signed-by=/etc/apt/keyrings/docker.asc] https://download.docker.com/linux/ubuntu noble stable\" > /etc/apt/sources.list.d/docker.list\n",
      "+ sh -c apt-get -qq update >/dev/null\n",
      "+ sh -c DEBIAN_FRONTEND=noninteractive apt-get -y -qq install docker-ce docker-ce-cli containerd.io docker-compose-plugin docker-ce-rootless-extras docker-buildx-plugin >/dev/null\n",
      "\n",
      "Running kernel seems to be up-to-date.\n",
      "\n",
      "The processor microcode seems to be up-to-date.\n",
      "\n",
      "No services need to be restarted.\n",
      "\n",
      "No containers need to be restarted.\n",
      "\n",
      "No user sessions are running outdated binaries.\n",
      "\n",
      "No VM guests are running outdated hypervisor (qemu) binaries on this host.\n",
      "+ sh -c docker version\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client: Docker Engine - Community\n",
      " Version:           28.1.1\n",
      " API version:       1.49\n",
      " Go version:        go1.23.8\n",
      " Git commit:        4eba377\n",
      " Built:             Fri Apr 18 09:52:14 2025\n",
      " OS/Arch:           linux/amd64\n",
      " Context:           default\n",
      "\n",
      "Server: Docker Engine - Community\n",
      " Engine:\n",
      "  Version:          28.1.1\n",
      "  API version:      1.49 (minimum version 1.24)\n",
      "  Go version:       go1.23.8\n",
      "  Git commit:       01f442b\n",
      "  Built:            Fri Apr 18 09:52:14 2025\n",
      "  OS/Arch:          linux/amd64\n",
      "  Experimental:     false\n",
      " containerd:\n",
      "  Version:          1.7.27\n",
      "  GitCommit:        05044ec0a9a75232cad458027ca83437aae3f4da\n",
      " runc:\n",
      "  Version:          1.2.5\n",
      "  GitCommit:        v1.2.5-0-g59923ef\n",
      " docker-init:\n",
      "  Version:          0.19.0\n",
      "  GitCommit:        de40ad0\n",
      "\n",
      "================================================================================\n",
      "\n",
      "To run Docker as a non-privileged user, consider setting up the\n",
      "Docker daemon in rootless mode for your user:\n",
      "\n",
      "    dockerd-rootless-setuptool.sh install\n",
      "\n",
      "Visit https://docs.docker.com/go/rootless/ to learn about rootless mode.\n",
      "\n",
      "\n",
      "To run the Docker daemon as a fully privileged service, but granting non-root\n",
      "users access, refer to https://docs.docker.com/go/daemon-access/\n",
      "\n",
      "WARNING: Access to the remote API on a privileged Docker daemon is equivalent\n",
      "         to root access on the host. Refer to the 'Docker daemon attack surface'\n",
      "         documentation for details: https://docs.docker.com/go/attack-surface/\n",
      "\n",
      "================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unable to find image 'hello-world:latest' locally\n",
      "latest: Pulling from library/hello-world\n",
      "e6590344b1a5: Pulling fs layer\n",
      "e6590344b1a5: Download complete\n",
      "e6590344b1a5: Pull complete\n",
      "Digest: sha256:c41088499908a59aae84b0a49c70e86f4731e588a737f1637e73c8c09d995654\n",
      "Status: Downloaded newer image for hello-world:latest\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hello from Docker!\n",
      "This message shows that your installation appears to be working correctly.\n",
      "\n",
      "To generate this message, Docker took the following steps:\n",
      " 1. The Docker client contacted the Docker daemon.\n",
      " 2. The Docker daemon pulled the \"hello-world\" image from the Docker Hub.\n",
      "    (amd64)\n",
      " 3. The Docker daemon created a new container from that image which runs the\n",
      "    executable that produces the output you are currently reading.\n",
      " 4. The Docker daemon streamed that output to the Docker client, which sent it\n",
      "    to your terminal.\n",
      "\n",
      "To try something more ambitious, you can run an Ubuntu container with:\n",
      " $ docker run -it ubuntu bash\n",
      "\n",
      "Share images, automate workflows, and more with a free Docker ID:\n",
      " https://hub.docker.com/\n",
      "\n",
      "For more examples and ideas, visit:\n",
      " https://docs.docker.com/get-started/\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Result cmd='docker run hello-world' exited=0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.execute(\"curl -sSL https://get.docker.com/ | sudo sh\")\n",
    "s.execute(\"sudo groupadd -f docker; sudo usermod -aG docker $USER\")\n",
    "s.execute(\"docker run hello-world\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jThD1pvyyVpn"
   },
   "source": [
    "We will also install the NVIDIA container toolkit, with which we can access GPUs from inside our containers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PucqFrcwyZtR",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://nvidia.github.io/libnvidia-container/stable/deb/$(ARCH) /\n",
      "#deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://nvidia.github.io/libnvidia-container/experimental/deb/$(ARCH) /\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: apt does not have a stable CLI interface. Use with caution in scripts.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get:1 https://nvidia.github.io/libnvidia-container/stable/deb/amd64  InRelease [1477 B]\n",
      "Hit:2 https://download.docker.com/linux/ubuntu noble InRelease\n",
      "Hit:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2404/x86_64  InRelease\n",
      "Get:4 https://nvidia.github.io/libnvidia-container/stable/deb/amd64  Packages [18.6 kB]\n",
      "Hit:5 http://security.ubuntu.com/ubuntu noble-security InRelease\n",
      "Get:6 http://nova.clouds.archive.ubuntu.com/ubuntu noble InRelease [256 kB]\n",
      "Get:7 http://nova.clouds.archive.ubuntu.com/ubuntu noble-updates InRelease [126 kB]\n",
      "Get:8 http://nova.clouds.archive.ubuntu.com/ubuntu noble-backports InRelease [126 kB]\n",
      "Fetched 528 kB in 1s (495 kB/s)\n",
      "Reading package lists...\n",
      "Building dependency tree...\n",
      "Reading state information...\n",
      "58 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
      "Reading package lists...\n",
      "Building dependency tree...\n",
      "Reading state information...\n",
      "The following additional packages will be installed:\n",
      "  libnvidia-container-tools libnvidia-container1 nvidia-container-toolkit-base\n",
      "The following NEW packages will be installed:\n",
      "  libnvidia-container-tools libnvidia-container1 nvidia-container-toolkit\n",
      "  nvidia-container-toolkit-base\n",
      "0 upgraded, 4 newly installed, 0 to remove and 58 not upgraded.\n",
      "Need to get 5849 kB of archives.\n",
      "After this operation, 27.9 MB of additional disk space will be used.\n",
      "Get:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2404/x86_64  libnvidia-container1 1.17.6-1 [926 kB]\n",
      "Get:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2404/x86_64  libnvidia-container-tools 1.17.6-1 [21.5 kB]\n",
      "Get:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2404/x86_64  nvidia-container-toolkit-base 1.17.6-1 [3711 kB]\n",
      "Get:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2404/x86_64  nvidia-container-toolkit 1.17.6-1 [1191 kB]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "debconf: unable to initialize frontend: Dialog\n",
      "debconf: (Dialog frontend will not work on a dumb terminal, an emacs shell buffer, or without a controlling terminal.)\n",
      "debconf: falling back to frontend: Readline\n",
      "debconf: unable to initialize frontend: Readline\n",
      "debconf: (This frontend requires a controlling tty.)\n",
      "debconf: falling back to frontend: Teletype\n",
      "dpkg-preconfigure: unable to re-open stdin: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetched 5849 kB in 2s (2656 kB/s)\n",
      "Selecting previously unselected package libnvidia-container1:amd64.\n",
      "(Reading database ... 113552 files and directories currently installed.)\n",
      "Preparing to unpack .../libnvidia-container1_1.17.6-1_amd64.deb ...\n",
      "Unpacking libnvidia-container1:amd64 (1.17.6-1) ...\n",
      "Selecting previously unselected package libnvidia-container-tools.\n",
      "Preparing to unpack .../libnvidia-container-tools_1.17.6-1_amd64.deb ...\n",
      "Unpacking libnvidia-container-tools (1.17.6-1) ...\n",
      "Selecting previously unselected package nvidia-container-toolkit-base.\n",
      "Preparing to unpack .../nvidia-container-toolkit-base_1.17.6-1_amd64.deb ...\n",
      "Unpacking nvidia-container-toolkit-base (1.17.6-1) ...\n",
      "Selecting previously unselected package nvidia-container-toolkit.\n",
      "Preparing to unpack .../nvidia-container-toolkit_1.17.6-1_amd64.deb ...\n",
      "Unpacking nvidia-container-toolkit (1.17.6-1) ...\n",
      "Setting up nvidia-container-toolkit-base (1.17.6-1) ...\n",
      "Setting up libnvidia-container1:amd64 (1.17.6-1) ...\n",
      "Setting up libnvidia-container-tools (1.17.6-1) ...\n",
      "Setting up nvidia-container-toolkit (1.17.6-1) ...\n",
      "Processing triggers for libc-bin (2.39-0ubuntu8.4) ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "debconf: unable to initialize frontend: Dialog\n",
      "debconf: (Dialog frontend will not work on a dumb terminal, an emacs shell buffer, or without a controlling terminal.)\n",
      "debconf: falling back to frontend: Readline\n",
      "debconf: unable to initialize frontend: Readline\n",
      "debconf: (This frontend requires a controlling tty.)\n",
      "debconf: falling back to frontend: Teletype\n",
      "\n",
      "Running kernel seems to be up-to-date.\n",
      "\n",
      "The processor microcode seems to be up-to-date.\n",
      "\n",
      "No services need to be restarted.\n",
      "\n",
      "No containers need to be restarted.\n",
      "\n",
      "No user sessions are running outdated binaries.\n",
      "\n",
      "No VM guests are running outdated hypervisor (qemu) binaries on this host.\n",
      "time=\"2025-04-30T19:28:58Z\" level=info msg=\"Config file does not exist; using empty config\"\n",
      "time=\"2025-04-30T19:28:58Z\" level=info msg=\"Wrote updated config to /etc/docker/daemon.json\"\n",
      "time=\"2025-04-30T19:28:58Z\" level=info msg=\"It is recommended that docker daemon be restarted.\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Result cmd='sudo systemctl restart docker' exited=0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get NVIDIA container toolkit\n",
    "s.execute(\"curl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey | sudo gpg --dearmor -o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg \\\n",
    "  && curl -s -L https://nvidia.github.io/libnvidia-container/stable/deb/nvidia-container-toolkit.list | \\\n",
    "    sed 's#deb https://#deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://#g' | \\\n",
    "    sudo tee /etc/apt/sources.list.d/nvidia-container-toolkit.list\")\n",
    "s.execute(\"sudo apt update\")\n",
    "s.execute(\"sudo apt-get install -y nvidia-container-toolkit\")\n",
    "s.execute(\"sudo nvidia-ctk runtime configure --runtime=docker\")\n",
    "# s.execute(\"sudo jq 'if has(\\\"exec-opts\\\") then . else . + {\\\"exec-opts\\\": [\\\"native.cgroupdriver=cgroupfs\\\"]} end' /etc/docker/daemon.json | sudo tee /etc/docker/daemon.json.tmp > /dev/null && sudo mv /etc/docker/daemon.json.tmp /etc/docker/daemon.json\")\n",
    "s.execute(\"sudo systemctl restart docker\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and we can install `nvtop` to monitor GPU usage:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s.execute(\"sudo apt update\")\n",
    "s.execute(\"sudo apt -y install nvtop\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9lu27RViybEw"
   },
   "source": [
    "In the following cell, we will verify that we can see our NVIDIA GPUs from inside a container, by passing `--gpus-all`. (The `-rm` flag says to clean up the container and remove its filesystem when it finishes running.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "-qiPAmiiycqG",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unable to find image 'ubuntu:latest' locally\n",
      "latest: Pulling from library/ubuntu\n",
      "2726e237d1a3: Pulling fs layer\n",
      "2726e237d1a3: Verifying Checksum\n",
      "2726e237d1a3: Download complete\n",
      "2726e237d1a3: Pull complete\n",
      "Digest: sha256:1e622c5f073b4f6bfad6632f2616c7f59ef256e96fe78bf6a595d1dc4376ac02\n",
      "Status: Downloaded newer image for ubuntu:latest\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Apr 30 19:29:23 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 560.35.05              Driver Version: 560.35.05      CUDA Version: 12.6     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA A100 80GB PCIe          Off |   00000000:27:00.0 Off |                    0 |\n",
      "| N/A   63C    P0             65W /  300W |       1MiB /  81920MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Result cmd='docker run --rm --gpus all ubuntu nvidia-smi' exited=0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.execute(\"docker run --rm --gpus all ubuntu nvidia-smi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mnZM9VYhykh6"
   },
   "source": [
    "## Pull and start container\n",
    "\n",
    "Let’s pull the container:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a container image - for MLFlow section\n",
    "\n",
    "Finally, we will build a container image in which to work in the MLFlow section, that has:\n",
    "\n",
    "-   a Jupyter notebook server\n",
    "-   Pytorch and Pytorch Lightning\n",
    "-   CUDA, which allows deep learning frameworks like Pytorch to use the NVIDIA GPU accelerator\n",
    "-   and MLFlow\n",
    "\n",
    "You can see our Dockerfile for this image at: [Dockerfile.jupyter-torch-mlflow-cuda](https://github.com/teaching-on-testbeds/mltrain-chi/tree/main/docker/Dockerfile.jupyter-torch-mlflow-cuda)\n",
    "\n",
    "Building this container may take a bit of time, but that’s OK: we can get it started and then continue to the next section while it builds in the background, since we don’t need this container immediately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z6u-nuWbyiz-",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda12-pytorch-2.5.1: Pulling from jupyter/pytorch-notebook\n",
      "54609b48ebc1: Pulling fs layer\n",
      "1bf84e16ee78: Pulling fs layer\n",
      "70c528583d48: Pulling fs layer\n",
      "4f4fb700ef54: Pulling fs layer\n",
      "d20e2bbc3444: Pulling fs layer\n",
      "e1236ab05074: Pulling fs layer\n",
      "eb55310ee8e5: Pulling fs layer\n",
      "247e1eb593d5: Pulling fs layer\n",
      "1ac7d388a98d: Pulling fs layer\n",
      "f27656f139fb: Pulling fs layer\n",
      "ac0c7c38d6ad: Pulling fs layer\n",
      "c48c65e32957: Pulling fs layer\n",
      "d79a028c87d5: Pulling fs layer\n",
      "1ac7d388a98d: Waiting\n",
      "f27656f139fb: Waiting\n",
      "e1236ab05074: Waiting\n",
      "ac0c7c38d6ad: Waiting\n",
      "eb55310ee8e5: Waiting\n",
      "4f4fb700ef54: Waiting\n",
      "d20e2bbc3444: Waiting\n",
      "c48c65e32957: Waiting\n",
      "97452dad12c2: Pulling fs layer\n",
      "aad099df5a3e: Pulling fs layer\n",
      "aad099df5a3e: Waiting\n",
      "41e13f849bef: Pulling fs layer\n",
      "3734b2c4ea95: Pulling fs layer\n",
      "5d39b32f56c0: Pulling fs layer\n",
      "41e13f849bef: Waiting\n",
      "dfe267ba30ed: Pulling fs layer\n",
      "92e65b90a905: Pulling fs layer\n",
      "bc6b6634a9a2: Pulling fs layer\n",
      "35eb378f4751: Pulling fs layer\n",
      "3734b2c4ea95: Waiting\n",
      "be517295261b: Pulling fs layer\n",
      "5d39b32f56c0: Waiting\n",
      "9ae764103bf5: Pulling fs layer\n",
      "92e65b90a905: Waiting\n",
      "b3983557f014: Pulling fs layer\n",
      "9ae764103bf5: Waiting\n",
      "5520f089e27b: Pulling fs layer\n",
      "be517295261b: Waiting\n",
      "bc6b6634a9a2: Waiting\n",
      "dfe267ba30ed: Waiting\n",
      "b3983557f014: Waiting\n",
      "3ce629816282: Pulling fs layer\n",
      "3ce629816282: Waiting\n",
      "1bf84e16ee78: Verifying Checksum\n",
      "1bf84e16ee78: Download complete\n",
      "70c528583d48: Verifying Checksum\n",
      "70c528583d48: Download complete\n",
      "d20e2bbc3444: Verifying Checksum\n",
      "d20e2bbc3444: Download complete\n",
      "54609b48ebc1: Download complete\n",
      "e1236ab05074: Verifying Checksum\n",
      "e1236ab05074: Download complete\n",
      "eb55310ee8e5: Verifying Checksum\n",
      "eb55310ee8e5: Download complete\n",
      "1ac7d388a98d: Verifying Checksum\n",
      "1ac7d388a98d: Download complete\n",
      "4f4fb700ef54: Verifying Checksum\n",
      "4f4fb700ef54: Download complete\n",
      "247e1eb593d5: Download complete\n",
      "54609b48ebc1: Pull complete\n",
      "f27656f139fb: Verifying Checksum\n",
      "f27656f139fb: Download complete\n",
      "c48c65e32957: Verifying Checksum\n",
      "c48c65e32957: Download complete\n",
      "ac0c7c38d6ad: Verifying Checksum\n",
      "ac0c7c38d6ad: Download complete\n",
      "d79a028c87d5: Verifying Checksum\n",
      "d79a028c87d5: Download complete\n",
      "41e13f849bef: Verifying Checksum\n",
      "41e13f849bef: Download complete\n",
      "1bf84e16ee78: Pull complete\n",
      "70c528583d48: Pull complete\n",
      "4f4fb700ef54: Pull complete\n",
      "d20e2bbc3444: Pull complete\n",
      "e1236ab05074: Pull complete\n",
      "eb55310ee8e5: Pull complete\n",
      "247e1eb593d5: Pull complete\n",
      "1ac7d388a98d: Pull complete\n",
      "3734b2c4ea95: Verifying Checksum\n",
      "3734b2c4ea95: Download complete\n",
      "aad099df5a3e: Verifying Checksum\n",
      "aad099df5a3e: Download complete\n",
      "5d39b32f56c0: Verifying Checksum\n",
      "5d39b32f56c0: Download complete\n",
      "92e65b90a905: Verifying Checksum\n",
      "92e65b90a905: Download complete\n",
      "bc6b6634a9a2: Verifying Checksum\n",
      "bc6b6634a9a2: Download complete\n",
      "35eb378f4751: Verifying Checksum\n",
      "35eb378f4751: Download complete\n",
      "dfe267ba30ed: Verifying Checksum\n",
      "dfe267ba30ed: Download complete\n",
      "97452dad12c2: Verifying Checksum\n",
      "97452dad12c2: Download complete\n",
      "b3983557f014: Verifying Checksum\n",
      "b3983557f014: Download complete\n",
      "5520f089e27b: Verifying Checksum\n",
      "5520f089e27b: Download complete\n",
      "be517295261b: Verifying Checksum\n",
      "be517295261b: Download complete\n",
      "9ae764103bf5: Verifying Checksum\n",
      "9ae764103bf5: Download complete\n",
      "f27656f139fb: Pull complete\n",
      "ac0c7c38d6ad: Pull complete\n",
      "c48c65e32957: Pull complete\n",
      "d79a028c87d5: Pull complete\n",
      "97452dad12c2: Pull complete\n",
      "aad099df5a3e: Pull complete\n",
      "41e13f849bef: Pull complete\n",
      "3734b2c4ea95: Pull complete\n",
      "5d39b32f56c0: Pull complete\n",
      "3ce629816282: Verifying Checksum\n",
      "3ce629816282: Download complete\n",
      "dfe267ba30ed: Pull complete\n",
      "92e65b90a905: Pull complete\n",
      "bc6b6634a9a2: Pull complete\n",
      "35eb378f4751: Pull complete\n",
      "be517295261b: Pull complete\n",
      "9ae764103bf5: Pull complete\n",
      "b3983557f014: Pull complete\n",
      "5520f089e27b: Pull complete\n",
      "3ce629816282: Pull complete\n",
      "Digest: sha256:3a4865f4cb19ea3195855c7da7e13df8445126c43cb83dc1333f1dd17c719a62\n",
      "Status: Downloaded newer image for quay.io/jupyter/pytorch-notebook:cuda12-pytorch-2.5.1\n",
      "quay.io/jupyter/pytorch-notebook:cuda12-pytorch-2.5.1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Result cmd='docker pull quay.io/jupyter/pytorch-notebook:cuda12-pytorch-2.5.1' exited=0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.execute(\"docker build -t jupyter-mlflow -f mltrain-chi/docker/Dockerfile.jupyter-torch-mlflow-cuda .\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Leave that cell running, and in the meantime, open an SSH sesson on your server. From your local terminal, run\n",
    "\n",
    "    ssh -i ~/.ssh/id_rsa_chameleon cc@A.B.C.D\n",
    "\n",
    "where\n",
    "\n",
    "-   in place of `~/.ssh/id_rsa_chameleon`, substitute the path to your own key that you had uploaded to CHI@TACC\n",
    "-   in place of `A.B.C.D`, use the floating IP address you just associated to your instance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start MLFlow tracking server system\n",
    "\n",
    "Now we are ready to get it started! Bring up our MLFlow system with:\n",
    "\n",
    "``` bash\n",
    "# run on node-mltrain\n",
    "docker compose -f mltrain-chi/docker/docker-compose-mlflow.yaml up -d\n",
    "```\n",
    "\n",
    "which will pull each container image, then start them.\n",
    "\n",
    "When it is finished, the output of\n",
    "\n",
    "``` bash\n",
    "# run on node-mltrain\n",
    "docker ps\n",
    "```\n",
    "\n",
    "should show that the `minio`, `postgres`, and `mlflow` containers are running."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Access dashboards for the MLFlow tracking server system\n",
    "\n",
    "Both MLFlow and MinIO include a browser-based dashboard. Let’s open these to make sure that we can find our way around them.\n",
    "\n",
    "The MinIO dashboard runs on port 9001. In a browser, open\n",
    "\n",
    "    http://A.B.C.D:9001\n",
    "\n",
    "where in place of `A.B.C.D`, substitute the floating IP associated with your server.\n",
    "\n",
    "Log in with the credentials we specified in the Docker Compose YAML:\n",
    "\n",
    "-   Username: `your-access-key`\n",
    "-   Password: `your-secret-key`\n",
    "\n",
    "Then,\n",
    "\n",
    "-   Click on the “Buckets” section and note the `mlflow-artifacts` storage bucket that we created as part of the Docker Compose.\n",
    "-   Click on “Monitoring \\> Metrics” and note the dashboard that shows the storage system health. MinIO works as a distributed object store with many advanced capabilities, although we are not using them; this dashboard lets operators keep an eye on system status.\n",
    "-   Click on “Object Browser”. In this section, you can look at the files that have been uploaded to the object store - but, we haven’t used MLFlow yet, so for now there is nothing interesting here. However, as you start to log artifacts to the MLFlow server, you will see them appear here.\n",
    "\n",
    "Next, let’s look at the MLFlow UI. This runs on port 8000. In a browser, open\n",
    "\n",
    "    http://A.B.C.D:8000\n",
    "\n",
    "where in place of `A.B.C.D`, substitute the floating IP associated with your server.\n",
    "\n",
    "The UI shows a list of tracked “experiments”, and experiment “runs”. (A “run” corresponds to one instance of training a model; an “experiment” groups together related runs.) Since we have not yet used MLFlow, for now we will only see a “Default” experiment and no runs. But, that will change very soon!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start a Jupyter server\n",
    "\n",
    "Finally, we’ll start the Jupyter server container, inside which we will run experiments that are tracked in MLFlow. Make sure your container image build, from the previous section, is now finished - you should see a “jupyter-mlflow” image in the output of:\n",
    "\n",
    "``` bash\n",
    "# run on node-mltrain\n",
    "docker image list\n",
    "```\n",
    "\n",
    "The command to run will depend on what type of GPU node you are using -\n",
    "\n",
    "If you are using an AMD GPU (node type `gpu_mi100`), run\n",
    "\n",
    "``` bash\n",
    "# run on node-mltrain IF it is a gpu_mi100\n",
    "HOST_IP=$(curl --silent http://169.254.169.254/latest/meta-data/public-ipv4 )\n",
    "docker run  -d --rm  -p 8888:8888 \\\n",
    "    --device=/dev/kfd --device=/dev/dri \\\n",
    "    --group-add video --group-add $(getent group | grep render | cut -d':' -f 3) \\\n",
    "    --shm-size 16G \\\n",
    "    -v ~/mltrain-chi/workspace_mlflow:/home/jovyan/work/ \\\n",
    "    -v food11:/mnt/ \\\n",
    "    -e MLFLOW_TRACKING_URI=http://${HOST_IP}:8000/ \\\n",
    "    -e FOOD11_DATA_DIR=/mnt/Food-11 \\\n",
    "    --name jupyter \\\n",
    "    jupyter-mlflow\n",
    "```\n",
    "\n",
    "Note that we intially get `HOST_IP`, the floating IP assigned to your instance, as a variable; then we use it to specify the `MLFLOW_TRACKING_URI` inside the container. Training jobs inside the container will access the MLFlow tracking server using its public IP address.\n",
    "\n",
    "Here,\n",
    "\n",
    "-   `-d` says to start the container and detach, leaving it running in the background\n",
    "-   `-rm` says that after we stop the container, it should be removed immediately, instead of leaving it around for potential debugging\n",
    "-   `-p 8888:8888` says to publish the container’s port `8888` (the second `8888` in the argument) to the host port `8888` (the first `8888` in the argument)\n",
    "-   `--device=/dev/kfd --device=/dev/dri` pass the AMD GPUs to the container\n",
    "-   `--group-add video --group-add $(getent group | grep render | cut -d':' -f 3)` makes sure that the user inside the container is a member of a group that has permission to use the GPU(s) - the `video` group and the `render` group. (The `video` group always has the same group ID, by convention, but [the `render` group does not](https://github.com/ROCm/ROCm-docker/issues/90), so we need to find out its group ID on the host and pass that to the container.)\n",
    "-   `--shm-size 16G` increases the memory available for interprocess communication\n",
    "-   the host directory `~/mltrain-chi/workspace_mlflow` is mounted inside the workspace as `/home/jovyan/work/`\n",
    "-   the volume `food11` is mounted inside the workspace as `/mnt/`\n",
    "-   and we pass `MLFLOW_TRACKING_URI` and `FOOD11_DATA_DIR` as environment variables.\n",
    "\n",
    "If you are using an NVIDIA GPU (node type `compute_liqid`), run\n",
    "\n",
    "``` bash\n",
    "# run on node-mltrain IF it is a compute_liqid\n",
    "HOST_IP=$(curl --silent http://169.254.169.254/latest/meta-data/public-ipv4 )\n",
    "docker run  -d --rm  -p 8888:8888 \\\n",
    "    --gpus all \\\n",
    "    --shm-size 16G \\\n",
    "    -v ~/mltrain-chi/workspace_mlflow:/home/jovyan/work/ \\\n",
    "    -v food11:/mnt/ \\\n",
    "    -e MLFLOW_TRACKING_URI=http://${HOST_IP}:8000/ \\\n",
    "    -e FOOD11_DATA_DIR=/mnt/Food-11 \\\n",
    "    --name jupyter \\\n",
    "    jupyter-mlflow\n",
    "```\n",
    "\n",
    "Note that we intially get `HOST_IP`, the floating IP assigned to your instance, as a variable; then we use it to specify the `MLFLOW_TRACKING_URI` inside the container. Training jobs inside the container will access the MLFlow tracking server using its public IP address.\n",
    "\n",
    "-   `-d` says to start the container and detach, leaving it running in the background\n",
    "-   `-rm` says that after we stop the container, it should be removed immediately, instead of leaving it around for potential debugging\n",
    "-   `-p 8888:8888` says to publish the container’s port `8888` (the second `8888` in the argument) to the host port `8888` (the first `8888` in the argument)\n",
    "-   `--gus all` pass the NVIDIA GPUs to the container\n",
    "-   `--shm-size 16G` increases the memory available for interprocess communication\n",
    "-   the host directory `~/mltrain-chi/workspace_mlflow` is mounted inside the workspace as `/home/jovyan/work/`\n",
    "-   the volume `food11` is mounted inside the workspace as `/mnt/`\n",
    "-   and we pass `MLFLOW_TRACKING_URI` and `FOOD11_DATA_DIR` as environment variables.\n",
    "\n",
    "Then, run\n",
    "\n",
    "    docker logs jupyter\n",
    "\n",
    "and look for a line like\n",
    "\n",
    "    http://127.0.0.1:8888/lab?token=XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
    "\n",
    "Paste this into a browser tab, but in place of `127.0.0.1`, substitute the floating IP assigned to your instance, to open the Jupyter notebook interface.\n",
    "\n",
    "In the file browser on the left side, open the `work` directory.\n",
    "\n",
    "Open a terminal (“File \\> New \\> Terminal”) inside the Jupyter server environment, and in this terminal, run\n",
    "\n",
    "``` bash\n",
    "# runs on jupyter container inside node-mltrain\n",
    "env\n",
    "```\n",
    "\n",
    "to see environment variables. Confirm that the `MLFLOW_TRACKING_URI` is set, with the correct floating IP address."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
